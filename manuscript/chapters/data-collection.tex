%------------------------------------------------

\begin{fullwidth}
Most data collection is now done using digital data entry
using tools that are specially designed for surveys.
These tools, called ``computer-assisted personal interviewing'' (CAPI) softwares,
provide a wide range of features designed to make
implementing even highly complex surveys easy, scalable, and secure.
However, these are not fully automatic:
you still need to actively design and manage the survey.
Each software has specific practices that you need to follow
to enable features such as Stata-compatibility and data encryption.

While you can work in any software you like,
this guide will present tools and best practices
for working with SurveyCTO, a proprietary implementation of Open Data Kit (ODK).
Most of the processes below will also replicate in ODK
with minimal adjustment for the exact setup you have.
The important parts, of course, are primarily conceptual:
this chapter should provide a motivation for
planning data structure during survey design,
developing surveys that are easy to control for quality and security,
and having proper file storage ready for sensitive PII data.
\end{fullwidth}

%------------------------------------------------

\section{Primary data collection with SurveyCTO}

\subsection{Questionnaire design}

SurveyCTO surveys are primarily created in Excel or Google Sheets,
making them one of the few outputs for which no coding is required.
\marginnote{The \texttt{ietestform} command, part of
\texttt{iefieldkit}, implements form-checking routines
for some SurveyCTO best practices. You can find a summary
of those practices at \url{https://dimewiki.worldbank.org/wiki/ietestform}.}
However, since they make extensive use of logical structure and
relate directly to the data that will be used later,
both the field team and the data team should
collaborate to make sure that the survey suits all needs.\cite{krosnick2018questionnaire}

Generally, this collaboration means building the experimental design
fundamentally into the structure of the survey.
When ID matching and tracking is essential,
the survey should be prepared to verify against preloads
of data from master records or from other rounds,
and ``extensive tracking'' sections --
in which reasons for attrition, treatment contamination, and
loss to follow-up can be documented --
are essential data components for completing CONSORT records.\cite{begg1996improving}

The survey design is the first part where the data team
and the field team must collaborate on data structure.
The field-oriented staff and the PIs will likely prefer
to capture a large amount of detailed \texttt{information}
in the field, some of which will serve very poorly as \texttt{data}.\sidenote{\url{
https://iriss.stanford.edu/sites/g/files/sbiybj6196/f/questionnaire\_design\_1.pdf}}
In particular, open-ended responses and questions which will have
many null or missing responses by design will not be very useful
in statistical analyses unless pre-planned.
You must work with the field team to determine the appropriate amount
of abstraction inherent in linking concepts to responses.\sidenote{\url{
https://www.povertyactionlab.org/sites/default/files/documents/Instrument\%20Design\_Diva\_final.pdf}}
It is always possible to ask for specific responses to questions,
but it is far more useful to ask for things like Likert responses.

Coded responses are always more useful than open-ended responses,
because they reduce the time necessary for post-processing by
expensive specialized staff.
For example, if collecting data on medication use or supplies,
you could collect: the brand name of the product;
the generic name of the product;
the ATC-coded compound of the product;
or the broad category to which each product belongs (antibiotic, etc.).
All four may be useful for different reasons,
but the latter two are likely to be the most useful for the analyst.
The ATC-coded compound requires providing a translation dictionary
to field staff, but enables automated rapid recoding for analysis
with no loss of information.
The generic class requires agreement on the broad categories of interest,
but allow for much more comprehensible top-line statistics and data quality checks.

Broadly, the questionnaire should be designed as follows.
The workflow will feel much like writing an essay:
we begin from broad concepts and slowly flesh them out to specifics.
\marginnote{Modules should not be numbered --
they should use a short prefix so they can be easily reordered.
There is not yet a full consensus over how individual questions should be identified:
formats like \texttt{hq\_1} are hard to remember and unpleasant to reorder,
but formats like \texttt{hq\_asked\_about\_loans} quickly become too long.}
The theory of change, experimental design, and pre-analysis plans should be discussed
and the structure of required data conceptualized.
All the conceptual outcomes of interest, as well as the covariates, classifications,
and other variables needed for the experimental design should be listed out.
The questionnaire \textit{modules} should be outlined based on this list.
Each module should then be expanded into specific indicators to observe in the field.
Finally, the questionnaire can be piloted in a non-experimental sample.
Revisions are made, and the survey is then translated into the appropriate language and programmed electronically.

\subsection{Secure data collection}

Any established data collection service will always encrypt
all data submitted from the field automatically while in transit
(ie, upload or download), so if you use servers hosted by SurveyCTO
this is nothing you need to worry about.
Your data will be encrypted from the time it leaves the device
(in tablet-assisted data collation) or your browser (in web data collection)
until it reaches the server.

\marginnote{Encryption at rest is the only way to ensure
that PII data remains private when it is stored
on someone else’s server on the open internet.
The World Bank’s and many of our donors’ security requirements
for data storage can only be fulfilled by this method.
We recommend keeping your data encrypted whenever PII data is collected --
therefore, we recommend it for all field data collection.}

Encryption in cloud storage, by contrast, is not enabled by default.
This is because the service will not encrypt user data unless you confirm
you know how to operate the encryption system and assume its risks.
Encryption at rest is different from password-protection:
encryption at rest makes the underlying data itself unreadable,
even if accessed, except to users who have a specific private key file.
Encryption at rest requires active participation from you, the user,
and you should be fully aware that if your private key is lost,
there is absolutely no way to recover your data.

To enable data encryption, you simply select the encryption option
when you create a new form on a SurveyCTO server.
At that time, the service will allow you to download -- once --
the keyfile pair needed to decrypt the data.
You must download and store this in a secure location.
Again, we recommend LastPass, a software whose free option
allows you to store passwords as well as small keyfiles like this.
You should make sure the ``secure note'' which you put the keyfile in
is descriptively named to match the survey to which it corresponds.
The Sync app will ask you for the location of this file
when you download and set up data for use,
and all you will need to do is copy that file to your desktop,
point Sync to it, and the rest is automatic.

Finally, you should ensure that all teams take basic precautions
to ensure the security of data, as most problems are due to human error.
Most importantly, all computers, tablets, and accounts used
\textit{must} have a logon password associated with them.
This policy should also be applied to physical data storage
such as flash drives and hard drives;
similarly, files sent to the field containing PII data
such as the sampling list should at least be password-protected.\sidenote{
This can be done using a zip-file creator.
LastPass can also be used to share passwords securely,
and you cannot share passwords across email.}
This step significantly mitigates the risk in case there is
a security breach such as loss, theft, hacking, or a virus,
and adds very little hassle to utilization.


%------------------------------------------------

\section{Field management and quality assurance}

%------------------------------------------------

\section{Managing primary data}

In this section, you finally get your hands on some data!
What do we do with it? Data handling is one of the biggest
``black boxes'' in primary research -- it always gets done,
but teams have wildly different approaches for actually doing it.
This section breaks the process into key conceptual steps
and provides at least one practical solution for each.
Initial receipt of data will proceed as follows:
the data will be downloaded, and a ``gold master'' copy
of the raw data should be permanently stored in a secure location.
Then, a ``master'' copy of the data is placed into an encrypted location
that will remain accessible on disk and backed up.
This handling satisfies the rule of three:
there are two on-site copies of the data and one off-site copy,
so the data can never be lost in case of hardware failure.

For this step, the remote location can be a variety of forms:
the cheapest is a long-term cloud storage service
such as Amazon Web Services or Microsoft Azure.
Equally sufficient is a physical hard drive
stored somewhere other than the primary work location.
Enterprise cloud solutions like Microsoft OneDrive
can also work, although because you do not control them
we typically do not recommend this for the gold master copy.
If the service satisfies your security needs,
the raw data can be stored unencrypted here.
If you remain lucky, you will never have to access this copy --
you just want to know it is out there, safe, if you need it.

The copy of the raw data you are going to use
should be handled with care.
Since you will probably need to share it among the team,
it should be placed in an encrypted storage location.
The combination of Dropbox and VeraCrypt
can satisfy this requirement for some teams,
since this will never make the data visible to someone
who gets access to the Dropbox,
without the key to the file that is generated on encryption.
\marginnote{LastPass or equivalent should be used to store and share keys.
Note also that this data copy can never be considered the gold master,
because other users or the service can accidentally delete it.}
\textit{The file must never be placed in Dropbox unencrypted, however.}
The way VeraCrypt works is that it creates a virtual copy
of the unencrypted file outside of Dropbox, and lets you access that copy.
Since you should never edit the raw data, this will not be very cumbersome,
but the unencryption and usage of the raw data is a manual process.

You should only interact with the raw master data a couple of times.
The first task is therefore to create a derived copy --
a \textbf{de-identified} copy --
which you can freely share to team members on Dropbox unencrypted.
This dataset is produced by removing personally-identifying fields
from the raw master dataset. Nothing else is altered.
This step is as easy as dropping variables,
and since it only affects survey fields,
the variables to be kept can be indicated at the time the survey is written.
There is no harm in being overly conservative at this stage,
since if you find that another field is required,
you can re-access the encrypted master data and add it to the dataset.
With the unencrypted, de-identified data in a shared location
and the raw data securely stored and backed up,
you are ready to move to data cleaning and analysis.
