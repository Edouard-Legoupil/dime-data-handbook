%------------------------------------------------

\begin{fullwidth}
Data analysis is hard. Making sense of a dataset in such a way
that makes a substantial contribution to scientific knowledge
requires a mix of subject expertise, programming skills,
and statistical and econometric knowledge.
The process of data analysis is therefore typically
a back-and-forth discussion between the various people
who have differing experiences, perspectives, and research interests.
The research assistant usually ends up being the fulcrum
for this discussion, and has to transfer and translate
results among people with a wide range of technical capabilities
while making sure that code and outputs do not become
tangled and lost over time (typically months or years).

Organization is the key to this task.
The structure of files needs to be well-organized,
so that any material can be found when it is needed.
Data structures need to be organized,
so that the various steps in creating datasets
can always be traced and revised without massive effort.
The structure of version histories and backups need to be organized,
so that different workstreams can exist simultaneously
and experimental analyses can be tried without a complex workflow.
Finally, the outputs need to be organized,
so that it is clear what results go with what analyses,
and that each individual output is a readable element in its own right.
This chapter will teach you how to stay organized,
so that you and the team can focus on getting the work right
rather than trying to understand what you did in the past.
\end{fullwidth}

%------------------------------------------------

\section{Organizing primary survey data}

There are many schemes to organize primary survey data.
We provide the \texttt{iefolder}\sidenote{
\url{https://dimewiki.worldbank.org/iefolder}}
package (part of \texttt{ietoolkit}) so that
a large number of teams will have identical folder structures.
This means that PIs and RAs face very small costs
when switching between projects, becuase all the materials
will be organized in exactly the same basic way.
Namely, there will be dedicated folders for raw data;
for de-identified data; for cleaned data; and for final (constructed) data.
There will be a folder for raw results, as well as for final outputs.
The folders that hold code will be organized in parallel to these,
so that the progression through the whole project can be followed
by anyone new to the team. Additionally, \texttt{iefolder}
provides \textbf{master do-files} so that the entire order
of the project is maintained in a top-level dofile,
ensuring that no complex instructions are needed
to exactly replicate the project.

Raw data should contain only materials that are recieved directly from the field.
These datasets will invariably come in a host of file formats
and nearly always contain personally-identifying information.
These should be retained in the raw data folder
\textit{exactly as they were recieved},
including the precise filename that was submitted,
along with detailed documentation about the source and contents
of each of the files. This data must be encrypted
if it is shared in an insecure fashion,
and it must be backed up in a secure offsite location.
Everything else can be replaced, but raw data cannot.
Therefore, raw data should never be interacted with directly.

Instead, the first step upon receipt of data is de-identification.
There will be a code folder and a corresponding data folder
so that it is clear how de-identification is done and where it lives.
Typically, this process only involves stripping fields from raw data,
naming, formatting, and optimizing the file for storage,
and placing it as a \texttt{.dta} or other data-format file
into the de-identified data folder.
The underlying data structure is unchanged:
it should contain only fields that were collected in the field,
without any modifications to the responses collected there.
This creates a survey-based version of the file that is able
to be shared among the team without fear of data corruption or exposure.
Only a core set of team members will have access to the underlying
raw data necessary to re-generate these datasets,
since the encrypted raw data will be password-protected.
The de-identified data will therefore be the underlying source
for all cleaned and constructed data.
It will also become the template dataset for final public release.

%------------------------------------------------

\section{Cleaning and constructing derivative datasets}

Once the de-identified dataset is created, data must be cleaned.
This process does not involve the creation of any new data fields.
Cleaning can be a long process: this is the phase at which
you obtain an extensive understanding of the contents and structure
of the data you collected in the field.
You should use this time to understand the types of responses collected,
both within each survey question and across repsondents.
Understanding this structure will make it possible to do analysis.

There are a number of tasks involved in cleaning data.\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Data_Cleaning}}
\marginnote{The \texttt{iecodebook} command suite, part of \texttt{iefieldkit},
is designed to make some of the most tedious components of this process,
such as renaming, relabeling, and value labeling,
much easier (including in data appending).
\url{https://dimewiki.worldbank.org/iecodebook}}
A data cleaning do-file will load the de-identified data,
make sure all the fields are named and labelled concisely and descriptively,
apply corrections to all values in the dataset,
make sure value labels on responses are accurate and concise,
and attach any experimental-design data (sampling and randomization)
back to the clean dataset before saving.
It will ensure that ID variables are correctly structured and matching,
ensure that data storage types are correctly set up
(including tasks like dropping open-ended responses and encoding strings),
and make sure that data missingness is appropriately documented
using other primary inputs from the field.

Clean data then becomes the basis for constructed (final) datasets.
While data cleaning typically takes one survey dataset
and mixes it with other data inputs to create a corresponding
cleaned version of that survey data (a one-to-one process),
data construction is a much more open-ended process.
Data construction files mix-and-match clean datasets
to create a large number of potential analysis datasets.
Data construction is the time to create derived variables
(binaries, indices, and interactions, to name a few);
it is the time to reshape data as necessary;
it is the time to create functionally-named variables;
and it is the time to sensibly subset data for analysis.

Data construction is never a finished process.
It comes ``before'' data analysis only in a limited sense:
the construction code must be run before the analysis code.
Typically, however it is the case that the construction and analysis code
are written concurrently -- both do-files will be open at the same time.
This is because it is only in the process of writing the analysis
that you will come to know which constructed variables are necessary,
and which subsets and alterations of the data are desired.

You should never attempt to create a ``canonical'' analysis dataset.
Each analysis dataset should be purpose-built to answer an analysis question.
It is typical to have many purpose-built analysis datasets:
there may be a \texttt{data-wide.dta},
\texttt{data-wide-children-only.dta}, \texttt{data-long.dta},
\texttt{data-long-counterfactual.dta}, and many more as needed.
Data construction should never be done in analysis files,
and since much data construction is specific to the planned analysis,
organizing analysis files by this method allows that level of specificity
to be introduced at the correct part of the process.
Then, when it comes time to finalize analysis files,
it is substantially easier to organize and prune that code
since there is no risk of losing construction code it depends on.


%------------------------------------------------

\section{Writing data analysis code}

Data analysis is the stage of the process to create research outputs.
This section will not include instructions on how to conduct specific analyses,
as these are highly specialized and require field and statistical expertise.\sidenote{
Some introductions to common code skills and analytical frameworks are:
\textit{R for Data Science} \url{https://r4ds.had.co.nz/};
\textit{A Practical Introduction to Stata} \url{https://scholar.harvard.edu/files/mcgovern/files/practical_introduction_to_stata.pdf};
\textit{Mostly Harmless Econometrics} \url{https://www.researchgate.net/publication/51992844_Mostly_Harmless_Econometrics_An_Empiricist's_Companion}; and
\textit{Causal Inference: The Mixtape} \url{http://scunning.com/mixtape.html}.}
Instead, we will outline the structure of writing analysis code,
assuming you have completed the process of data cleaning and construction.
(As mentioned above, typically you will continue to keep
the data-construction files open, and add to them as needed,
while writing analysis code.)

It is important to ignore the temptation to write lots of analysis
into one big, impressive, start-to-finish analysis file.
This is counterproductive because it subtly encourages poor practices,
such as not clearing the workspace or loading fresh data.
Every output -- every table or figure -- should have
a completely fresh workspace and load its data directly.
This encourages data manipulation to be done upstream (in construction),
and prevents you from accidentally writing pieces of code
that depend on each other, leading to the too-familiar
``run this part, then that part, then this part'' process.
Each output should be able to run completely independently
of all other code sections.
You can go as far as coding every output in a separate file.
There is nothing wrong with code files being short and simple.

To accomplish this, you will need to make sure that you
have an effective system of naming, organization, and version control.
Version control allows you to effectively manage the history of your code,
including tracking the addition and deletion of files.
This lets you get rid of code you no longer need
simply by deleting it in the working directory,
and allows you to recover those chunk easily
if you ever need to get back previous work.
Therefore, version control supports a tidy workspace.
(It also allows effective collaboration in this space,
but that is a more advanced tool not covered here.)

Organizing your workspace effectively is now essential.
\texttt{iefolder} will have created a \texttt{/Dofiles/Analysis/} folder;
you should feel free to add additional subfolders as needed.
It is completely acceptable to have folders for each task,
and compartmentalize each analysis as much as needed.
It is always better to have more code files open
than to have to scroll around repeatedly inside a given file.
Just like you named each of the analysis datasets,
each of the individual analysis files should be descriptively named.
Code such as \texttt{spatial-diff-in-diff.do},
\texttt{matching-villages.do}, and \texttt{summary-statistics.do}
are clear indicators of what each file is doing
and allow code to be found very quickly.
Eventually, you will need to compile a ``release package''
that links the outputs more structurally to exhibits,
but at this stage function is more important than structure.

Outputs should, whenever possible, be created in lightweight formats.
For graphics, \texttt{.eps} or \texttt{.tif} files are acceptable;
for tables, \texttt{.tex} is preferred (don't worry about making it
really nicely formatted until you are ready to publish).
Excel \texttt{.xlsx} files are also acceptable,
although if you are working on a large report
they will become cumbersome to update after revisions.
If you are writing your code in Git/GitHub,
you should output results into that directory,
not the directory where data is stored (ie, Dropbox).
This is because results will differ across different branches
and will constantly be re-writing each other:
you should have each result stored with the code that created it.

%------------------------------------------------

\section{Visualizing data}

Data visualization is increasingly popular,
but a great deal of it is very low quality.\cite{healy2018data,wilke2019fundamentals}
The default graphics settings in Stata, for example,
are pretty bad.\sidenote{Gray Kimbrough's
\textit{Uncluttered Stata Graphs} code is an excellent
default replacement that is easy to install.
\url{https://graykimbrough.github.io/uncluttered-stata-graphs/}}
Thankfully, graphics tools like Stata are highly customizable;
there is a fair amount of learning curve associated with
extremely-fine-grained adjustment,
but basic colors, alignments, and labelling tools
are well worth reading the graphics manual for.\sidenote{
\url{https://www.stata.com/manuals/g.pdf}}
The manual is worth skimming through in full, as it provides
many visual examples of what is possible and basic code examples
detailing how to accomplish it.

Graphics are hard to get right because you, as the analyst,
will already have a pretty good idea of what you are trying to convey.
Someone seeing the illustration for the first time,
by contrast, will have no idea what they are looking at.
Therefore a visual image has to be compelling in the sense
that it shows, first, that there is something interesting going on,
and compels the reader to figure out exactly what it is.
There are a variety of resources to help you
figure out how best to structure a given visual:
The Stata Visual Library has many examples of figures that communicate effectively.\sidenote{
\url{https://worldbank.github.io/Stata-IE-Visual-Library/}}
The Tapestry conference focuses on ``storytelling with data''.\sidenote{
\url{https://www.youtube.com/playlist?list=PLb0GkPPcZCVE9EAm9qhlg5eXMgLrrfMRq}}
\textit{Fundamentals of Data Visualization} provides extensive details on practical application;\sidenote{
\url{https://serialmentor.com/dataviz}}
as does \textit{Data Visualization: A Practical Introduction}.\sidenote{
\url{http://socvis.co}}




%------------------------------------------------
