%-----------------------------------------------------------------------------------------------

\begin{fullwidth}
Sampling, randomization, and power calculations are the core elements of experimental design.
\textbf{Sampling} and \textbf{randomization} determine
which units are observed and in which states.
Each of these processes introduces statistical noise
or uncertainty into the final estimates of effect sizes.
Sampling noise produces some probability of
selection of units to measure that will produce significantly wrong estimates, and
randomization noise produces some probability of
placement of units into treatment arms that does the same.
Power calculation is the method by which these probabilities of error are meaningfully assessed.
Good experimental design has high \textbf{power} -- a low likelihood that these noise parameters
will meaningfully affect estimates of treatment effects.

Not all studies are capable of achieving traditionally high power:
the possible sampling or treatment assignments may simply be fundamentally too noisy.
This may be especially true for novel or small-scale studies --
things that have never been tried before may be hard to fund or execute at scale.
What is important is that every study includes reasonable estimates of its power,
so that the evidentiary value of its results can be honestly assessed.
Demonstrating that sampling and randomization were taken seriously into consideration
before going to field lends credibility to any research study.
Using these tools to design the most highly-powered experiments possible
is a responsible and ethical use of donor and client resources,
and maximizes the likelihood that reported effect sizes are accurate.
\end{fullwidth}

%-----------------------------------------------------------------------------------------------

\section{Random processes in Stata}

Most experimental designs rely directly on random processes,
particularly sampling and randomization, to be executed in code.
The fundamental econometrics behind impact evaluation
depends on establishing that the observations in the sample
and any experimental treatment assignment processes are truly random.
Therefore, understanding and programming for sampling and randomization
is essential to ensuring that planned experiments
are correctly implemented in the field, so that the results
can be interpreted according to the experimental design.
(Note that there are two distinct concepts referred to here by ``randomization'':
the conceptual process of assigning units to treatment arms,
and the technical process of assigning random numbers in statistical software,
which is a part of all tasks that include a random component.\sidenote{
  \url{https://blog.stata.com/2016/03/10/how-to-generate-random-numbers-in-stata/}})

Randomization is challenging. It is deeply unintuitive for the human brain.
``True'' randomization is also nearly impossible to achieve for computers,
which are inherently deterministic. There are plenty of sources to read about this.\sidenote{
  \url{https://www.random.org/randomness/}}
For our purposes, we will focus on what you need to understand
in order to produce truly random results for your project using Stata,
and how you can make sure you can get those exact results again in the future.
This takes a combination of strict rules, solid understanding, and careful programming.
This section introduces the strict rules: these are non-negotiable (but thankfully simple).
The second section provides basic introductions to the tasks of sampling and randomization,
and the third introduces common varieties encountered in the field.
The fourth section discusses more advanced topics that are used
to analyze the random processes directly in order to understand their properties.
However, the needs you will encounter in the field will inevitably
be more complex than anything we present here,
and you will need to recombine these lessons to match your project's needs.

\subsection{Reproducibility in random Stata processes}

Reproducibility in statistical programming means that random results
can be re-obtained at a future time.
All random methods should be reproducible.\cite{orozco2018make}
Stata, like most statistical software, uses a \textbf{pseudo-random number generator}.
Basically, it has a really long ordered list of numbers with the property that
knowing the previous one gives you precisely zero information about the next one.
Stata uses one of these numbers every time it has a task that is non-deterministic.
In ordinary use, it will cycle through these numbers starting from a fixed point
every time you restart Stata, and by the time you get to any given script,
the current state and the subsequent states will be as good as random.\sidenote{
  \url{https://www.stata.com/manuals14/rsetseed.pdf}}
However, for true reproducible randomization, we need two additional properties:
we need to be able to fix the starting point so we can come back to it later;
and we need that starting point to be independently random from our process.
In Stata, this is accomplished through three command concepts:
\textbf{versioning}, \textbf{sorting}, and \textbf{seeding}.

\textbf{Versioning} means using the same version of the software.
If anything is different, the underlying randomization algorithms may have changed,
and it will be impossible to recover the original result.
In Stata, the \texttt{version} command ensures that the software algorithm is fixed.
We recommend using \texttt{version 13.1} for back-compatibility;
the algorithm was changed after Stata 14 but its improvements do not matter in practice.
(Note that you will \textit{never} be able to transfer a randomization to another software such as R.)
The \texttt{ieboilstart} command in \texttt{ietoolkit} provides functionality to support this requirement.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/ieboilstart}}
We recommend, you use \texttt{ieboilstart} at the beginning of your master do-file.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Master_Do-files}}
However, note that testing your do-files without running them
via the master do-file may produce different resuls:
Stata's \texttt{version} expires after execution just like a \texttt{local}.

\textbf{Sorting} means that the actual data that the random process is run on is fixed;
because numbers are assigned to each observation in sequence,
changing their order will change the result of the process.
A corollary is that the underlying data must be unchanged between runs:
you must make a fixed final copy of the data when you run a randomization for fieldwork.
In Stata, the only way to guarantee a unique sorting order is to use\texttt{isid [id\_variable], sort}. (The \texttt{sort , stable} command is insufficient.)
You can additional use the \texttt{datasignature} commannd to make sure the data is fixed.

\textbf{Seeding} means manually setting the start-point of the randomization algorithm.
You can draw a six-digit seed randomly by visiting \url{http://bit.ly/stata-random}.
There are many more seeds possible but this is a large enough set for most purposes.
In Stata, \texttt{set seed [seed]} will set the generator to that state.
You should use exactly one seed per randomization process:
what is important is that each of these seeds is truly random.
You will see in the code below that we include the source and timestamp for verification.
Any process that includes a random component is a random process,
including sampling, randomization, power calculation, and algorithms like bootstrapping.
Other commands may induce randomness or alter the seed without you realizing it,
so carefully confirm exactly how your code runs before finalizing it.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Randomization_in_Stata}}
To confirm that a randomization has worked well before finalizing its results,
save the outputs of the process in a temporary location,
re-run the code, and use \texttt{cf} or \texttt{datasignature} to ensure nothing has changed.

%-----------------------------------------------------------------------------------------------

\section{Sampling and randomization}

\subsection{Sampling}

\textbf{Sampling} is the process of randomly selecting units of observation
from a master list for data collection.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Sampling_\%26_Power_Calculations}}
  \index{sampling}
That master list may be called a \textbf{sampling universe}, a \textbf{listing frame}, or something similar.
We refer to it as a \textbf{master data set}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Master_Data_Set}}
because it is the authoritative source for the existence and fixed characteristics
of each of the units that may be surveyed.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Unit_of_Observation}}
The master data set indicates how many individuals are eligible for data collection,
and therefore contains statistical information about the likelihood that each will be chosen.

The simplest form of random sampling is \textbf{uniform-probability random sampling}.
This means that every observation in the master data set
has an equal probability of being included in the sample.
The most explicit method of implementing this process
is to assign random numbers to all your potential observations,
order them by the number they are assigned,
and mark as `sampled' those with the lowest numbers, to the desired proportion.
(In general, we will talk about sampling proportions rather than numbers of observations.
Sampling specific numbers of observations is complicated and should be avoided,
because it will make the probability of selection very hard to calculate.)
There are a number of shortcuts to doing this process,
but they all use this method as the starting point,
so you should become familiar with exactly how this method works.

Almost all of the relevant considerations for sampling come from two sources:
deciding what population, if any, a sample is meant to represent (including subgroups);
and deciding that different individuals should have different probabilities
of being included in the sample.
These should be determined in advance by the study design,
since otherwise the sampling process will not be clear,
and the interpretation of measurements is directly linked to who is included in them.
Often, data collection can be designed to keep complications to a minimum,
so long as it are carefully thought through from this perspective.
Ex post changes to the study scope using a sample drawn for a different purpose
usually involve tedious calculations of probabilities and should be avoided.

\section{Randomization}

\textbf{Randomization} is the process of assigning units to some kind of treatment program.
Most of the Stata commands shown for sampling can be directly transferred to randomization,
since randomization is also a process of splitting a sample into groups.
Where sampling determines whether a particular individual
will be observed at all in the course of data collection,
randomization determines what state each individual will be observed in.
Randomizing a treatment guarantees that, \textit{on average},
the treatment will not be correlated with anything it did not cause.\cite{duflo2007using}
Causal inference from randomization therefore depends on a specific counterfactual:
that the units who recieved the treatment program might not have done so.
Therefore, controlling the exact probability that each individual receives treatment
is the most important part of a randomization process,
and must be carefully worked out in more complex designs.

Just like sampling, the simplest form of randomization is a uniform-probability process.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Randomization_in_Stata}}
Sampling typically has only two possible outcomes: observed and unobserved.
Randomization, by contrast, often involves multiple possible results
which each represent various varieties of treatments to be delivered;
in some cases, multiple treatment assignments are intended to overlap in the same sample.
Complexity can therefore grow very quickly in randomization
and it is doubly important to fully understand the conceptual process
that is described in the experimental design,
and fill in any gaps in the process before implmenting it in Stata.

Some types of experimental designs necessitate that randomization be done live in the field.
It is possible to do this using survey software or live events.
These methods typically do not leave a record of the randomization,
so particularly when the experiment is electronic,
it is best to execute the randomization in advance and preload the results.
Even when randomization absolutely cannot be done in advance, it is still useful
to build a corresponding model of the randomization process in Stata
so that you can conduct statistical analysis later
including checking for irregularities in the field assignment.
Understanding that process will also improve the ability of the team
to ensure that the field randomization process is appropriately designed and executed.

%-----------------------------------------------------------------------------------------------

\section{Clustering and stratification}

For a variety of experimental and theoretical reasons,
the actual sampling and randomization processes we need to perform
are rarely as straightforward as a uniform-probability draw.
We may only be able to implement treatment on a certain group of units
(such as a school, a firm, or a market)
or we may want to ensure that minority groups appear
in either our sample or in specific treatment groups.
The most common methods used in real studies are \textbf{clustering} and \textbf{stratification}.
They allow us to control the randomization process with high precision,
which is often necessary for appropriate inference,
particularly when samples or subgroups are small.\cite{athey2017econometrics}
(By contrast, re-randomizing or resampling are never appropriate for this.)
These techniques can be used in any random process;
their implementation is nearly identical in both sampling and randomization.

\subsection{Clustering}

Many studies collect data at a different level of observation than the randomization unit.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Unit_of_Observation}}
For example, a policy may only be able to affect an entire village,
but the study is interested in household behavior.
This type of structure is called \textbf{clustering},\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Multi-stage_(Cluster)_Sampling}}
and the groups in which units are assigned to treatment are called clusters.
The same principle extends to sampling:
it may be infeasible to decide whether to test individual children
within a single classroom, for example.

Clustering is procedurally straightforward in Stata,
although it typically needs to be performed manually.
To cluster sampling or randomization,
\texttt{preserve} the data, keep one observation from each cluster
using a command like \texttt{bys [cluster] : keep if _n == 1}.
Then sort the data and set the seed, and generate the random assignment you need.
Save the assignment in a separate dataset or a \texttt{tempfile},
then \texttt{restore} and \texttt{merge} the assignment back on to the original dataset.

When sampling or randomization is conducted using clusters,
the clustering variable should be clearly identified
since it will need to be used in subsequent statistical analysis.
Namely, standard errors for these types of designs must be clustered
at the level at which the randomization was clustered.\sidenote{
  \url{https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle}}
This accounts for the design covariance within the cluster --
the information that if one individual was observed or treated there,
the other members of the clustering group were as well.

\subsection{Stratification}

\texttt{Stratification} is a study design component
that breaks the full set of observations into a number of subgroups
before performing randomization within each subgroup.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Stratified_Random_Sample}}
This has the effect of ensuring that members of each subgroup
are included in all groups of the randomization process,
since it is possible that a global randomization
would put all the members of a subgroup into just one of the outcomes.
In this context, the subgroups are called \textbf{strata}.

Manually implementing stratified randomization in Stata is prone to error.
In particular, it is difficult to precisely account
for the interaction of strata sizes with multiple treatment arms.
Even for a very simple design, the method of randomly ordering the observations
will often create very skewed assignments.
This is especially true when a given stratum contains a small number of clusters,
and when there are a large number of treatment arms,
since the strata will rarely be exactly divisible by the number of arms.\cite{carril2017dealing}
The user-written \texttt{randtreat} command properly implements stratification.
However, the options and outputs (including messages) from the command should be carefully reviewed
so that you understand exactly what has been implemented.
Notably, it is extremely hard to target precise numbers of observations
in stratified designs, because exact allocations are rarely round fractions
and the process of assigning the leftover ``misfit'' observations
imposes an additional layer of randomization above the specified division.

Whenever stratification is used for randomization,
the analysis of differences within the strata (especially treatment effects)
requires a control in the form of an indicator variable for all strata (fixed effects).
This accounts for the fact that randomizations were conducted within the strata,
comparing units to the others within its own strata by correcting for the local mean.
Stratification is typically used for sampling
in order to ensure that individuals with various types will be observed;
no adjustments are necessary as long as the sampling proportion is constant across all strata.
One common pitfall is to vary the sampling or randomization \textit{probability}
across different strata (such as ``sample/treat all female heads of household'').
If this is done, you must calculate and record the exact probability
of inclusion for every unit, and re-weight observations accordingly.
The exact formula depends on the analysis being performed,
but is usually related to the inverse of the likelihood of inclusion.


%-----------------------------------------------------------------------------------------------

\section{Power calculation and randomization inference}

The fundamental contribution of sampling to the power of a research design is this:
if you randomly sample a set number of observations from a set frame,
there are a large -- but fixed -- number of sample sets which you may draw.\sidenote{\url{https://davegiles.blogspot.com/2019/04/what-is-permutation-test.html}}
From any large group, you can find some possible sample sets
that have higher-than-average values of some measure;
similarly, you can find some sets that have lower-than-average values.
The variation of these values across the range of all possible sample sets is what creates
\textbf{sampling noise}, the uncertainty in statistical estimates caused by sampling.
\index{sampling noise}

\subsection{Sampling error and randomization noise}

Portions of this noise can be reduced through design choices
such as clustering and stratification.
In general, all sampling requires \textbf{inverse probability weights}.
These are conceptually simple in that the weights for each individual must be precisely the inverse of the probability
with which that individual is included in the sample, but may be practically difficult to calculate for complex methods.
When the sampling probability is uniform, all the weights are equal to one.
Sampling can be structured such that subgroups are guaranteed to appear in a sample:
that is, you can pick ``half the level one facilities and half the level two facilities'' instead of
``half of all facilities''. The key here is that, \textit{for each facility},
the probability of being chosen remains the same -- 0.5.
By contrast, a sampling design that chooses unbalanced proportions of subgroups
has changed the probability that a given individual is included in the sample,
and needs to be reweighted in case you want to calculate overall average statistics.\sidenote{\url{http://blogs.worldbank.org/impactevaluations/tools-of-the-trade-when-to-use-those-sample-weights}}

The sampling noise in the process that we choose
determines the size of the confidence intervals
for any estimates generated from that sample.\sidenote{\url{https://economistjourney.blogspot.com/2018/06/what-is-sampling-noise.html}}
In general, for any underlying distribution,
the Central Limit Theorem implies that
the distribution of variation across the possible samples is exactly normal.
Therefore, we can use what are called \textbf{asymptotic standard errors}
to express how far away from the true population parameters our estimates are likely to be.
These standard errors can be calculated with only two datapoints:
the sample size and the standard deviation of the value in the chosen sample.
The code below illustrates the fact that sampling noise
has a distribution in the sense that some actual executions of the sample
give estimation results far from the true value,
and others give results close to it.

The output of the code is a distribution of means in sub-populations of the overall data.
This distribution is centered around the true population mean,
but its dispersion depends on the exact structure of the population.
We use an estimate of the population variation taken from the sample
to assess how far away from that true mean any given sample draw is:
essentially, we estimate the properties of the distribution you see now.
With that estimate, we can quantify the uncertainty in estimates due to sampling noise,
calculate precisely how far away from the true mean
our sample-based estimate is likely to be,
and report that as the standard error of our point estimates.
The interpretation of, say, a 95\% \textbf{confidence interval}
\index{confidence interval}
in this context is that, conditional on our sampling strategy,
we would anticipate that 95\% of future samples from the same distribution
would lead to parameter estimates in the indicated range.
This approach says nothing about the truth or falsehood of any hypothesis.

With this program created and executed,
the next part of the code, shown below,
can set up for reproducibility.
Then it will call the randomization program by name,
which executes the exact randomization process we programmed
to the data currently loaded in memory.
Having pre-programmed the exact randomization does two things:
it lets us write this next code chunk much more simply,
and it allows us to reuse that precise randomization as needed.
Specifically, the user-written \texttt{ritest} command\sidenote{\url{http://hesss.org/ritest.pdf}}
\index{randomization inference}
allows us to execute a given randomization program repeatedly,
visualize how the randomization might have gone differently,
and calculate alternative p-values against null hypotheses.
These \textbf{randomization inference}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Randomization\_Inference}} significance levels may be very different
than those given by asymptotic confidence intervals,
particularly in small samples (up to several hundred clusters).

After generating the ``true'' treatment assignment,
\texttt{ritest} illustrates the distribution of correlations
that randomization can spuriously produce
between \texttt{price} and \texttt{treatment}.

\subsection{Power calculations}

When we have decided on a practical sampling and randomization design,
we next assess its \textbf{power}.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Power_Calculations_in_Stata}}
\index{power}
Statistical power can be described in a few ways,
each of which has different uses.\sidenote{\url{http://www.stat.columbia.edu/~gelman/stuff_for_blog/chap20.pdf}}
The purpose of power calculations is not to
demonstrate that a study is ``strong'',
but rather to identify where the strengths and weaknesses
of your design are located, so that readers
can correctly assess the evidentiary value of
any results (or null results) in the analysis.
This should be done before going to the field,
across the entire range of research questions
your study might try to answer,
so you know the relative tradeoffs you will face
by changing your sampling and randomization schemes
and can select your final strategy appropriately.

The classic definition of power is
``the likelihood that your design detects a significant treatment effect,
given that there is a non-zero true effect in reality''.
Here we will look at two useful practical applications
of that definition and show what quantitative results can be obtained.
We suggest doing all power calculations by simulation;
you are very unlikely to be able to determine analytically
the power of your study unless you have a very simple design.
Stata has some commands that can calculate power for
very simple designs -- \texttt{power} and \texttt{clustersampsi} --
but they will not answer most of the practical questions
that complex experimental designs require.

To determine the \textbf{minimum detectable effect}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Minimum_Detectable_Effect}}
\index{minimum detectable effect}
-- the smallest true effect that your design can detect --
conduct a simulation for your actual design.
The structure below uses fake data,
but you should use real data whenever it is available,
or you will have to make assumptions about the distribution of outcomes.
If you are willing to make even more assumptions,
you can use one of the built-in functions.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Power_Calculations_in_Stata}}

Here, we use an outer loop to vary the size of the assumed treatment effect,
which is later used to simulate outcomes in a ``true''
data-generating process (DGP).
The data generating process is written similarly to a
regression model, but it is a separate step.
A data generating process is the ``truth''
that the regression model is trying to estimate.
If our regression results are close to the DGP,
then the regression is ``good'' in the sense we care about.
For each of 100 runs indexed by \texttt{i},
we ask the question: If this DGP were true,
would our design have detected it in this draw?
We run our planned regression model including all controls and store the result,
along with an indicator of the effect size we assumed.

When we have done this 100 times for each effect size we are interested in,
we have built a large matrix of regression results.
That can be loaded into data and manipulated directly,
where each observation represents one possible randomization result.
We flag all the runs where the p-value is significant,
then visualize the proportion of significant results
from each assumed treatment effect size.
Knowing the design's sensitivity to a variety of effect sizes
lets us calibrate whether the experiment we propose
is realistic given the constraints of the amount of data we can collect.

Another way to think about the power of a design
is to figure out how many observations you need to include
to test various hypotheses -- the \textbf{minimum sample size}.
This is an important practical consideration
when you are negotiating funding or submitting proposals,
as it may also determine the number of treatment arms
and types of hypotheses you can test.
The basic structure of the simulation is the same.
Here, we use the outer loop to vary the sample size,
and report significance across those groups
instead of across variation in the size of the effect.

Using the concepts of minimum detectable effect
and minimum sample size in tandem can help answer a key question
that typical approaches to power often do not.
Namely, they can help you determine what tradeoffs to make
in the design of your experiment.
Can you support another treatment arm?
Is it better to add another cluster,
or to sample more units per cluster?
Simultaneously, planning out the regression structure
in advance saves a lot of work once the data is in hand,
and helps you think critically about what you are really testing.
It also helps you to untangle design issues before they occur.
Therefore, simulation-based power analysis is often more of a design aid
than an output for reporting requirements.
At the end of the day, you will probably have reduced
the complexity of your experiment significantly.
For reporting purposes, such as grantwriting and registered reports,
simulation ensures you will have understood the key questions well enough
to report standard measures of power once your design is decided.

\subsection{Randomization inference}

% code

\codeexample{replicability.do}{./code/replicability.do}

\codeexample{randomization-cf.do}{./code/randomization-cf.do}

\codeexample{simple-sample.do}{./code/simple-sample.do}

\codeexample{sample-noise.do}{./code/sample-noise.do}

\codeexample{randomization-program-1.do}{./code/randomization-program-1.do}

\codeexample{randomization-program-2.do}{./code/randomization-program-2.do}

\codeexample{randtreat-strata.do}{./code/randtreat-strata.do}

\codeexample{randtreat-clusters.do}{./code/randtreat-clusters.do}

\codeexample{minimum-detectable-effect.do}{./code/minimum-detectable-effect.do}

\codeexample{minimum-sample-size.do}{./code/minimum-sample-size.do}
