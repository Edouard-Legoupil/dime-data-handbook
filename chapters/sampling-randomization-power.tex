%-----------------------------------------------------------------------------------------------

\begin{fullwidth}
	Sampling and randomized assignment are two core elements of research design.
	In experimental methods, sampling and randomization directly determine
	the set of individuals who are going to be observed
	and what their status will be for the purpose of effect estimation.
	Since we only get one chance to implement a given experiment,
	we need to have a detailed understanding of how these processes work
	and how to implement them properly.
	In quasi-experimental methods, sampling determines what populations the study
	will be able to make meaningful inferences about,
	and randomization analyses simulate counterfactual possibilities;
	what would have happened in the absence of the event.
	Demonstrating that sampling and randomization were taken into consideration
	before going to field lends credibility to any research study.
	
	All random processes introduce statistical noise
	or uncertainty into the final estimates of effect sizes.
	Choosing one sample from all the possibilities produces some probability of
	choosing a group of units that are not, in fact, representative.
	Choosing one final randomization assignment similarly produces some probability of
	creating groups that are not good counterfactuals for each other.
	Power calculation and randomization inference
	are the main methods by which these probabilities of error are assessed.
	These analytical dimensions are particularly important in the initial phases of development research --
	typically conducted well before any actual field work occurs --
	and often have implications for feasibility, planning, and budgeting.
	
	In this chapter, we first cover the necessary practices to ensure that random processes are reproducible.
	We next turn to how to implement sampling and randomized assignment,
	both for simple, uniform probability cases, and more complex designs,
	such as those that require clustering or stratification.
	We include code examples so the guidance is concrete and applicable.
	The last section discusses power calculations and randomization inference,
	and how both are important tools to critically and quantitatively assess different
	sampling and randomization designs and to make optimal choices when planning studies.
	
	
	%from old design chapter
	
	Research design is the process of defining the methods and data
	that will be used to answer a specific research question.
	
	
	Thinking through research design before starting data work is important for several reasons. You will save a lot of time by understanding the way
	your data needs to be organized
	in order to be able to produce meaningful analytics throughout your projects.
	
	
\end{fullwidth}

%-----------------------------------------------------------------------------------------------



\section{Planning your data based on your research design}

Your project's data needs will differ depending on what research design your project use. 
There are already many great resources on research design,
so this chapter will only cover how they impact your data needs and related tools. 
We assume that the reader have some level of familiarity with the resarch designs mentioned here. 
If not go and read the appendix XYZ where you have more details and links to even more details.  

All research designs discussed here compare a group that received some treatment\sidenote{
	\textbf{Treatment:} The general word for the event we evaluate the impact of. 
	That event can be receiving training or cash transfer from a program, experience a natural disaster etc.}
to another, counterfactual group.\sidenote{
	\textbf{Counterfactual:} A statistical description of what would have happened to specific individuals in an alternative scenario,
	for example, a different treatment assignment outcome.}
\index{counterfactual} 
The key assumption is that every
person, facility, or village (or whatever the unit of intervention is)
has two possible states: their outcomes if they do not receive some treatment
and their outcomes if they do receive that treatment.
The impact of the treatment is defined as the difference in these two states
,however, we can never observe the same unit in our data
in both their treated and untreated states simultaneously.

Instead, the treated observations are compared to observations
that are \textit{statistically similar} in a \textbf{control} group. 
Different research designs have different methods 
for how the \textit{statistically similar} control observation are identified. 
You need a PhD in economics to fully navigate this, 
but this section we will cover how that affect how you should plan your data accordingly.

\textit{Statistically similarity} can be tested using \textbf{balance checks} 
where the similarity between treatment and control groups can be formally tested. 
Since this test is so common, 
we have developed a Stata command called \texttt{iebaltab}\sidenote{
	\url{https://dimewiki.worldbank.org/iebaltab}}
in the package \texttt{ietoolkit} that generates a table of balance checks.


\subsection{Identification of control groups in different research designs}

%%%%% Experimental 

In \textbf{experimental research designs} the research team can control which part of the studied population will get the treatment. 
This is often done by random assignment\sidenote{For example \textbf{randomized control trial (RCT) --}
	\url{https://dimewiki.worldbank.org/Randomized_Control_Trials}}
\index{randomized control trials}
where a subset of the eligible population is randomized to receive the treatment (see later in this chapter for how to implement randomization). 
The intuition is that if everyone in the eligible population is assigned group randomly, then they will, on average, be statistically similar.

To randomly assign treatment you need data over all individuals in the eligible population. 
This can be a census when running a traditional household survey, 
but it can also be anything from all companies in a country's tax records
to all Twitter accounts that liked a tweet.
It is important that the completeness of the eligible population in your data has no bias, 
for example missing many poor households, 
as that bias will then be included in your research design and your results will have the same bias.


%%%%% Quasi Experimental 

\textbf{Quasi-experimental} research designs,\sidenote{
	\url{https://dimewiki.worldbank.org/Quasi-Experimental_Methods}}
by contrast, are based on events not controlled by the research team. Instead, they rely on ``experiments of nature'',
in which natural variation in treatment can be argued to approximate randomization. 

Unlike carefully planned experimental designs,
quasi-experimental designs typically require the extra luck
of having access to data collected at the right times and places
to exploit events that occurred in the past,
or having the ability to collect data in a time and place
where an event that produces causal identification occurred or will occur.

Therefore, these methods often use either secondary data,
including administrative data or other new classes of routinely-collected information.

%%%%% Regression discontinuity

\textbf{Regression discontinuity (RD)} designs exploit sharp breaks or limits
in policy designs to separate a single group of potentially eligible recipients
into comparable groups of individuals who do and do not receive a treatment.\sidenote{
	\url{https://dimewiki.worldbank.org/Regression_Discontinuity}} \index{regression discontinuity}
Common examples are test score thresholds and income thresholds, 
where the individuals on one side of that threshold receive a treatment and bot those on the other side do not.\sidenote{
	\url{https://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn}}

The intuition is that, on average, a large number of individuals immediately on one side of the threshold
is \textit{statistically similar} to the individuals on the other side, 
and the only difference is the treatment. 
In your data you need a unambiguous way to define which observations were above or below the cut-off.
Apart from that requirement there is no special need for your data,
and you can use any type of primary or secondary data in your RD design.


%%%%% IV regression

\textbf{Instrumental variables (IV)} designs, unlike the previous approaches,
assume that the treatment not directly identifiable.
Similar to RD designs,
IV designs focus on a subset of the variation in treatment take-up, 
but whereas RD designs have a ``sharp'' or binary cut-off,
IV designs are ``fuzzy'', meaning that they do not completely determine
the treatment status but instead influence the \textit{probability of treatment}.

You will need variables in your data that can be used to estimate the probibality of treatment. 
These variables are called \textit{instruments}. 
Testing that a variable is a valid instrument is a non-trivial and important task
that is outside the scope of this book.
Additionally, you will have to use special regressions to estimate the effect of the treatment.
Stata has a built in command called \texttt{ivregress} but a perhaps more popular approach is to use the user-written command \texttt{ivreg2}.

%%%%% Matching

\textbf{Matching}\sidenote{
	\url{https://dimewiki.worldbank.org/Matching}}
methods use observable characteristics to construct pairs of treatment and control groups
so that the observations in each pair is as similar as possible. \index{matching}
The treatment and control pair can either consist of exactly two observations (one-to-one),
or it can be a pair of two groups of observations where either both groups have more than one observation (many-to-many),
or where only one group have more than one observation (one-to-many)

The matching can be done before the random assignment, 
so that treatment can be randomized within each treatment pair. 
This is a type of experimental design. 
If no control observations were identified before the treatment, 
then matching can be used to ex-post identify a control group, 
by finding the observations that are the most similar to the treated group.
It is very difficult to test the validity of an ex-post matching 
as one would have to prove that the difference in outcome is 
due to the impact of the treatment and not due to the groups not being similar enough.

A valid matching must be made on data that is not related to the treatment 
or anything that the treatment could have affected in any way. 
Many matching algorithms can only match on a single variable, 
so you first have to turn many variables into a single by using an index or a propensity score.
The \texttt{iematch} command in the \texttt{ietoolkit} package developed by DIME Analytics 
produces matchings based on a single continuous matching variable.\sidenote{
	\url{https://dimewiki.worldbank.org/iematch}}


%-----------------------------------------------------------------------------------------------


\subsection{One observation or multiple observations over time}

Most of the research designs in the previous section can be implemented 
using data collected either only after the treatment, 
or using data collected at multiple time periods, 
for example before and after the treatment. 
The advantage of multiple points in time is 
that you can control for trends and each observations initial status.

A study that observes data in only one time period is called 
a \textbf{cross-sectional study} is any type of study. 
This type of data is easy to collect and handle because
you do not need to track individuals across time. 
Instead, the challenge in a cross-sectional study is to
show that the control group is indeed a valid counterfactual to the treatment group.

A study that observes data in multiple time periods is called \texttt{longitudinal} and
can either be a \textbf{repeated cross-sections study} or a \textbf{panel data study} 
depending on if the same sample is used in the multiple observations.
In repeated cross-sections, each successive round of data collection use a new random sample
of observations from the treatment and control groups, 
but in a panel data study the same observations are tracked and observed each round.

While you can control for both trends 
and each observation's initial status (only \textbf{panel data studies}), 
longitudinal data requires you to have a way to track the observations over period of time.
This is challenging if you are collecting your own data, 
and will require a lot more effort during field work as detailed tracking records must be kept.

You must keep track of the attrition rate which is the share of observations not observed in the follow up data.
It is common that the observations not possible to track can be correlated with the outcome you study,
for example poorer households live in more informal dwellings,
patients with worse health conditions might not survive until the follow up etc. 
If this is the case then your results might only be an effect of your remaining sample
being a subset of the original sample that were better or worse off from the beginning.
A balance table could provide insights if your attrition observations were better or worse off 
originally compared to the rest of the sample, 
but since the sample sizes in that balance table is likely to be big, 
it might be difficult to know how to interpret those results.


\subsection{Creating a data map}

So far we have only talked about each data set by itself. 
In most projects however, more than one data set is needed to answer the research question. 
This can be multiple rounds of the same data set, 
this can be a combination of admin data, survey data, web scraping and monitor data, 
and a multitude of other complex combinations of data. 

Often you also have different levels of unit of observation. 
It is common that your unit of observation for your analysis is different from the unit of observation in the treatment assignment.
A typical examples of that could be that schools are divided into treatment and control, 
but what we study is the performance of the students. 
If this is the case then you need to have one data set measuring the performance of the students, 
and another data set indicating which schools received the treatment. 
And you need a way to link those two data sets to answer questions about the treatments impact on the students performance.

You might think that you are able to keep all of these details in your head, and perhaps you are right. 
But, unless your project has unusually simple data requirements, 
we know that you are wrong if you think that your whole research team can have the same understanding, 
at all times, of all the data sets required.
The only way to make sure that the team has the same share understanding is to write it down, 
and that is called a \textbf{data map}.\sidenote{
	\url{https://dimewiki.worldbank.org/data_map} (TO BE CREATED)} 
\index{data maps}

A data map is more than just a list of data sets. 
Its purpose is also specify the characteristics of those data sets. 
Some basic characteristics should filled in
Example of those characteristics are: 
What is the unit of observation? 
What is the name of the ID variable that uniquely and fully identifies each data set? 
Should any data sets be possible to merge one-to-one? 
And if so, will the have the same unique identifier?
Should any data sets be possible to merge many-to-one (for example school and student)? 
And if so, what identifier will be used?
Solving all of these questions before you acquire any data, 
and making sure that the full research team knows where to find this info,
will save you a ton of time later on.

Other characteristics should be updated as the project progresses.
Example of those characteristics are:
Where will the data be obtain from?
Where in the project folder are the raw original data stored and where are back ups stored?
See the wiki article linked to above for an template that you can use for your project.



\section{Taking research design into account when planing your data work}

The two most important steps during your projects planning stage,
to translate the research design of your project into your data work,
is sampling and treatment assignment.
Therefore, understanding and correctly implementing sampling and treatment assignment
is essential to ensuring that planned experiments
are correctly carried out in the field, so that your results
are valid interpretations to your research question given your research design.

Not all projects need to do sampling and treatment assignment. 
Sampling is almost always required when you collect your own data, 
as it is often prohibitively expensive to collect data on all individuals. 
However, if the data you use is, for example, the tax filings of all companies in a country,
then you can estimate the treatment effect using the full population and not a sample of it. 
Such big data might need a cluster of computers to process,
so you might still want to know how make a representative sample, 
so that you can develop your code on your computer before running it on an expensive computer cluster.
Treatment assignment is only applicable if your research design is experimental,
and the research team can control which individuals receive the treatment. 

While you might not yet have a population data set to sample from, 
or a complete list of eligible observation to randomize assignment on, 
this is still the time to understand those task 
and what you need to think about as you acquire those data sets.
If you do not need to create a sample or assign treatment, 
then you could skip the rest of this chapter.

% what we mean when we talk about randomization

Most representative sampling and experimental treatment assignment rely on randomization.
There are two distinct random processes referred to here:
the conceptual process of assigning units to a sample or a treatment arm,
and the technical process of assigning random numbers in statistical software,
which is a part of all tasks that include a random component.\sidenote{
	\url{https://blog.stata.com/2016/03/10/how-to-generate-random-numbers-in-stata}}
Randomization in a software is non-trivial and its mechanics are unintuitive for the human brain.
``True'' randomization is also nearly impossible to achieve for computers,
which are inherently deterministic.\sidenote{
	\url{https://www.random.org/randomness}}
The principles of randomization we will outline, apply not just to random sampling, random assignment, 
it applies to all processes that have a random components, for example, simulations and bootstrapping.



\subsection{Implementing random processes reproducibly in Stata}

% what is means for randomization to be reproducible
For statistical programming to be considered reproducible, it must be possible for the outputs of random processes
to be re-obtained at a future time.\cite{orozco2018make}
For our purposes, we will focus on what you need to understand
in order to produce truly random results for your project using Stata,
and how you can make sure you can get those exact results again in the future.
This takes a combination of strict rules, solid understanding, and careful programming.
This section introduces the strict rules: these are non-negotiable (but thankfully simple). 
At the end of the section, 
we provide a do-file that provides a concrete example of how to implement these principles. 

% what Stata is doing when it generates random numbers
Stata, like most statistical software, uses a \textbf{pseudo-random number generator}.
Basically, it has a pre-calculated really long ordered list of numbers with the property that
knowing the previous one gives you precisely zero information about the next one, i.e. a list of random numbers.
Stata uses one number from this list every time it has a task that is non-deterministic.
In ordinary use, it will cycle through these numbers starting from a fixed point
every time you restart Stata, and by the time you get to any given script,
the current state and the subsequent states will be as good as random.\sidenote{
	\url{https://www.stata.com/manuals14/rsetseed.pdf}}
However, for true reproducible randomization, we need two additional properties:
we need to be able to fix the starting point so we can come back to it later;
and we need to ensure that the starting point is independently random from our process.
In Stata, this is accomplished through three command concepts:
\textbf{versioning}, \textbf{sorting}, and \textbf{seeding}.

% rule 1: versioning
\textbf{Versioning} means using the same version of the software each time you run the random process.
If anything is different, the underlying list of random numbers may have changed,
and it will be impossible to recover the original result.
In Stata, the \texttt{version} command ensures that the list of random numbers is fixed.\sidenote{
	At the time of writing we recommend using \texttt{version 13.1} for backward compatibility;
	the algorithm used to create this list of random numbers was changed after Stata 14 but its improvements do not matter in practice.
	You will \textit{never} be able to reproduce a randomization in a different software,
	such as moving from Stata to R or vice versa.}
The \texttt{ieboilstart} command in \texttt{ietoolkit} provides functionality to support this requirement.\sidenote{
	\url{https://dimewiki.worldbank.org/ieboilstart}}
We recommend you use \texttt{ieboilstart} at the beginning of your master do-file.\sidenote{
	\url{https://dimewiki.worldbank.org/Master_Do-files}}
However, testing your do-files without running them
via the master do-file may produce different results,
since Stata's \texttt{version} setting expires after each time you run your do-files.

% rule 2: sorting
\textbf{Sorting} means that the actual data that the random process is run on is fixed.
Because random numbers are assigned to each observation in row-by-row starting from
the top row,
changing their order will change the result of the process.
Since the exact order must be unchanged, the underlying data itself must be unchanged as well between runs.
This means that if you expect the number of observations to change (for example increase during
ongoing data collection) your randomization will not be stable unless you split your data up into
smaller fixed datasets where the number of observations does not change. You can combine all
those smaller datasets after your randomization.
In Stata, the only way to guarantee a unique sorting order is to use
\texttt{isid [id\_variable], sort}. (The \texttt{sort, stable} command is insufficient.)
You can additionally use the \texttt{datasignature} command to make sure the
data is unchanged.

% rule 3: seeding
\textbf{Seeding} means manually setting the start-point in the list of random numbers.
The seed is a number that should be at least six digits long and you should use exactly
one unique, different, and randomly created seed per randomization process.\sidenote{You
	can draw a uniformly distributed six-digit seed randomly by visiting \url{https://bit.ly/stata-random}.
	(This link is a just shortcut to request such a random seed on \url{https://www.random.org}.)
	There are many more seeds possible but this is a large enough set for most purposes.}
In Stata, \texttt{set seed [seed]} will set the generator to that start-point. In R, the \texttt{set.seed} function does the same.
To be clear: you should not set a single seed once in the master do-file,
but instead you should set a new seed in code right before each random process.
The most important thing is that each of these seeds is truly random,
so do not use shortcuts such as the current date or a seed you have used before.
You should also describe in your code how the seed was selected.
Other commands may induce randomness in the data or alter the seed without you realizing it,
so carefully confirm exactly how your code runs before finalizing it.\sidenote{
	\url{https://dimewiki.worldbank.org/Randomization_in_Stata}}

% testing randomization reproducibility
To confirm that a randomization has worked well before finalizing its results,
save the outputs of the process in a temporary location,
re-run the code, and use \texttt{cf} or \texttt{datasignature} to ensure
nothing has changed. It is also advisable to let someone else reproduce your
randomization results on their machine to remove any doubt that your results
are reproducible.

\codeexample{replicability.do}{./code/replicability.do}



\subsection{Sampling}

% sampling universe: the master data set
\textbf{Sampling} is the process of randomly selecting units of observation
from a master list of individuals to be included in data collection.\sidenote{
	\url{https://dimewiki.worldbank.org/Sampling_\%26_Power_Calculations}}
\index{sampling}
That master list may be called a \textbf{sampling universe}, a \textbf{listing frame}, or something similar.
We recommend that this list be organized in a \textbf{master dataset}\sidenote{
	\url{https://dimewiki.worldbank.org/Master_Data_Set}},
creating an authoritative source for the existence and fixed
characteristics of each of the units that may be surveyed.\sidenote{
	\url{https://dimewiki.worldbank.org/Unit_of_Observation}}
The master dataset indicates how many individuals are eligible for data collection,
and therefore contains statistical information about the likelihood that each will be chosen.

% implement uniform-probability random sampling
The simplest form of random sampling is \textbf{uniform-probability random sampling}.
This means that every observation in the master dataset
has an equal probability of being included in the sample.
The most explicit method of implementing this process
is to assign random numbers to all your potential observations,
order them by the number they are assigned,
and mark as ``sampled'' those with the lowest numbers, up to the desired proportion.
(In general, we will talk about sampling proportions rather than numbers of observations.
Sampling specific numbers of observations is complicated and should be avoided,
because it will make the probability of selection very hard to calculate.)
There are a number of shortcuts to doing this process,
but they all use this method as the starting point,
so you should become familiar with exactly how it works.
The do-file below provides an example of how to implement uniform-probability sampling in practice. 

\codeexample{simple-sample.do}{./code/simple-sample.do}

% what to think about before sampling
Almost all of the relevant considerations for sampling come from two sources:
deciding what population, if any, a sample is meant to represent (including subgroups);
and deciding that different individuals should have different probabilities
of being included in the sample.
These should be determined in advance by the research design,
since otherwise the sampling process will not be clear,
and the interpretation of measurements is directly linked to who is included in them.
Often, data collection can be designed to keep complications to a minimum,
so long as it is carefully thought through from this perspective.
Ex post changes to the study scope using a sample drawn for a different purpose
usually involve tedious calculations of probabilities and should be avoided.


\subsection{Randomized assignment}

% what does randomized assignment mean
\textbf{Randomization}, in this context, is the process of assigning units into treatment arms.
Most of the code processes used for randomization are the same as those used for sampling,
since randomization is also a process of splitting a sample into groups.
Where sampling determines whether a particular individual
will be observed at all in the course of data collection,
randomization determines if each individual will be observed
as a treatment unit or used as a counterfactual.
Randomizing a treatment guarantees that, \textit{on average},
the treatment will not be correlated with anything but the results of that treatment.\cite{duflo2007using}
Causal inference from randomization therefore depends on a specific counterfactual:
that the units who received the treatment program
could just as well have been randomized into the control group.
Therefore, controlling the exact probability that each individual receives treatment
is the most important part of a randomization process,
and must be carefully worked out in more complex designs.

% How randomization code differs from sampling code
Just like sampling, the simplest form of randomization is a uniform-probability process.\sidenote{
	\url{https://dimewiki.worldbank.org/Randomization_in_Stata}}
Sampling typically has only two possible outcomes: observed and unobserved.
Randomization, by contrast, often involves multiple possible results
which each represent different varieties of treatments to be delivered;
in some cases, multiple treatment assignments are intended to overlap in the same sample.
Complexity can therefore grow very quickly in randomization
and it is doubly important to fully understand the conceptual process
that is described in the experimental design,
and fill in any gaps before implementing it in code.
The do-file below provides an example of how to implement a simple random assignment of multiple treatment arms. 

\codeexample{simple-multi-arm-randomization.do}{./code/simple-multi-arm-randomization.do}

% Randomizing in the field
Some types of experimental designs necessitate that randomization results be revealed in the field.
It is possible to do this using survey software or live events, such as a live lottery.
These methods typically do not leave a record of the randomization,
so particularly when the experiment is done as part of data collection,
it is best to execute the randomization in advance and preload the results.
Even when randomization absolutely cannot be done in advance, it is still useful
to build a corresponding model of the randomization process in Stata
so that you can conduct statistical analysis later
including checking for irregularities in the field assignment.
Understanding that process will also improve the ability of the team
to ensure that the field randomization process is appropriately designed and executed.


%-----------------------------------------------------------------------------------------------

\section{Implementing complex research design}

% the cases discussed so far are the most simple, but not the most common
For a variety of experimental and theoretical reasons,
the actual sampling and randomization processes we need to perform
are rarely as straightforward as a uniform-probability draw.
We may only be able to implement treatment on a certain group of units
(such as a school, a firm, or a market)
or we may want to ensure that minority groups appear
in either our sample or in specific treatment groups.
The most common methods used in real studies are \textbf{clustering} and \textbf{stratification}.
They allow us to control the randomization process with high precision,
which is often necessary for appropriate inference,
particularly when samples or subgroups are small.\cite{athey2017econometrics}
These techniques can be used in any random process;
their implementation is nearly identical in both sampling and randomization.

\subsection{Clustering}

% clustering is needed when unit of observation in the data is different from the randomization unit
Many studies observe data at a different level than the randomization unit.\sidenote{
	\url{https://dimewiki.worldbank.org/Unit_of_Observation}}
For example, a policy may be implemented at the village level,
but the outcome of interest for the study is behavior changes at the household level.
This type of structure is called \textbf{clustering},\sidenote{
	\url{https://dimewiki.worldbank.org/Multi-stage_(Cluster)_Sampling}}
and the groups in which units are assigned to treatment are called clusters.
The same principle extends to sampling:
it may be be necessary to observe all the children
in a given teacher's classroom, for example.

% How to implement randomization with clusters
Clustering is procedurally straightforward in Stata,
although it typically needs to be performed manually.
To cluster a sampling or randomization,
create or use a dataset where each cluster unit is an observation,
randomize on that dataset, and then merge back the results.
When sampling or randomization is conducted using clusters,
the clustering variable should be clearly identified
since it will need to be used in subsequent statistical analysis.
Namely, standard errors for these types of designs must be clustered
at the level at which the randomization was clustered.\sidenote{
	\url{https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle}}
This accounts for the design covariance within the cluster --
the information that if one individual was observed or treated there,
the other members of the clustering group were as well.

\subsection{Stratification}

% what is stratification
\textbf{Stratification} is a research design component
that breaks the full set of observations into a number of subgroups
before performing randomization within each subgroup.\sidenote{
	\url{https://dimewiki.worldbank.org/Stratified_Random_Sample}}
This has the effect of ensuring that members of each subgroup
are included in all groups of the randomization process,
since it is possible that a global randomization
would put all the members of a subgroup into just one of the treatment arms.
In this context, the subgroups are called \textbf{strata}.

% using randtreat for stratified randomization
Manually implementing stratified randomization in Stata is prone to error.
In particular, it is difficult to precisely account
for the interaction of strata sizes with multiple treatment arms.
Even for a very simple design, the method of randomly ordering the observations
will often create very skewed assignments.\sidenote{\url
	{https://blogs.worldbank.org/impactevaluations/tools-of-the-trade-doing-stratified-randomization-with-uneven-numbers-in-some-strata}}
This is especially true when a given stratum contains a small number of clusters,
and when there are a large number of treatment arms,
since the strata will rarely be exactly divisible by the number of arms.\cite{carril2017dealing}
The user-written \texttt{randtreat} command properly implements stratification, 
as shown in the do-file below. 
However, the options and outputs (including messages) from the command should be carefully reviewed
so that you understand exactly what has been implemented.
Notably, it is extremely hard to target precise numbers of observations
in stratified designs, because exact allocations are rarely round fractions
and the process of assigning the leftover ``misfit'' observations
imposes an additional layer of randomization above the specified division.

\codeexample{randtreat-strata.do}{./code/randtreat-strata.do}

% analyzing stratified experiments
Whenever stratification is used for randomization,
the analysis of differences within the strata (especially treatment effects)
requires a control in the form of an indicator variable for all strata (fixed effects).
This accounts for the fact that randomizations were conducted within the strata,
comparing units to the others within its own strata by correcting for the local mean.
Stratification is typically used for sampling
in order to ensure that individuals with relevant characteristics will be observed;
no adjustments are necessary as long as the sampling proportion is constant across all strata.
One common pitfall is to vary the sampling or randomization \textit{probability}
across different strata (such as ``sample/treat all female heads of household'').
If this is done, you must calculate and record the exact probability
of inclusion for every unit, and re-weight observations accordingly.\sidenote{
	\url{https://blogs.worldbank.org/impactevaluations/tools-of-the-trade-when-to-use-those-sample-weights}}
The exact formula depends on the analysis being performed,
but is usually related to the inverse of the likelihood of inclusion.

%-----------------------------------------------------------------------------------------------

\section{Assessing validity of data work}

% sampling error, randomization noise and the need for power calcs
Both sampling and randomization are noisy processes:
they are random, after all, so it is impossible to predict the result in advance.
By design, we know that the exact choice of sample or treatment
will be uncorrelated with our key outcomes,
but this lack of correlation is only true ``in expectation'' --
that is, across a large number of randomizations.
In any \textit{particular} randomization,
the correlation between the sampling or randomization and the outcome variable
is guaranteed to be \textit{nonzero}:
this is called the \textbf{in-sample} or \textbf{finite-sample correlation}.

Since we know that the true correlation
(over the ``population'' of potential samples or randomizations)
is zero, we think of the observed correlation as an \textbf{error}.
In sampling, we call this the \textbf{sampling error},
and it is defined as the difference between the true population parameter
and the observed mean due to chance selection of units.\sidenote{
	\url{https://economistjourney.blogspot.com/2018/06/what-is-sampling-noise.html}}
In randomization, we call this the \textbf{randomization noise},
and define it as the difference between the true treatment effect
and the estimated effect due to placing units in groups.
The intuition for both measures is that from any group,
you can find some possible subsets that have higher-than-average values of some measure;
similarly, you can find some that have lower-than-average values.
Your sample or randomization will inevitably fall in one of these categories,
and we need to assess the likelihood and magnitude of this occurrence.\sidenote{
	\url{https://davegiles.blogspot.com/2019/04/what-is-permutation-test.html}}
Power calculation and randomization inference are the two key tools to doing so.

\subsection{Power calculations}

% why to do power calculations
Power calculations report the likelihood that your experimental design
will be able to detect the treatment effects you are interested in.
This measure of \textbf{power} can be described in various different ways,
each of which has different practical uses.\sidenote{
	\url{https://www.stat.columbia.edu/~gelman/stuff_for_blog/chap20.pdf}}
The purpose of power calculations is to identify where the strengths and weaknesses
of your design are located, so you know the relative tradeoffs you will face
by changing your randomization scheme for the final design.
They also allow realistic interpretations of evidence:
low-power studies can be very interesting,
but they have a correspondingly higher likelihood
of reporting false positive results.

% intuition of power concets: MDE and minimum sample size
The classic definition of power
is the likelihood that a design detects a significant treatment effect,
given that there is a non-zero treatment effect in reality.
This definition is useful retrospectively,
but it can also be re-interpreted to help in experimental design.
There are two common and useful practical applications
of that definition that give actionable, quantitative results.
The \textbf{minimum detectable effect (MDE)}\sidenote{
	\url{https://dimewiki.worldbank.org/Minimum_Detectable_Effect}}
is the smallest true effect that a given research design can detect.
This is useful as a check on whether a study is worthwhile.
If, in your field, a ``large'' effect is just a few percentage points
or a fraction of a standard deviation,
then it is nonsensical to run a study whose MDE is much larger than that.
This is because, given the sample size and variation in the population,
the effect needs to be much larger to possibly be statistically detected,
so such a study would not be able to say anything about the effect size that is practically relevant.
Conversely, the \textbf{minimum sample size} pre-specifies expected effect sizes
and tells you how large a study's sample would need to be to detect that effect,
which can tell you what resources you would need to avoid that problem.

% stata commands for power calcs
Stata has some commands that can calculate power analytically for
very simple designs -- \texttt{power} and \texttt{clustersampsi} --
but they will not answer most of the practical questions
that complex experimental designs require.\sidenote{
	\url{https://dimewiki.worldbank.org/Power_Calculations_in_Stata}}
We suggest doing more advanced power calculations by simulation,
since the interactions of experimental design,
sampling and randomization,
clustering, stratification, and treatment arms
quickly becomes complex.
Furthermore, you should use real data on the population of interest whenever it is available,
or you will have to make assumptions about the distribution of outcomes.

% why you should do simulations instead
Together, the concepts of minimum detectable effect
and minimum sample size can help answer a key question
that typical approaches to power often do not.
Namely, they can help you determine what trade-offs to make
in the design of your experiment.
Can you support another treatment arm?
Is it better to add another cluster,
or to sample more units per cluster?
Simultaneously, planning out the regression structure
in advance saves a lot of work once the data is in hand,
and helps you think critically about what you are really testing.
It also helps you to untangle design issues before they occur.
Therefore, simulation-based power analysis is often more of a design aid
than an output for reporting requirements.
At the end of the day, you will probably have reduced
the complexity of your experiment significantly.
For reporting purposes, such as grant writing and registered reports,
simulation ensures you will have understood the key questions well enough
to report standard measures of power once your design is decided.

% wrap up
Not all studies are capable of achieving traditionally high power:
sufficiently precise sampling or treatment assignments may not be available.
This may be especially true for novel or small-scale studies --
things that have never been tried before may be hard to fund or execute at scale.
What is important is that every study includes reasonable estimates of its power,
so that the evidentiary value of its results can be assessed.

\subsection{Randomization inference}

% what is randomization inference
Randomization inference is used to analyze the likelihood
that the randomization process, by chance,
would have created a false treatment effect as large as the one you observed.
Randomization inference is a generalization of placebo tests,
because it considers what the estimated results would have been
from a randomization that did not in fact happen in reality.
Randomization inference is particularly important
in quasi-experimental designs and in small samples,
because these conditions usually lead to the situation
where the number of possible \textit{randomizations} is itself small.
It is difficult to draw a firm line on when a study is ``large enough'',
but even when a treatment effect is real,
it can take ten or one hundred times more sample size
than the minimum required power
to ensure that the average statistically significant estimate
of the treatment effect is accurate.\cite{schwarzer2015small}
In those cases, we cannot rely on the usual assertion
(a consequence of the Central Limit Theorem)
that the variance of the treatment effect estimate is normal,
and we therefore cannot use the ``asymptotic'' standard errors from Stata.
We also don't need to: these methods were developed when it was very costly
to compute large numbers of combinations using computers,
and in most cases we now have the ability to do these calculations reasonably quickly.

% randomization inference in stata: ritest
Instead, we directly simulate a large variety of possible alternative randomizations.
Specifically, the user-written \texttt{ritest} command\sidenote{
	\url{http://hesss.org/ritest.pdf}}
allows us to execute a given randomization repeatedly,
visualize how the randomization might have gone differently,
and calculate empirical p-values for the effect size in our sample.
After analyzing the actual treatment assignment,
\texttt{ritest} illustrates the distribution of false correlations
that this randomization could produce by chance
between outcomes and treatments.
The randomization-inference p-value is the number of times
that a false effect was larger than the one you measured,
and it is interpretable as the probability that a program with no effect
would have given you a result like the one actually observed.
These randomization inference\sidenote{
	\url{https://dimewiki.worldbank.org/Randomization\_Inference}}
significance levels may be very different
than those given by asymptotic confidence intervals,
particularly in small samples (up to several hundred clusters).

% why you should use randomization inference
Randomization inference can therefore be used proactively during experimental design.
As long as there is some outcome data usable at this stage,
you can use the same procedure to examine the potential treatment effects
that your exact design is likely to produce.
The range of these effects, again, may be very different
from those predicted by standard approaches to power calculation,
and randomization inference further allows visual inspection of results.
If there is significant heaping at particular result levels,
or if results seem to depend dramatically on the placement of a small number of individuals,
randomization inference will flag those issues before the experiment is fielded
and allow adjustments to the design to be made.