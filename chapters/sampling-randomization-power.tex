%-----------------------------------------------------------------------------------------------

\begin{fullwidth}

  Thinking through research design before starting data work
  is important for several reasons.
	You will save a lot of time by understanding how data needs to be organized
	in order to be able to produce meaningful analytics throughout your projects.
  This section is based around creating and using
  the very first datasets you need to create to achieve this,
  which are the master datasets for every unit of observation in your study.
  These datasets will serve three key authoritative functions.
  First, they will list all of the units who are eligible for the study,
  and enable you to map data collected or received from the field to the research design.
  Second, they will allow you to define the different groups you intend to study,
  including treatment and control groups and other comparisons of interest.
  Finally, in designs where your team has direct control over interventions or other field work,
  they will allow you to complete sampling and randomization tasks
  before collecting further data from the field.





	In this chapter, we first cover the necessary practices to ensure that random processes are reproducible.
	We next turn to how to implement sampling and randomized assignment,
	both for simple, uniform probability cases, and more complex designs,
	such as those that require clustering or stratification.
	We include code examples so the guidance is concrete and applicable.
	The last section discusses power calculations and randomization inference,
	and how both are important tools to critically and quantitatively assess different
	sampling and randomization designs and to make optimal choices when planning studies.


  This section covers how these different designs
  determine how you should plan data accordingly.
  Your project's data requirements will depend on your project's research design.
  \index{research designs}
  There are many published resources about research design,
  so this chapter will focus specifically on how the design impacts data requirements
  and related tools.
  We assume you have a working familiarity
  with the research designs mentioned here.
  For more details, you can reference Appendix XYZ,
  where you will find more details and specific sources for each design.


	%from old design chapter

	Research design is the process of defining the methods and data
	that will be used to answer a specific research question.

\end{fullwidth}

%-----------------------------------------------------------------------------------------------

\section{Translating research design to master data sets}

In most projects, more than one dataset is needed to answer the research question.
This could be multiple survey rounds,
integration of different existing data sources (e.g. administrative data,
web scraping, implementation monitoring, etc)
or a multitude of complex combinations of the above.
For example, you may have different levels of unit-of-observation\sidenote{
	\url{https://dimewiki.worldbank.org/Unit\_of\_Observation}},
or the units of observation you do have (or their subunits)
may vary from round to round.

However your study is structured, you need to know how to link all the datasets you obtain
and analyze the relationships between the units that appear in them
to answer all your research questions.
You might think that you are able to keep all of the relevant details in your head,
but your whole research team is unlikely to have the same understanding,
at all times, of all the datasets required.
The only way to make sure that the full team shares the same understanding
is to create \textit{master datasets} and a \textit{data map}.


\subsection{Creating master datasets and a data map}

A \textbf{master dataset}\sidenote{
	\url{https://dimewiki.worldbank.org/Master\_Data\_Set}},
\index{master data sets}
at the very least,
details all project-wide time-invariant information
about all units that could appear in the data
and their relationship to the research design,
like sampling and treatment status once those are determined.

You should create a master dataset
for each unit-of-observation
important to the research.
A unit used for any significant research activity,
like data collection or data analysis, 
must always be considered an important unit-of-observation.
Therefore, any unit
that will be used in sampling or treatment assignment,
must have a master dataset,
and that master dataset -- not field data --
should be used when sampling or assigning treatment.

You also need to record how all datasets with each unit-of-observation
will link or merge with each other as needed.
This linking scheme is called a \textbf{data map}\sidenote{
	\url{https://dimewiki.worldbank.org/data\_map} (TO BE CREATED)}.
\index{data maps} 
A data map is more than just a list of datasets.
Its purpose is to specify the characteristics and linkages of those datasets.
Therefore the master datasets must include fully unique ID variables.\sidenote{
	\url{https://dimewiki.worldbank.org/ID\_Variable\_Properties}}
The master two datasets should indicate whether datasets should be merge one-to-one,
for examples merging baseline data and endline data that use the same unit-of-observation.
Or whether two datasets should be merged many-to-one, 
for example school admin data merged with student data.
Your data map must indicate which ID variables can used and how to merge data sets.
It is common that admin data use IDs 
that are different than the project IDs, 
and the linkage between should be clear from your master dataset.

Solving all of these questions before you acquire any data,
and making sure that the full research team knows where to find this information,
will save a ton of time later on,
and will increase the quality of your research.

The data map should also include meta-data about the handling of all information.
These characteristics may be updated as the project progresses.
For example, you will need to note the source of each dataset,
as well as the project folder where raw original data is stored
and where the back-ups for the each raw dataset are stored.

Some of the characteristics in your master datasets and your data map
should be filled in during the planning stage,
but both of them should be active resources 
that are updated as your project evolves.
Finally, your master data should not include any missing values. 
If the information is missing for one unit, 
then the reason should always be indicated with a code.
An example for such reason could be that a unit was not included in the treatment assignment
as it was not sampled in the first place.

\subsection{Defining study comparisons using master data}

Your research design will determine what statistical comparisons you need
to estimate in your analytical work.
The research designs discussed here compare a group that received
some kind of \textbf{treatment}\sidenote{
	\textbf{Treatment:} The general word for the evaluated intervention or event.
	This includes being offered training or cash transfer from a program, experiencing a natural disaster etc.}
against a counterfactual control group.\sidenote{
	\textbf{Counterfactual:} A statistical description of what would have happened
	to specific individuals in an alternative scenario,
	for example, a different treatment assignment outcome.}
\index{counterfactual}
The key assumption is that each
person, facility, or village (or whatever the unit of intervention is)
had two possible states: their outcome if they did receive the treatment
and their outcome if they did not receive that treatment.
The average impact of the treatment is defined as
the difference between these two states averaged over all units.

However, we can never observe the same unit
in both the treated and untreated state simultaneously,
so we cannot calculate these differences directly.
Instead, the treated group is compared to a control group
that is statistically indistinguishable,
which makes the average impact of the treatment
mathematically equivalent to the difference in averages between the groups.

The rest of this section covers how the data requirement differs between different research design.
What does not differ however is that the authoritative source,
for which unit is a treated unit and which is a control unit,
should always be one or several variables in your master dataset. 
You will often have to merge that data to other datasets, 
but that is an easy task if you created a data map.

%%%%% Experimental design

In \textbf{experimental research designs},\sidenote{
	For example, \textbf{randomized control trials (RCTs) --}
	\url{https://dimewiki.worldbank.org/Randomized\_Control\_Trials}}
\index{randomized control trials} \index{experimental research designs}
the research team is in control which members
of the studied population will receive the treatment.
This is typically done by a random assignment process
in which a subset of the eligible population
is randomly selected to receive the treatment
(see the \textit{Randomized treatment assignment} section
later in this chapter for how to implement this).
The intuition is that if everyone in the eligible population
is assigned at random to either the treatment or control group,
then the two groups will, on average, be statistically indistinguishable,
The treatment will therefore not be correlated with anything
but the impact of that treatment.\cite{duflo2007using}
The random assignment should be done using the master data,
and the result should be saved there before being merged to other data sets.

%%%%% Quasi-experimental design

\textbf{Quasi-experimental research designs},\sidenote{
	\url{https://dimewiki.worldbank.org/Quasi-Experimental\_Methods}}
\index{quasi-experimental research designs}
by contrast, are based on events not controlled by the research team.
Instead, they rely on ``experiments of nature'',
in which natural variation in treatment can be argued to approximate randomization.
You must have a way to measure this natural variation,
and how the variation is approximated as randomization should be documented in your master dataset.
Unlike carefully planned experimental designs,
quasi-experimental designs typically require the extra luck
of having access to data collected at the right times and places
to exploit events that occurred in the past.
Therefore, these methods often use either secondary data,
including administrative data or other new classes of routinely-collected information,
and it is important that your data map documents 
how that data is merged to any other of your data.


%%%%% Regression discontinuity

\textbf{Regression discontinuity (RD)}\sidenote{
	\url{https://dimewiki.worldbank.org/Regression\_Discontinuity}}
\index{regression discontinuity}
designs exploit sharp breaks or limits
in policy designs to separate a single group of potentially eligible recipients
into comparable groups of individuals who do and do not receive a treatment.
Common examples are test score thresholds and income thresholds,
where the individuals on one side of some threshold receive
a treatment but those on the other side do not.\sidenote{
	\url{https://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn}}

The intuition is that, on average,
individuals immediately on one side of the threshold
are statistically indistinguishable from the individuals on the other side,
and the only difference is receiving the treatment.
In your data you need an unambiguous way
to define which observations were above or below the cutoff.
The cutoff is often a continuous variable 
that used to divide the sample into two or more groups. 
These variables should be saved in your master dataset.

%%%%% IV regression

\textbf{Instrumental variables (IV)}\sidenote{
	\url{https://dimewiki.worldbank.org/Instrumental\_Variables}}
\index{instrumental variables}
designs, unlike the previous approaches,
assume that the treatment effect is not directly identifiable.
Similar to RD designs,
IV designs focus on a subset of the variation in treatment take-up.
Where RD designs use a \textit{sharp} or binary cutoff,
IV designs are \textit{fuzzy}, meaning that the input does not completely determine
the treatment status, but instead influence the \textit{probability of treatment}.

You will need variables in your data
that can be used to estimate the probability of treatment for each unit.
These variables are called \textit{instruments}.
Testing that a variable is a valid instrument is a non-trivial and important task
that is outside the scope of this book.
In IV designs, instead of the ordinary regression estimator,
a special version called two-stage least squares (2SLS) is used
to estimate the impact of the treatment.
Stata has a built-in command called \texttt{ivregress},
and another popular implementation is the user-written command \texttt{ivreg2}.
In an IV design there is no specific requirement of a variable saved
to the master dataset, 
as the probability to treatment is estimated as a part of the analysis.


%%%%% Matching

\textbf{Matching}\sidenote{
	\url{https://dimewiki.worldbank.org/Matching}}
methods use observable characteristics to construct
sets or pairs of treatment and control units
so that the observations in each set are as similar as possible. \index{matching}
These sets can either consist of exactly one treatment and one control observation (one-to-one),
or it can be a set observations where either
both groups have more than one observation represented (many-to-many),
or where only one group has more than one observation included (one-to-many).
By now you can probably guess that the result of the matching needs to be saved in the master dataset.
This is best done by assigning an ID to each matching pair, 
and create a variable in the master dataset with the ID for the pair each unit belongs to.

The matching can even be done before the random assignment,
so that treatment can be randomized within each matching set.
This is a type of experimental design.
Furthermore, if no control observations were identified before the treatment,
then matching can be used to ex-post identify a control group,
by finding the observations that are the most similar to the treated group.
It is very difficult to test the validity of an ex-post matching
as one would have to show that the difference in outcome is
due to the impact of the treatment and not due to the groups not being similar enough.

A valid matching must be made on data that is not related to the treatment
or anything that the treatment could have affected in any way.
Many matching algorithms can only match on a single variable,
so you first have to turn many variables into a single varaible
by using an index or a propensity score.\sidenote{
	\url{https://dimewiki.worldbank.org/Propensity\_Score\_Matching}}
The \texttt{iematch}\sidenote{
	\url{https://dimewiki.worldbank.org/iematch}}
command in the \texttt{ietoolkit} package developed by DIME Analytics
produces matchings based on a single continuous matching variable.

%-----------------------------------------------------------------------------------------------
\subsection{Other data requirements}

A study that observes data in only one time period is called
a \textbf{cross-sectional study}.
\index{cross-sectional data}
This type of data is relatively easy to collect and handle because
you do not need to track individuals across time.
Instead, the challenge in a cross-sectional study is to
show that the control group is indeed a valid counterfactual to the treatment group.

A study that observes data in multiple time periods is called \textbf{longitudinal} and
can either be a \textbf{repeated cross-sections} or a \textbf{panel data} study
depending on whether the same observations are included in each of the time periods.
\index{longitudinal data}\index{repeated cross-sectional data}\index{panel data}

In repeated cross-sections,
each successive round of data collection uses a new random sample
of observations from the treatment and control groups,
but in a panel data study the same observations are tracked and included each round.

You must keep track of the attrition rate in longitudinal data,
which is the share of observations not observed in follow-up data.
It is common that the observations not possible to track
can be correlated with the outcome you study.
For example, poorer households may live in more informal dwellings,
patients with worse health conditions might not survive to follow-up,
and so on.
If this is the case, then your results might only be an effect of your remaining sample
being a subset of the original sample that were better or worse off from the beginning.
A balance check could provide insights
as to whether the lost observations were systematically different
compared to the rest of the sample,
and there are a variety of methods for estimating treatment effects
with selective attrition.


MONITORING DATA


%-----------------------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------
\section{Implementing sampling and randomization designs}

Sampling and randomized assignment are two core elements of research design.
In experimental methods, sampling and randomization directly determine
the set of individuals who are going to be observed
and what their status will be for the purpose of effect estimation.
In quasi-experimental methods, sampling determines what populations the study
will be able to make meaningful inferences about,
and randomization analyses simulate counterfactual possibilities;
what would have happened in the absence of the event.
Randomization is used to ensure that a sample is representative and
that any treatment and control groups are statistically indistinguishable
after treatment assignment.

Randomization in statistical software is non-trivial
and its mechanics are unintuitive for the human brain.
The principles of randomization we will outline
apply not just to random sampling, random assignment,
but they apply to all statistical computing processes that have a random components,
such as simulations and bootstrapping.
Furthermore, all random processes introduce statistical noise
or uncertainty into the final estimates of effect sizes.
Choosing one sample from all the possibilities produces some probability of
choosing a group of units that are not, in fact, representative.
Choosing one final randomization assignment similarly produces some probability of
creating groups that are not good counterfactuals for each other.
\textit{Power calculation} and \textit{randomization inference}
are the main methods by which these probabilities of error are assessed.
These analytical dimensions are particularly important in the initial phases of development research --
typically conducted well before any actual field work occurs --
and often have implications for feasibility, planning, and budgeting.

%-----------------------------------------------------------------------------------------------
\subsection{Performing sampling and randomization}

% sampling universe: the master dataset
\textbf{Sampling} is the process of randomly selecting units of observation
from a list of individuals eligible to be included in data collection.\sidenote{
	\url{https://dimewiki.worldbank.org/Sample\_Size\_and\_Power\_Calculations}}
\index{sampling}
\textbf{Randomized treatment assignment} is the process of assigning observations into treatment arms,
and this is central to experimental research design.
Most of the code processes used for random assignment are the same as those used for sampling,
since it is also a process of splitting a a list of observations into groups.
Where sampling determines whether a particular individual
will be observed at all in the course of data collection,
random assignment determines if each individual will be observed
as a treatment observation or used as a counterfactual.
That list of units to sample or randomize from may be called a \textbf{sampling universe},
a \textbf{listing frame}, or something similar.
We recommend that this list is your \textbf{master dataset} discussed above.

% implement uniform-probability random sampling
The simplest form of randomization or sampling is \textbf{uniform-probability random sampling}.
This means that every observation in the master dataset
has an equal probability of being selected.
The most explicit method of implementing this process
is to assign random numbers to all your potential observations,
order them by the number they are assigned,
and mark as ``sampled'' or ``treated'' those with the lowest numbers, up to the desired proportion.
There are a number of shortcuts to doing this process,
but they all use this method as the starting point,
so you should become familiar with exactly how it works.
The do-file below provides an example of
how to implement uniform-probability sampling or randomization in practice.
This code uses a Stata built-in example dataset and is fully reproducible,
so anyone that runs this code in any version of Stata later than 13.1
(the version set in this code)
will get the exact same randomized results.

\codeexample{simple-sample.do}{./code/simple-sample.do}

Sampling typically has only two possible outcomes: observed and unobserved.
Similarly, a simple random assignment has two outcomes: treatment and control,
and the logic in the code would be identical to the sampling code example.
However, random assignment often involves multiple treatment arms
which each represent different varieties of treatments to be delivered;
in some cases, multiple treatment arms are intended to overlap in the same sample.
Complexity can therefore grow very quickly in random assignment
and it is doubly important to fully understand the conceptual process
that is described in the experimental design,
and fill in any gaps before implementing it in code.
The do-file below provides an example of how to implement
a simple random assignment of multiple treatment arms.

\codeexample{simple-multi-arm-randomization.do}{./code/simple-multi-arm-randomization.do}

%-----------------------------------------------------------------------------------------------

\subsection{Implementing random processes reproducibly}

% what it means for randomization to be reproducible
For statistical programming to be reproducible,
you must be able to re-obtain its outputs in the future.\cite{orozco2018make}
We will focus on what you need to do to produce
truly random results for your project,
to ensure you can get those exact results again.
This takes a combination of strict rules, solid understanding, and careful programming.
This section introduces the strict rules:
these are non-negotiable (but thankfully simple).
Stata, like most statistical software, uses a \textbf{pseudo-random number generator}
which, in ordinary research use, is as good as random.\sidenote{
	\url{https://dimewiki.worldbank.org/Randomization\_in\_Stata}}
However, for \textit{reproducible} randomization, we need two additional properties:
we need to be able to fix the sequence of numbers generated and
ensure that the first number is independently randomized.
In Stata, this is accomplished through three concepts:
\textbf{versioning}, \textbf{sorting}, and \textbf{seeding}.
We again use Stata in our examples,
but the same principles translate to all other programming languages.

% rule 1: versioning
\textbf{Versioning} means using the same version of the software each time you run the random process.
If anything is different, the underlying list of random numbers may have changed,
and it will be impossible to recover the original result.
In Stata, the \texttt{version} command ensures that the list of random numbers is fixed.\sidenote{
	At the time of writing, we recommend using \texttt{version 13.1} for backward compatibility;
	the algorithm used to create this list of random numbers was changed after Stata 14 but the improvements do not matter in practice.
	You will \textit{never} be able to reproduce a randomization in a different software,
	such as moving from Stata to R or vice versa.}
The \texttt{ieboilstart} command in \texttt{ietoolkit} provides functionality to support this requirement.\sidenote{
	\url{https://dimewiki.worldbank.org/ieboilstart}}
We recommend you use \texttt{ieboilstart} at the beginning of your master do-file.\sidenote{
	\url{https://dimewiki.worldbank.org/Master_Do-files}}
However, testing your do-files without running them
via the master do-file may produce different results,
since Stata's \texttt{version} setting expires after each time you run your do-files.

% rule 2: sorting
\textbf{Sorting} means that the actual data that the random process is run on is fixed.
Because random numbers are assigned to each observation row-by-row starting from
the top row,
changing their order will change the result of the process.
In Stata, the only way to guarantee a unique sorting order is to use
\texttt{isid [id\_variable], sort}.
(The \texttt{sort, stable} command is insufficient.)
Since the exact order must be unchanged,
the underlying data itself must be unchanged as well between runs.
This means that if you expect the number of observations to change
(for example to increase during ongoing data collection),
your randomization will not be stable unless you split your data up into
smaller fixed datasets where the number of observations does not change.
You can combine all
those smaller datasets after your randomization.


% rule 3: seeding
\textbf{Seeding} means manually setting the start point in the list of random numbers.
A seed is just a single number that specifies one of the possible start points.
It should be at least six digits long and you should use exactly
one unique, different, and randomly created seed per randomization process.\sidenote{You
	can draw a uniformly distributed six-digit seed randomly by visiting \url{https://bit.ly/stata-random}.
	(This link is a just shortcut to request such a random number on \url{https://www.random.org}.)
	There are many more seeds possible but this is a large enough set for most purposes.}
In Stata, \texttt{set seed [seed]} will set the generator
to the start point identified by the seed.
In R, the \texttt{set.seed} function does the same.
To be clear: you should not set a single seed once in the master do-file,
but instead you should set a new seed in code right before each random process.
The most important thing is that each of these seeds is truly random,
so do not use shortcuts such as the current date or a seed you have used before.
You should also describe in your code how the seed was selected.

% testing randomization reproducibility
Other commands may induce randomness in the data,
change the sorting order,
or alter the place of the random generator without you realizing it,
so carefully confirm exactly how your code runs before finalizing it.
To confirm that a randomization has worked correctly before finalizing its results,
save the outputs of the process in a temporary location,
re-run the code, and use \texttt{cf} or \texttt{datasignature} to ensure
nothing has changed. It is also advisable to let someone else reproduce your
randomization results on their machine to remove any doubt that your results
are reproducible.
Once the result of a randomization is used in the field,
there is no way to correct any mistakes.

\codeexample{reproducible-randomization.do}{./code/reproducible-randomization.do}

Some types of experimental designs necessitate
that random assignment results be revealed in the field.
It is possible to do this using survey software or live events, such as a live lottery.
These methods typically do not leave a record of the randomization,
as such can never be reproducible. If an experiment is done as part of data collection,
it is best to execute the randomization in advance and preload the results.
Even when randomization absolutely cannot be done in advance, it is still useful
to build a corresponding model of the randomization process in your statistical software
so that you can conduct statistical analysis later
including checking for irregularities in the field assignment.
Understanding that process will also improve the ability of the team
to ensure that the field randomization process is appropriately designed and executed.

%-----------------------------------------------------------------------------------------------

\subsection{Clustering or stratifying a sample or randomization}

% the cases discussed so far are the most simple, but not the most common
For a variety of reasons, actual sampling and randomization
is rarely as straightforward as a uniform-probability draw.
The most common variants are \textbf{clustering} and \textbf{stratification}.\cite{athey2017econometrics}
\textbf{Clustering} occurs when you observe data at a different level
than the unit of randomization or sampling.\sidenote{
	\url{https://dimewiki.worldbank.org/Unit\_of\_Observation}}
For example, a policy may be implemented at the village level,
or you may only be able to send enumerators at a fixed number of villages,
but the outcomes of interest for the study are measured at the household level.\sidenote{
	\url{https://dimewiki.worldbank.org/Multi-stage_(Cluster)_Sampling}}
\index{clustered randomization}
The groups in which units are assigned to treatment are called clusters.
% what is stratification
\textbf{Stratification} breaks the full set of observations into subgroups
before performing random assignment within each subgroup.\sidenote{
	\url{https://dimewiki.worldbank.org/Stratified\_Random\_Sample}}
\index{stratification}
The subgroups are called \textbf{strata}.
This ensures that members of every subgroup
are included in all groups of the random assignment process,
or that members of all groups are observed in the sample.
Without stratification, it is possible that randomization
would put all the members of a subgroup into just one of the treatment arms,
or fail to select them into the sample.
For both clustering and stratification,
implementation is nearly identical in both sampling and random assignment.

% How to implement randomization with clusters
Clustering is procedurally straightforward in Stata,
although it typically needs to be performed manually.
To cluster a sampling or random assignment,
create or use a dataset where each cluster unit is an observation,
randomize on that dataset, and then merge back the results.
When sampling or random assignment is conducted using clusters,
the clustering variable should be clearly identified in the master dataset
since it will need to be used in subsequent statistical analysis.
Namely, standard errors for these types of designs must be clustered
at the level at which the randomization was clustered.\sidenote{
	\url{https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle}}
This accounts for the design covariance within the cluster --
the information that if one individual was observed or treated there,
the other members of the clustering group were as well.

% using randtreat for stratified randomization
By contrast, implementing stratified designs is prone to error.
Even for a very simple design, the ordinary method of randomly ordering the observations
will often create very skewed assignments.\sidenote{\url
	{https://blogs.worldbank.org/impactevaluations/tools-of-the-trade-doing-stratified-randomization-with-uneven-numbers-in-some-strata}}
The user-written \texttt{randtreat} command properly implements stratification.\cite{carril2017dealing}
The options and outputs (including messages) from the command should be carefully reviewed
so that you understand exactly what has been implemented.
Notably, it is extremely hard to target precise numbers of observations
in stratified designs, because exact allocations are rarely round fractions
and the process of assigning the leftover ``misfit'' observations
imposes an additional layer of randomization above the specified division.

%-----------------------------------------------------------------------------------------------

\subsection{Power calculations for research design}

% sampling error, randomization noise and the need for power calcs
Both sampling and random assignment are noisy processes:
they are random, so it is impossible to predict the result in advance.
By design, we know that the exact choice of sample or treatment
will be uncorrelated with our key outcomes,
but this lack of correlation is only true ``in expectation'' --
that is, across a large number of randomizations.
In any \textit{particular} randomization,
the correlation between the sampling or random assignments and the outcome variable
is guaranteed to be \textit{nonzero}:
this is called the \textbf{in-sample} or \textbf{finite-sample correlation}.

Since we know that the true correlation
(over the ``population'' of potential samples or randomizations)
is zero, we think of the observed correlation as an \textbf{error}.
In sampling, we call this the \textbf{sampling error},
and it is defined as the difference between the true population parameter
and the observed mean due to chance selection of units.\sidenote{
	\url{https://economistjourney.blogspot.com/2018/06/what-is-sampling-noise.html}}
In randomization, we call this the \textbf{randomization noise},
and define it as the difference between the true treatment effect
and the estimated effect due to placing units in groups.
The intuition for both measures is that from any group,
you can find some possible subsets that have higher-than-average values of some measure;
similarly, you can find some that have lower-than-average values.
Your sample or randomization will inevitably fall in one of these categories,
and we need to assess the likelihood and magnitude of this occurrence.\sidenote{
	\url{https://davegiles.blogspot.com/2019/04/what-is-permutation-test.html}}
Power calculation and randomization inference are the two key tools to doing so.

% why to do power calculations
\textbf{Power calculations} report the likelihood that your experimental design
\index{power calculations}
will be able to detect the treatment effects you are interested in.\sidenote{
	\url{https://dimewiki.worldbank.org/Sample\_Size\_and\_Power\_Calculations}}
This measure of \textbf{power} can be described in various different ways,
each of which has different practical uses.\sidenote{
	\url{https://www.stat.columbia.edu/~gelman/stuff_for_blog/chap20.pdf}}
The purpose of power calculations is to identify where the strengths and weaknesses
of your design are located, so you know the relative tradeoffs you will face
by changing your randomization scheme for the final design.

The \textbf{minimum detectable effect (MDE)}\sidenote{
	\url{https://dimewiki.worldbank.org/Minimum_Detectable_Effect}}
is the smallest true effect that a given research design can detect.
This is useful as a check on whether a study is worthwhile.
If, in your field, a ``large'' effect is just a few percentage points
or a small fraction of a standard deviation,
then it is nonsensical to run a study whose MDE is much larger than that.
This is because, given the sample size and variation in the population,
the effect needs to be much larger to possibly be statistically detected,
so such a study would not be able to say anything about the effect size that is practically relevant.
Conversely, the \textbf{minimum sample size} pre-specifies expected effect sizes
and tells you how large a study's sample would need to be to detect that effect,
which can tell you what resources you would need to avoid that problem.

% what is randomization inference
\textbf{Randomization inference} is used to analyze the likelihood
\index{randomization inference}
that the randomization process, by chance,
would have created a false treatment effect as large as the one you observed.
Randomization inference is a generalization of placebo tests,
because it considers what the estimated results would have been
from a random assignment that did not in fact happen in reality.
Randomization inference is particularly important
in quasi-experimental designs and in small samples,
where the number of possible \textit{randomizations} is itself small.
Randomization inference can therefore be used proactively during experimental design,
to examine the potential spurious treatment effects your exact design is able to produce.
If there is significant heaping at particular result levels,
or if results seem to depend dramatically on the outcomes for a small number of individuals,
randomization inference will flag those issues before the experiment is fielded
and allow adjustments to the design to be made.
