%------------------------------------------------

\begin{fullwidth}

	Policy decisions are made every day using the results of development briefs and studies,
	and these have wide-reaching effects on the lives of millions.
	As the range of policy questions asked by researchers grows,
	so too does the scrutiny under which research methods and results are placed.
  Three major components make up this scrutiny:
  credibility, transparency, and reproducibility.
  These three components contribute to one simple idea:
  research should be high quality and well-documented.
  Research consumers,
  including the policy makers who will use the evidence it creates to make decisions,
  should be easily able to examine and recreate such evidence.
	In this framework, it is useful to think of research as a public service
  that requires researchers as a group to be accountable to their methods.
	This means acting to collectively protect the credibility of development research
	by following modern practices for research planning, transparency, and reproducibility.

  Across the social sciences, the open science movement has been fueled
  by concerns about the proliferation of low-quality research practices,
	data and code that are inaccessible to the public,
  analytical errors in major research papers,
	and in some cases even outright fraud.
  While the development research community has not yet
  experienced any major scandals,
  it has become clear that there are necessary improvements
	in the way that code and data are handled as part of open research.
  As a community, having common standards and practices
  for creating and sharing materials, code, and data with others
  will improve the value of the work we do.
	In this chapter, we outline principles and practices that help to ensure
	research consumers can be confident in the conclusions reached.
We discuss each of the three components - credibility, transparency, and reproducibility - in turn.
  We share our experience implementing best practices in open science research at DIME,
  and focus on practical guidance tested on projects across our portfolio.


\end{fullwidth}

%------------------------------------------------

\section{Developing a credible research project}

% Why development researchers should care about transparency
The evidentiary value of research is traditionally a function of design choices.\cite{angrist2010credibility,ioannidis2005most}
Was the research design sufficiently powered through its sampling and randomization?
Were the key research outcomes pre-specified or chosen ex-post?
How sensitive were the results to changes in specifications or definitions?
Reproducible and transparent methods are key to maintaining credibility
in these choices and avoiding serious errors.\cite{christensen2019transparent}
This is especially relevant for research that relies on original data sources,
from innovative big data sources to unique surveys.
One frequent target for critics of such research\cite{ioannidis2017power}
is the fact that most researchers have a lot of leeway
in selecting the projects, results, or outcomes they focus on
\textit{after} already having had the experience of implementing a project
or collecting data in the field,
which increases the likelihood of finding ``false positive''
results that are not true outside carefully-selected data.
Development researchers should take these concerns seriously.
Such flexibility can be a significant issue for the quality of evidence overall,
particularly if researchers believe that certain types of results
are substantially better for their careers or their publication chances.

This section presents three popular methods
for researchers to commit to particular research questions or methods,
and to avoid potential criticisms of cherry-picking results for publication:
registration, pre-analysis plans, and registered reports.
Each of these methods involves documenting all or part of a research study,
ideally before carrying out the study.
Registering studies provides notice that a study was attempted (even if never completed).
Pre-analysis plans are a more formal commitment
to use specific methods on particular questions.
Writing and releasing a pre-analysis plan
in advance of working with data is therefore often used to protect the credibility
of approaches that have a high likelihood of returning false results if decided ex post.
Finally, registered reports allow researchers to approach research planning itself
as a process at the level of a full peer review.
Registered reports enable close scrutiny of a project design,
a feedback and improvement process,
and a commitment from a publisher to publish the study
based on the credibility of the design, rather than the specific results.

% Pre-registration
\subsection{Registering research studies}

Registration of research studies is an increasingly common practice,
and more journals are beginning to require
the registration of studies they publish.\cite{vilhuber2020report}
Study registration intended to ensure that a complete record of research inquiry is easily available.\sidenote{
	\url{https://dimewiki.worldbank.org/Pre-Registration}}
Registering research studies ensures that future scholars can quickly
find out what work has been carried out on a given question,
even if some or all of the work done never results in formal publication.
Registration of studies is increasingly required by publishers
and can be done very quickly before, during, or after the study
with only basic information about the study purpose.
Some common registries at the time of writing this book are operated by the \textbf{AEA},\sidenote{\url{https://www.socialscienceregistry.org}}
\textbf{3ie},\sidenote{\url{https://ridie.3ieimpact.org}}
\textbf{eGAP},\sidenote{\url{https://egap.org/content/registration}}
and \textbf{OSF},\sidenote{\url{https://osf.io/registries}}.
They all have different target audiences and features,
so select one that is appropriate to your work.
\index{pre-registration}

Pre-registering studies \textit{before they start} is a further extension of this principle.\cite{nosek2018preregistration}
Registration of a study before it goes to field,
particularly when specific hypotheses are included in the registration,
provides a simple and low-effort way for researchers
to conclusively demonstrate that a particular line of inquiry
was not generated by the process of data collection or analysis itself.
Pre-registrations need not provide exhaustive details about how
a particular hypothesis will be approached; only that it will be.\sidenote{\url{https://datacolada.org/12}}
This can be highly valuable and requires very little additional investment.
As a result, the DIME team requires pre-registration of all studies
in a public database with at least some primary hypotheses prespecified,
prior to providing funding for impact evaluation research.

% Pre-analysis plans
\subsection{Writing pre-analysis plans}

If a researcher has a large amount of flexibility
to define how they approach a particular hypothesis,
study registration may not be sufficient to avoid the criticism of
``hypothesizing after the results are known'', or HARKing.\cite{kerr1998harking}
Examples of such flexibility include a diverse set of measures of an abstract concept;
future choices about sample inclusion or exclusion;
or decisions about how to construct derived indicators.
When the researcher is collecting a large amount of information
and has leverage over even a moderate number of these options,
it is almost guaranteed that they can come up with any result they like.\cite{gelman2013garden}

Pre-analysis plans (PAPs) can be used to assuage these concerns
\index{pre-analysis plan}
by fully specifying some set of analyses the researchers intend to conduct.\sidenote{
	\url{https://dimewiki.worldbank.org/Pre-Analysis_Plan}}
The pre-analysis plan should be written up in detail
for areas that are known to provide a large amount of leeway
for researchers to make later decisions,
particularly for things like interaction effects or subgroup analysis.
Pre-analysis plans shoud not, however, be viewed as binding the researcher's hands.\cite{olken2015promises}
Depending on what is known about the study at the time of writing,
pre-analysis plans can vary widely in the amount of detail they should include.\sidenote{
  \url{https://blogs.worldbank.org/impactevaluations/pre-analysis-plans-and-registered-reports-what-new-opinion-piece-does-and-doesnt}}
The core function of a PAP is to carefully and explicitly describe
one or more specific data-driven inquiries,
as specific formulations are often very hard to justify in retrospect
with data or projects that potentially provide many avenues to approach
a single theoretical question.
Anything outside the original plan is just as interesting and valuable
as it would have been if the the plan was never published;
but having pre-committed to the details of a particular inquiry makes its results
immune to a wide range of criticisms of specification searching or multiple testing.\cite{duflo2020praise}

% Registered reports

\subsection{Publishing registered reports}

\textbf{Registered reports}\sidenote{
	\url{https://blogs.worldbank.org/impactevaluations/registered-reports-piloting-pre-results-review-process-journal-development-economics}}
take the process of pre-specifying a complex research design
to the level of a formal publication.
In a registered report, a journal or other publisher
will accept a particular format of study description for publication,
typically then guaranteeing the acceptance of a later publication
that carries out the analysis described in the registered report.
While far stricter and more complex to carry out than
ordinary study registration or pre-analysis planning,
the registered report has the added benefit
of soliciting peer review and expert feedback
on the design and structure of the proposed study.

This process is in part meant to combat the ``file-drawer problem'',\cite{simonsohn2014p}
and ensure that researchers are transparent in the sense that
all promised results obtained from registered-report studies are actually published.
This approach has the advantage of pre-specifying in great detail
a complete research and analytical design,
and securing a commitment for publication regardless of the outcome.
This may be of special interest for researchers
studying events or programs where either there is a substantial risk
that they would either not be able to publish a null or negative result,
or where they may wish to avoid any pressure toward finding a particular result,
for example when the program or event is the subject of substantial social or political pressures.
As with pre-registration and pre-analysis,
nothing in a registered report should be understood
to prevent a researcher from pursuing additional avenures of inquiry
once the study is complete, either in the same or separate research outputs.

\section{Documenting and cataloguing transparent research}
Transparent research exposes not only the code,
but all research processes involved in developing the analytical approach.\sidenote{
	\url{https://www.princeton.edu/~mjs3/open_and_reproducible_opr_2017.pdf}}
This means that readers are able to judge for themselves whether the research was done well
and the decision-making process was sound.
If the research is well-structured, and all of the relevant documentation\sidenote{
	\url{https://dimewiki.worldbank.org/Data_Documentation}}
is shared, it is easy for the reader to understand the analysis fully.
Researchers that expect process transparency also have an incentive to make better decisions,
be skeptical and thorough about their assumptions,
and, as we hope to convince you, save themselves time,
because transparent research methods are labor-saving over the complete course of a project.

Clearly documenting research work is necessary
to allow others to evaluate exactly what data was acquired and how it was used
to obtain a particular result.
Many development research projects are purpose-built
to address specific questions,
and often use unique data, novel methods, or small samples.
These approaches can yield new insights into essential academic questions,
but need to be transparently documented so they can be reviewed
or replicated by others in the future.\cite{duvendack2017meant}
Unlike disciplines where data is more standardized
or where research is more oriented around secondary data,
 the exact data used in a development project
has often not been observed by anyone else in the past
and may not be able to be re-collected by others in the future.
Regardless of the novelty of study data,
transparent documentation methods help ensure
that data was collected and handled appropriately
and that studies and interventions were implemented correctly.
As with study registrations, project and data documentation
should be released on external archival repositories
so they can always be accessed and verified.

% Documenting a project carefully makes it more transparent
\subsection{Documenting data acquisition and analysis}

Documenting a project in detail greatly increases transparency.
Many disciplines have a tradition of keeping a ``lab notebook'',
and adapting and expanding this process to create a
lab-style workflow in the development field is a
critical step towards more transparent practices.
This means explicitly noting decisions as they are made,
and explaining the process behind the decision-making.
Careful documentation will also save the research team a lot of time during a project,
as it prevents you from having the same discussion twice (or more!),
since you have a record of why something was done in a particular way.
There are a number of available tools
that will contribute to producing documentation,
\index{project documentation}
but project documentation should always be an active and ongoing process,
not a one-time requirement or retrospective task.
New decisions are always being made as the plan begins contact with reality,
and there is nothing wrong with sensible adaptation so long as it is recorded and disclosed.

% Tools for transparency: GitHub, OSF
There are various software solutions for building documentation over time.
Some work better for field records such as implementation decisions,
research design, and survey development;
others work better for recording data work and code development.
The \textbf{Open Science Framework}\sidenote{\url{https://osf.io}} provides one such solution,\index{Open Science Framework}
with integrated file storage, version histories, and collaborative wiki pages.
\textbf{GitHub}\sidenote{\url{https://github.com}} provides a transparent documentation system
through commit messages, issues, read me files, and pull requests,\sidenote{
	\url{https://dimewiki.worldbank.org/Getting_started_with_GitHub}}\index{task management}\index{GitHub}
in addition to version histories and wiki pages.
Such services offer multiple different ways
to record the decision process leading to changes and additions,
track and register discussions, and manage tasks.
These are flexible tools that can be adapted to different team and project dynamics.
Services that log your research process can show things like modifications made in response to referee comments,
by having tagged version histories at each major revision.
They also allow you to use issue trackers
to document the research paths and questions you may have tried to answer
as a resource to others who have similar questions.
Each project has specific requirements for data, code, and documentation management,
and the exact transparency tools to use will depend on the team's needs,
but they should be agreed upon prior to project launch.
This way, you can start building a project's documentation as soon as you start making decisions.
(Email, however, is \textit{not} a documentation service, because communications are rarely well-ordered,
can be easily deleted, and are not available for future team members.)

% Documenting survey instrument and survey code
\subsection{Cataloging and archiving data}

Data and data collection methods should be fully cataloged, archived, and documented,
whether you are collecting data yourself or receiving it from an outside partner.
In some cases this is as simple as uploading
a survey instrument or an index of datasets and a codebook to an archive.
In other cases this will be more complex.
Proper documentation of data collection will often require
a detailed description of the overall sampling procedure.
For example, settings with many overlapping strata,
treatment arms, excluded observations, or resampling protocols
might require extensive additional field work documentation.
This documentation should be continuously updated
and kept with the other study materials;
it is often necessary to collate these materials
for an appendix for publication in any case.

% Preparing an initial catalog and release of data
The raw dataset that comes in from the field should be immediately archived and cataloged.
Some project funders
provide specific respositories in which they require the deposit of data they funded,\sidenote{For example, \url{https://data.usaid.gov}}
and you should take advantage of these when possible.
If this is not provided, you must be aware of privacy issues
with directly identifying data and questions of data ownership
before uploading raw data to any third-party server, whether public or not;
this is a legal question for your home organization.
This type of data depositing or archiving
is different from publishing or releasing the data:
data at this stage may still need to be embargoed
or have other, potentially permanent, access restrictions.
Similarly, data catalogs (such as the World Bank Data Catalog\sidenote{\url{https://datacatalog.worldbank.org}})
do not necessarily make data accessible: they simply create a record of its existence
and provide instructions on how access would be obtained.

Archiving and cataloging data does not resolve
the need to create an authoritative, citable, \textit{published} data set
in a publicly accessible first- or third-party archive,
even if access restrictions are still required.
For more on the steps required to prepare and publish a de-identified dataset,
you can refer to Chapter 6 and Chapter 7 of this book.
You should use a tool such as a Dataverse or the OSF
to upload and store de-identified and publishable versions of your data,
after archiving the complete data in an institution-approved location.
Data publication should create a data citation and a digital object identifier (DOI),
or some other persistent index that you can use in your future work
to unambiguously indicate the location of your data.
This data publication should also include the methodological documentation
as well as complete human-readable codebooks for all the variables there.

%%
\section{Preparing for reproducible analysis}

% What is reproducibility

Development research is rapidly moving in the direction of requiring strict adherence
to specific reproducibility guidelines.\cite{christensen2018transparency}
Major publishers and funders, most notably the American Economic Association,
have taken steps to require that code and data
are accurately reported, cited, and preserved as outputs in themselves.\sidenote{
	\url{https://www.aeaweb.org/journals/policies/data-code}}
Common research standards from journals and funders feature both ex ante
(or ``regulation'') and ex post (or ``verification'') policies.\cite{stodden2013toward}
Ex ante policies require that authors
provide replication materials before publication
which are then reviewed by the journal for completeness.
Ex post policies require that authors make certain materials available to the public,
but their completeness is not a precondition for publication.
Other journals have adopted ``guidance'' policies that offer checklists
for reporting on whether and how various practices were implemented,
without specifically requiring any.\cite{nosek2015promoting}
Documentation on data processing and additional hypotheses tested
will be expected in the supplemental materials to any publication.

At DIME, all research outputs are required to be reproducible.
Before releasing a working paper,
a research team is required to submit
a reproducible analysis package with de-identified data,
DIME Analytics then verifies computational reproducibility:
does the provided package produce exactly the same results that appear in the paper?
In addition, the team checks whether the package includes sufficient documentation.
Additionally, the team organizes frequent peer code review of works in progress,
and our general recommendation is to ensure that projects
are \textit{always} externally reproducible
instead of waiting until the final stages to prepare this material.
Once the computational reproducibility check is complete,
the team receives a completed reproducibility certificate
that also lists any publicly available materials to accompany the package,
for use as an appendix to the publication.


\subsection{Reproducible research is a public good}
% Reproducible research is a public good

Making your research reproducible is also a public good.\sidenote{
	\url{https://dimewiki.worldbank.org/Reproducible_Research}}
It enables other researchers to re-use your code and processes
to do their own work more easily and effectively in the future.
This may mean applying your techniques to their data
or implementing a similar structure in a different context.
As a pure public good, this is nearly costless.
The useful tools and standards you create will have high value to others.
If you are personally or professionally motivated by citations,
producing these kinds of resources can lead to that as well.
Therefore, your code should be written neatly with clear instructions and published openly.
It should be easy to read and understand in terms of structure, style, and syntax.
Finally, the corresponding analytical dataset should always be made openly accessible
to the greatest legal and ethical extent that it can be.\sidenote{
	\url{https://dimewiki.worldbank.org/Publishing_Data}}

\subsection{Completing a reproducibility package}
% What is transparency and how it makes research better
Can another researcher reuse the same code on the same data
and get the exact same results as in your published paper?\sidenote{
  \url{https://blogs.worldbank.org/impactevaluations/what-development-economists-talk-about-when-they-talk-about-reproducibility}}
This is a standard known as \textbf{computational reproducibility},
and it is an increasingly common requirement for publication.\sidenote{
\url{https://www.nap.edu/resource/25303/R&R.pdf}}
It is best practice to verify computational reproducibility before submitting a paper for publication.
This should be done by someone who is not on your research team, on a different computer,
using exactly the package of code and data files you plan to submit with your paper.
As we will discuss in Chapter 5,
code that is well-organized into a master script, and written to be easily run by others,
makes this task simpler.

% Open data is necessary for reproducibility
For research to be reproducible,
all code files for data cleaning, construction and analysis
should be public, unless they contain confidential information.
Nobody should have to guess what exactly comprises a given index,
or what controls are included in your main regression,
or whether or not you clustered standard errors correctly.
That is, as a purely technical matter, nobody should have to ``just trust you'',
nor should they have to bother you to find out what would happen
if any or all of these things were to be done slightly differently.\cite{simmons2011false,simonsohn2015specification,wicherts2016degrees}
Letting people play around with your data and code
is a great way to have new questions asked and answered
based on the valuable work you have already done.\sidenote{
	\url{https://blogs.worldbank.org/opendata/making-analytics-reusable}}

A reproducibility package should include the complete materials needed
to exactly re-create your final analysis,
and be accessible and well-documented so that others can identify
and adjust potential decision points that they are interested in.
They should be able to easily identify:
what data was used and how that data can be accessed;
what code generates each table, figure and in-text number;
how key outcomes are constructed;
and how all project results can be reproduced.
A well-organized reproducibility package usually takes the form
of a complete directory structure, including documentation and a master script,
that leads the reader through the process and rationale
for the code behind each of the outputs
when considered in combination with the corresponding publication.

\bigskip
% Concluding paragraph
With the ongoing rise of empirical research and increased public scrutiny of scientific evidence,
making analysis code and data available
is necessary but not sufficient to guarantee that findings will be credible.
Even if your methods are highly precise,
your evidence is only as good as your data --
and there are plenty of mistakes that can be made between
establishing a design and generating final results that would compromise its conclusions.
That is why transparency is key for research credibility.
It allows other researchers, and research consumers,
to verify the steps to a conclusion by themselves,
and decide whether their standards for accepting a finding as evidence are met.
Every investment you make in documentation and transparency up front
protects your project down the line, particularly as these standards continue to tighten.
