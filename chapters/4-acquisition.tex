%------------------------------------------------

\begin{fullwidth}
High-quality data is essential to most modern development research.
Many research questions require the team to collect original data,
because no source of publicly available data measures the
inputs or outcomes of interest for the relevant population.
Data acquisition can take many forms, including:
primary data generated through surveys;
private sector partnerships granting access to new data sources, such as administrative and sensor data;
digitization of paper records, including administrative data; web scraping;
primary data capture by unmanned aerial vehicles or other types of remote sensing;
or novel integration of various types of datasets, e.g. combining survey and sensor data.
Much of the recent push toward credibility in the social sciences has focused on analytical practices.
However, credible development research often depends, first and foremost, on the quality of the raw data.
For data work to be reproducible,
in the sense that other researchers would be able to make
equally informed decisions about analytical choices,
the data acquisition process must be clearly and carefully documented.

This chapter covers data acquisition,
special considerations for generating high-quality survey data,
and protocols for safely and securely handling confidential data.
The first section discusses reproducible data acquisition:
how to establish and document your right to use the data.
This applies to all development datasets that are not publicly available,
whether collected for the first time through surveys or sensors or acquired through a unique partnership.
The second section goes into detail on data acquisition through surveys,
as this process is typically more involved than acquisition of secondary data,
and has more in-built opportunities for quality control.
It provides detailed guidance on the electronic survey workflow,
from questionnaire design to programming and monitoring data quality.
We conclude with a discussion of safe data handling,
providing guidance on how to receive, transfer, store, and share confidential data.
Secure file management is a basic requirement to comply with the legal and
ethical agreements that allow  access to personal information for research purposes.


\end{fullwidth}

%------------------------------------------------
\section{Acquiring development data}

Clearly establishing and documenting data access is critical for reproducible research.
This section provides guidelines for receiving data from development partners,
establishing data ownership,
and documenting the research team's right to use the data.
It is the researchers' responsibility to respect the rights
of people who own the data and the people who are described by it;
but also to make sure that information is as available and accessible as possible.
These twin responsibilities can and do come into tension,
so it is important to be fully informed about what other researchers are doing
and to fully inform other researchers of what you are doing.
Writing down and agreeing to specific details is a good way of doing that.

\subsection{Receiving data from development partners}

Research teams granted access to existing data may receive that data in a number of different ways.
You may receive access to an existing server,
or physical access to extract certain information,
or a one-time data transfer.
In all cases, you must take action to ensure
that data is transferred through
secure channels so that confidentiality is not compromised.
See the section \textit{Handling data securely} later in this chapter for how to do that.
Keep in mind that compliance with ethical research standards may
in some cases require a stricter level of security than initially proposed by the partner agency.

At this stage, it is very important to assess
documentation and cataloging of the data and associated metadata.
It is not always clear what pieces of information will jointly constitute a ``dataset'',
and many of the datasets you receive will not be organized for research.
You should always retain the original data exactly as received
alongside a copy of the corresponding ownership agreement or license.
You should maintain a simple \texttt{README} document noting the date of receipt,
the source and recipient of the data,
and a brief description of each file received.
All too often data will be provided as vaguely-named spreadsheets,
or digital files with non-specific titles,
and documentation will be critical for future access and reproducibility.
This metadata -- the documentation about the data -- is critical for future use of the data.
Metadata should include documentation of how the data was created,
what they measure, and how they are to be used.
In the case of survey data, this includes the survey instrument and associated manuals;
the sampling protocols and field adherence to those protocols, and any sampling weights;
what variable(s) uniquely identify the dataset(s), and how different datasets can be linked;
and a description of field procedures and quality controls.
DIME uses as a standard the Data Documentation Initiative (DDI), which is supported by the
World Bank's Microdata Catalog.\sidenote{\url{https://microdata.worldbank.org}}

As soon as the desired pieces of information are stored together,
think about which ones are the components of what you would call a dataset.
Often, when you are recieving ``data'' from a partner,
even highly-structured materials such as registers or records
are not, as recieved, equivalent to a research dataset,
and require initial cleaning, restructuring, or recombination
to reach the stage we would consider a raw research dataset.
This is as much an art than a science:
you want to keep information together that is best contextualized together,
but you also want to information granular as much as possible,
particularly when there are varying units of observation.
There usually won't be a single correct way to answer this question,
and the research team will need to decide how the materials recieved.
Soon, you will begin to build datasets from this set of information,
and these will become your original raw data,
which will be the material published, released, and cited as your raw dataset.
These first datasets created from the recieved materials
are the objects you need to catalog, release, and license.
Now is a good time to being assessing disclosure risk
and/or seek publication licenses in collaboration with data providers,
while you are in close contact with them.

\subsection{Data ownership}
Except in the case of primary surveys conducted by the research team,
such data is typically not owned by the research team.
Rather, the team must enter into data licensing agreements
to access the data and publish research outputs based on it.
These agreements should make clear from the outset whether the
research team can make the original data, any portion thereof, or derivatives\sidenote{
	\textbf{Derivatives} of data can be indicators, aggregates,
	visualizations etc. created from the original data.}
of the data, public.

Data ownership\sidenote{need to add DIME Wiki page} can sometimes be challenging to establish,
as various jurisdictions have differing laws regarding data and information,
and the research team may have their own information regulations.
In some, data is implicitly owned by the people who it is about.
In others, it is owned by the people who collected it.
The best approach is always to consult with a local partner,
and enter into specific legal agreements establishing ownership,
access, and publication rights.
This is particularly critical where confidential data is involved
-- that is, when people are disclosing information to you
that you could not obtain simply by observation or through public records.

For original data generated by the research team, such as survey data,
it is important to clarify up front who owns the data,
and who will have access to it.
These details need to be shared with respondents when they are offered the opportunity
to consent to participate in the study.
If the research team is not collecting the data directly --
for example, if a government, private company, or research partner is doing the data collection --
make sure that you have an explicit agreement
about who owns the resulting data.
The contract for data collection should include specific terms
as to the rights and responsibilities of each party.
It must clearly stipulate who owns the data produced.
The contract should also explicitly indicate that the contracted firm
is responsible for protecting respondent privacy,
that the data collection will not be delegated to any third parties,
and that the data will not be used by the firm or subcontractors for any purpose not expressly stated in the contract,
before, during or after the assignment.
The contract should stipulate that the vendor is required to comply with
ethical standards for social science research,
and adhere to the specific terms of agreement with the relevant
Institutional Review Board (IRB)\sidenote{
	\url{https://dimewiki.worldbank.org/IRB\_Approval}}
and any applicable local authority.
Finally, it should include policies on reuse, storage,
and retention or destruction of data.

Research teams that generate their own data
should consider future data licensing,
through the terms they will use to release that data
to other researchers or to the general public.
One option is to publicly release the data in full,
after removing personal identifiers.
The team should consider whether it will be acceptable for data to
be stored anywhere in the world,
even places that are owned by corporations or governments in other countries.
The team could alternatively opt to decide permission on a case-by-case basis,
dependent on specific proposed uses.
The license could require that users of your data cite or credit you,
or could require users in turn to release
their derivative datasets or publications under similar licenses.
Whatever your answers are to these questions,
make sure the agreement or contract
you use to acquire the data
and the license you issue to accompany its release
specifically details those requirements.

\subsection{Data licensing}
Data licensing is the formal act of the dataset owner
giving some data rights to a specific user,
while retaining ownership of the dataset.
If you are not the owner of the dataset you want to analyze,
you must enter into a licensing agreement to access it for research purposes.
Similarly, when you own a dataset,
you must consider whether you will make the dataset accessible to other researchers,
and what terms of use you require.

If the research team requires access to existing data for research,
terms of use should be agreed on with the data owner,
typically through a data licensing agreement\sidenote{need to add a DIME wiki page}.
Keep in mind that the data owner is likely not highly familiar
with the research process, and therefore may be surprised
at some of the things you want to do if you are not clear up front.
You will typically want intellectual property rights
to all research outputs developed used the data,
a license for use of derivative datasets,
and potentially permission for public release or specific redistribution.
This is important to allow the research team
to store, catalog, and publish, in whole or in part,
either the original licensed dataset or datasets derived from the original.
Make sure that the license you obtain from the data owner allows these uses,
and that you consult with the owner
if you foresee exceptions with specific portions of the data.

The World Bank has a template Data License Agreement which DIME follows.\sidenote{\url{https://worldbankgroup.sharepoint.com/teams/ddh/SiteAssets/SitePages/ddh/DataLicenseAgreementTemplate_v4.pdf?cid=68a54269-bbff-4b47-846d-cab248ad7de1}}
The template specifies the specific objectives of the data sharing,
and whether the data can be used only for the established purpose or for other objectives.
It classifies the data into one of four access categories,
depending on who can access it,
and whether case-by-case authorization is needed.
The data provider may impose similar restrictions
to sharing derivative works and any or all of the metadata.
The template also specifies the required citation for the data.
While you do not need to use the World Bank's template
or its categories if you do not work on a World Bank project,
we still think it is important that you use this information in two ways.
First, while you do not have to use the World Bank template,
make sure to base your Data License Agreement on some template.
Second, we strongly recommend you use the World Bank access categories,
or some variation of it.
Then you should have different procedures for each category,
or use one procedure appropriate for the strictest category for all your data.

%------------------------------------------------
\section{Collecting data using electronic surveys}
In this section, we detail specific considerations
for acquiring high-quality data through surveys.
There are many survey software options available to researchers,
and the market is rapidly evolving.
There are also many excellent resources on questionnaire design and field supervision,
but few covering the particular challenges and opportunities presented by electronic surveys.
Therefore, we focus on specific workflow considerations for digitally-collected data,
and on basic concepts rather than software-specific tools.
If you are collecting data directly from the research subjects yourself,
you are most likely designing and fielding an electronic survey.
These types of data collection technologies
have greatly accelerated our ability to bring in high-quality data
using purpose-built survey instruments,
and therefore improved the precision of research.
At the same time, electronic surveys create new pitfalls to avoid.
Programming surveys efficiently requires a very different mindset
than simply designing them in word processing software,
and ensuring that they flow correctly and produce data
that can be used in statistical software requires careful organization.
This section will outline the major steps and technical considerations
you will need to follow whenever you field a custom survey instrument,
no matter the scale.

\subsection{Developing a data collection instrument}

A well-designed questionnaire results from careful planning,
consideration of analysis and indicators,
close review of existing questionnaires,
survey pilots, and research team and stakeholder review.
There are many excellent resources on questionnaire design,
such as from the World Bank's Living Standards Measurement Survey.\cite{glewwe2000designing}
The focus of this section is the design of electronic field surveys,
often referred to as Computer Assisted Personal Interviews (CAPI).\sidenote{
  \url{https://dimewiki.worldbank.org/Computer-Assisted\_Personal\_Interviews\_(CAPI)}}
Although most surveys are now collected electronically, by tablet, mobile phone or web browser,
\textbf{questionnaire design}\sidenote{
  \url{https://dimewiki.worldbank.org/Questionnaire\_Design}}
  \index{questionnaire design}
(content development) and \textbf{questionnaire programming}\sidenote{
\url{https://dimewiki.worldbank.org/Questionnaire\_Programming}} (functionality development)
should be seen as two strictly separate tasks.
Therefore, the research team should agree on all questionnaire content
and design a version of the survey on paper
before beginning to program the electronic version.
This facilitates a focus on content during the design process
and ensures teams have a readable, printable version of their questionnaire.
Most importantly, it means the research, not the technology,
drives the questionnaire design.

We recommend this approach because an easy-to-read paper questionnaire
is very useful for training data collection staff,
because it focuses on the survey content and structure
before diving into the technical components.
It is much easier for enumerators to understand
the range of possible participant responses
and how to handle them correctly on a paper survey than on a tablet,
and it is easier to translate that logic to digital functionality later.
Finalizing this version of the questionnaire before beginning any programming
avoids version control concerns that arise from concurrent work
on paper and electronic survey instruments.
Finally, a readable paper questionnaire is a necessary component of data documentation,
since it is difficult to work backwards from the survey program to the intended concepts.

The workflow for designing a questionnaire will feel much like writing an essay or writing pseudocode:
it begins from broad concepts and slowly fleshes out the specifics.\sidenote{
  \url{https://iriss.stanford.edu/sites/g/files/sbiybj6196/f/questionnaire\_design\_1.pdf}}
It is essential to start with a clear understanding of the
\textbf{theory of change}\sidenote{
  \url{https://dimewiki.worldbank.org/Theory\_of\_Change}}
and \textbf{research design} for your project.
The first step of questionnaire design is to list key outcomes of interest,
as well as the main covariates to control for and any variables needed for the specific research design.
The ideal starting point for this is a \textbf{pre-analysis plan}.\sidenote{
  \url{https://dimewiki.worldbank.org/Pre-Analysis\_Plan}}

Use the list of key outcomes to create an outline of questionnaire \textit{modules}.
Do not number the modules; instead use a short prefix
as section numbers quickly become outdated when modules are reordered.
For each module, determine if the module is applicable to the full sample,
or only to specific respondents,
and whether or how often the module should be repeated.
A few examples:
a module on maternal health only applies
to households with a woman who has children,
a household income module should be answered
by the person responsible for household finances,
and a module on agricultural production
might be repeated for each crop the household cultivated.
Each module should then be expanded
into specific indicators to observe in the field.\sidenote{
  \url{https://dimewiki.worldbank.org/Literature\_Review\_for\_Questionnaire}}
At this point, it is useful to do a  \textbf{content-focused pilot}\sidenote{
  \url{https://dimewiki.worldbank.org/Piloting\_Survey\_Content}}
of the questionnaire.
Doing this pilot with a pen-and-paper questionnaire encourages more significant revisions,
as there is no need to factor in time costs of re-programming,
and as a result improves the overall quality of the survey instrument.
Questionnaires must also include ways to document the reasons for \textbf{attrition} and
treatment \textbf{contamination}.\index{attrition}\index{contamination}
These are essential data components for completing CONSORT records,
a standardized system for reporting enrollment, intervention allocation, follow-up,
and data analysis through the phases of a randomized trial.\cite{begg1996improving}

A comprehensive survey pilot is a critical stage in survey design.\sidenote{
	\url{https://dimewiki.worldbank.org/Survey\_Pilot}} 
The pilot must be done out-of-sample,
but in a context as similar as possible to the study sample.
The survey pilot includes three steps: 
a \textbf{pre-pilot}, a \textbf{content-focused pilot}, and a \textbf{data-focused pilot}.\sidenote{
	\url{https://dimewiki.worldbank.org/wiki/Structuring\_a\_Survey\_Pilot}} 
The first step is a pre-pilot. 
The pre-pilot is a qualitative exercise, done early in the questionnaire design process.
The objective is to answer broad questions about how to measure key outcome variables,
and gather qualitative information relevant to any of the planned survey modules.  
A pre-pilot is particularly important when designing new survey instruments.
The second step is a content-focused pilot\sidenote{
  \url{https://dimewiki.worldbank.org/Piloting\_Survey\_Content}}.
The objectives at this stage are to improve the structure and length of the questionnaire,
refine the phrasing and translation of specific questions,
and confirm coded response options are exhaustive.\sidenote{
	\url{https://dimewiki.worldbank.org/index.php?title=Checklist:\_Refine\_the\_Questionnaire\_(Content)}}
In addition, it is an opportunity to test and refine all survey protocols,\sidenote{
	\url{https://dimewiki.worldbank.org/wiki/Piloting\_Survey_Protocols}}
such as how units will be sampled or pre-selected units identified.
The content-focused pilot is best done on pen and paper, before the questionnaire is programmed,
because changes at this point may be deep and structural,
which are hard to adjust in code.
The final stage is a data-focused pilot.
The objective at this stage is to refine the questionnaire programming; this is discussed in more detail in the following section.\sidenote{
	\url{https://dimewiki.worldbank.org/wiki/Timeline\_of\_Survey\_Pilot}}


\subsection{Designing surveys for electronic deployment}
Once the team is satisfied with the content and structure of the survey,
it is time to move on to implementing it electronically.
Electronic data collection has great potential
to simplify survey implementation and improve data quality.
Electronic questionnaires are typically created
in a spreadsheet (e.g. Excel or Google Sheets)
or a software-specific form builder,
all of which are accessible even to novice users.\sidenote{
  \url{https://dimewiki.worldbank.org/Questionnaire\_Programming}}
We will not address software-specific form design in this book;
rather, we focus on coding conventions that are important to follow
for electronic surveys regardless of software choice.\sidenote{
  \url{https://dimewiki.worldbank.org/SurveyCTO\_Coding\_Practices}}
Survey software tools provide a wide range of features
designed to make implementing even highly complex surveys
easy, scalable, and secure.
However, these are not fully automatic:
you need to actively design and manage the survey.
Here, we discuss specific practices that you need to follow
to take advantage of electronic survey features
and ensure that the exported data
is compatible with your statistical software.

From a data perspective, questions with pre-coded response options
are always preferable to open-ended questions.
The content-based pilot is an excellent time to ask open-ended questions
and refine fixed responses for the final version of the questionnaire --
do not count on coding up lots of free text after a full survey.
Coding responses helps to ensure that the data
will be useful for quantitative analysis.
Two examples help illustrate the point.
First, instead of asking ``How do you feel about the proposed policy change?'',
use techniques like \textbf{Likert scales}\sidenote{
  \textbf{Likert scale:} an ordered selection of choices indicating the respondent's level of agreement or disagreement with a proposed statement.}.
Second, if collecting data on medication use or supplies, you could collect:
the brand name of the product; the generic name of the product; the coded compound of the product;
or the broad category to which each product belongs (antibiotic, etc.).
All four may be useful for different reasons,
but the latter two are likely to be the most useful for rapid data analysis.
The coded compound requires providing a translation dictionary to field staff,
but enables automated rapid recoding for analysis with no loss of information.
The generic class requires agreement on the broad categories of interest,
but allows for much more comprehensible top-line statistics and data quality checks.
Rigorous field testing is required to ensure that answer categories are comprehensive;
however, it is best practice to include an \textit{other, specify} option.
Keep track of those responses in the first few weeks of field work.
Adding an answer category for a response frequently showing up as \textit{other} can save time,
as it avoids extensive post-coding.

It is essential to name the fields in your questionnaire
in a way that will also work in your data analysis software.
Most survey programs will not enforce this by default,
since limits vary across statistical software,
and survey software will encourage you
to use long sentences as question labels
and detailed descriptions as choice options.
This is what you want for the enumerator-respondent interaction,
but you should already have analysis-compatible labels programmed in the background
so the resulting data can be rapidly imported in analytical software.
There is some debate over how exactly individual questions should be identified:
formats like \texttt{hq\_1} are hard to remember and unpleasant to reorder,
but formats like \texttt{hq\_asked\_about\_loans} quickly become cumbersome.
We recommend using descriptive names with clear prefixes so that variables
within a module stay together when sorted alphabetically.\sidenote{
  \url{https://medium.com/@janschenk/variable-names-in-survey-research-a18429d2d4d8}}
Variable names should stick to simple structures,
using English characters and numbers;
we prefer all-lowercase naming with underscores.
Take special care with the length:
very long names will be cut off in some softwares,
which could result in a loss of uniqueness
and lots of manual work to restore compatibility.
We also discourage explicit question numbering,
at least at first, as it discourages re-ordering questions,
which is a common recommended change after the pilot.
In the case of follow-up surveys, numbering can quickly become convoluted,
too often resulting in uninformative variables names like
\texttt{ag\_15a}, \texttt{ag\_15\_new}, \texttt{ag\_15\_fup2}, and so on.

\subsection{Programming electronic questionnaires}

The starting point for questionnaire programming
is a complete paper version of the questionnaire,
piloted for content and translated where needed.
Doing so reduces version control issues
that arise from making significant changes
to concurrent paper and electronic survey instruments.
Changing structural components of the survey
after programming has been started
often requires the coder to substantially re-work the entire code.
This is because the more efficient way to code surveys is non-linear.
When programming, we do not start with the first question and proceed through to the last question.
Instead, we code from high level to small detail,
following the same questionnaire outline established at design phase.
The outline provides the basis for survey pseudocode,
allowing you to start with high level structure and work down to the level of individual questions.
This will save time and reduce errors,
particularly where sections or fields
are interdependent or repeated in complex ways.

Electronic surveys are more than simply
a paper questionnaire displayed on a mobile device or web browser.
All common survey software allows you to automate survey logic
and include hard or soft constraints on survey responses.
These features make enumerators' work easier,
and they create the opportunity to identify and resolve
data issues in real time,
simplifying data cleaning and improving response quality.
Well-programmed questionnaires should include
most or all of the following features:

\begin{itemize}
  \item{\textbf{Localization}}: the survey instrument should display full-text questions and responses in all potential survey languages, and it should also have English and code-compatible versions of all text and labels.
	\item{\textbf{Survey logic}}: built-in tests should be included for all logic connections between questions, so that only relevant questions appear, rather than relying on enumerators to follow complex survey logic. This covers simple skip codes, as well as more complex interdependencies (e.g., a child health module is only asked to households that report the presence of a child under 5).
	\item{\textbf{Range checks}}: add range checks for all numeric variables to catch data entry mistakes (e.g. \texttt{age} must be less than 120).
	\item{\textbf{Confirmation of key variables}}: require double entry of essential information (such as a contact phone number in a survey with planned phone follow-ups), with automatic validation that the two entries match and rejection and re-entry otherwise.
	\item{\textbf{Multimedia}}: electronic questionnaires facilitate collection of images, video, and geolocation data directly during the survey, using the camera and GPS built into the tablet or phone.
	\item{\textbf{Preloaded data}}: data from previous rounds or related surveys can be used to prepopulate certain sections of the questionnaire, and validated during the survey.
	\item{\textbf{Filtered response options}}: filters reduce the number of response options dynamically (e.g. filtering a ``cities'' choice list based on the state selected).
	\item{\textbf{Location checks}}: enumerators submit their actual location using in-built GPS, to confirm they are in the right place for the interview.
	\item{\textbf{Consistency checks}}: check that answers to related questions align, and trigger a warning if not so that enumerators can probe further (e.g., if a household reports producing 800 kg of maize, but selling 900 kg of maize from their own production).
	\item{\textbf{Calculations}}: make the electronic survey instrument do all math, rather than relying on the enumerator or asking them to carry a calculator.
\end{itemize}

All established survey software include debugging and test options
to correct syntax errors and make sure that
the survey instruments will successfully compile.
This is not sufficient, however, to ensure that the resulting dataset
will load without errors in your data analysis software of choice.
DIME Analytics developed the \texttt{ietestform} command,\sidenote{
  \url{https://dimewiki.worldbank.org/ietestform}}
part of the Stata package \texttt{iefieldkit},
to implement a form-checking routine for \textbf{SurveyCTO},
a proprietary implementation of the open source \textbf{Open Data Kit (ODK)} software.
Intended for use during questionnaire programming and before field work,
\texttt{ietestform} tests for best practices
in coding, naming and labeling, and choice lists.
Although \texttt{ietestform} is software-specific,
many of the tests it runs are general and important to consider regardless of software choice.
To give a few examples, \texttt{ietestform} tests that no variable names exceed
32 characters, the limit in Stata (variable names that exceed that limit will
be truncated, and as a result may no longer be unique).
It checks whether ranges are included for numeric variables.
\texttt{ietestform} also removes all leading and trailing blanks from response lists,
which could be handled inconsistently across software.

The final stage of survey piloting, the data-focused pilot, should be done at this stage (after the questionnaire is programmed).
The objective of this \textbf{data-focused pilot}\sidenote{
  \url{https://dimewiki.worldbank.org/index.php?title=Checklist:\_Refine\_the\_Questionnaire\_(Data)}}
is to validate the programming and export a sample dataset.
Significant desk-testing of the instrument is required to debug the programming
as fully as possible before going to the field.
It is important to plan for multiple days of piloting,
so that any debugging or other revisions to the electronic survey instrument
can be made at the end of each day and tested the following, until no further field errors arise.
The data-focused pilot should be done in advance of enumerator training.

\subsection{Finalizing data collection}

When all data collection is complete,
the survey team should prepare a final field report,
which should report reasons for any deviations between the original sample and the data collected.
Identification and reporting of \textbf{missing data} and \textbf{attrition}
is critical to the interpretation of survey data.
It is important to structure this reporting in a way that not only
groups broad rationales into specific categories
but also collects all the detailed, open-ended responses
to questions the field team can provide for any observations that they were unable to complete.
This reporting should be validated and saved alongside the final raw data, and treated the same way.
This information should be stored as a dataset in its own right
-- a \textbf{tracking dataset} -- that records all events in which survey substitutions
and attrition occurred in the field and how they were implemented and resolved.


%------------------------------------------------
\section{Handling data securely}

All confidential data must be handled in such a way that only people specifically
approved by an Institutional Review Board (IRB)
are able to access the data.
Data can be confidential for multiple reasons; two 
very common reasons are that the data contains personally identifiable information (PII)\sidenote{
  \url{https://dimewiki.worldbank.org/Personally\_Identifiable\_Information\_(PII)}}
or that the data owner has specified restricted access.


\subsection{Encryption}

\index{encryption}\textbf{Data encryption} is the best way to protect confidential data.\sidenote{
	\url{https://dimewiki.worldbank.org/Encryption}} 
Data encryption is a group of methods that ensure that files are unreadable
even if laptops are stolen, servers are hacked,
or unauthorized access to the data is obtained in another way.
Proper encryption can rarely be condensed into a single method,
as the data will travel through many servers, devices, and computers
from the source of the data to the final analysis.
Encryption should be seen as a system
that is only as secure as its weakest link.
This section recommends a streamlined workflow,
so that it is easy as possible to make sure
the weakest link is still sufficiently secure.

Authorized members of the research team can access encrypted data by decrypting it,
making the data readable again by entering a specific key. 
Without the specific decryption key,
encrypted data files are completely unusable. 
This is a higher level of security than password-protection.
It is never secure to share decryption keys (or passwords) by email,
instant messaging, or other common modes of communication;
instead, use a secure password manager.\sidenote{
	\url{https://lastpass.com} or \url{https://bitwarden.com}
	are common, but there are many options that provide similar services. See the DIME Analytics guide to password managers: \url{
		https://github.com/worldbank/dime-standards/blob/master/dime-research-standards/pillar-4-data-security/data-security-resources/password-manager-guidelines.md
}}
In addition to providing a way to securely share passwords,
password managers also provide a secure location
for long term storage of passwords and keys,
whether they are shared or not.
Make sure you store your keys with descriptive names,
as they will be needed over the lifetime of a project.

There are two methods of encryption you should be aware of;
\textbf{encryption-in-transit}\sidenote{
	\url{https://dimewiki.worldbank.org/Encryption\#Encryption\_in\_Transit}}
and \textbf{encryption-at-rest}\sidenote{
	\url{https://dimewiki.worldbank.org/Encryption\#Encryption\_at\_Rest}}.
Encryption-in-transit protects your data when in transit over the internet.
It is easy to implement encryption-in-transit such that no input from the user is required,
since the server and browser can keep track of the key for the short duration of the connection.
All well-established data transfer services have this implemented and you rarely have to worry about it.
However, if you are setting up a custom solution,
it is important that you verify that it use encryption-in-transit,
as without it, your data and credentials are very vulnerable.
Encryption-at-rest protects your data when it is stored somewhere,
for example on a server or on your computer.
In contrast to encryption-in-transit,
you as a user need to keep track of the decryption key.
Only people listed on your IRB should be allowed to have access to this file.

\subsection{Storing data securely}
% doesn't cover 'big' data
Before acquiring any data, you must plan how you will securely store that data after you have received it.
Typically, you want to store your data so that you can decrypt and access it,
interact with it, and then encrypt it again.
That is a perfect use case for \textbf{symmetric encryption},
where the same key is used to both encrypt and decrypt your files.
Think of this as a physical safe where you have one key
used to both add and remove content.

If the data you are working with is an appropriate size to be stored on your computer,
you can use an encrypted folder for the equivalent of a safe.
If you have the decryption key, you can interact with the files in an encrypted folder as usual (after entering the key).
\sidenote{One software option for encrypting folders is VeraCrypt
	\url{https://www.veracrypt.fr}. DIME guidelines for VeraCrypt use:
	\url{https://github.com/worldbank/dime-standards/blob/master/dime-research-standards/pillar-4-data-security/data-security-resources/veracrypt-guidelines.md}.}
An encrypted folder is an implementation of encryption-at-rest.
There is absolutely no way to restore your data if you lose your key,
so we cannot stress enough the importance of using a password manager,
or equally secure solution, to store these encryption keys.

If, on the other hand, the data you are working with is too big to store on a regular computer,
and instead the data is stored and processed in a cloud environment,
you will need a more advanced solution. 
The first step is to understand exactly how the data is encrypted
and how the keys are handled for the particular cloud storage option you choose.
A typical research team
will need to consult a cybersecurity expert to ensure adequate data security protocols are in place.

\subsection{Collecting data securely in the field}

All common data collection software will automatically encrypt
data in transit (i.e., upload from field or download from server).
However, it is also necessary to ensure that confidential data
are protected when stored on a server that can be accessed
by the data collection software provider or by your local IT team.
You should not assume that your data is encrypted at rest there by default.
In most data collection platforms,
encryption-at-rest needs to be explicitly enabled and operated by the user.

When collecting data, you want the mobile phones, tablets or browsers used for data collection
to encrypt all information before submitting it to the server.
At the same time, you do not want those devices to have any way of decrypting already submitted data.
This is a use case for \textbf{asymmetric encryption},
where there are two keys that together make a public/private key pair.
The \textit{public} key can only be used for encrypting data before it is submitted to a server,
so it can safely be sent to all tablets or browsers as part of the survey program.
The \textit{private} key in the pair can only be used to decrypt that data
so it can be accessed after it has been received.
The private key is therefore requested with data access.
It should be securely stored
and may not be shared with anyone not listed on the IRB.
Again, you must store the key pair in a secure location,
such as a secure note in a password manager,
as there is no way to recover your data if the private key is lost.\sidenote{
See the DIME guidelines for encryption in SurveyCTO/ODK at
\url{https://github.com/worldbank/dime-standards/blob/master/dime-research-standards/pillar-4-data-security/data-security-resources/surveycto-encryption-guidelines.md}}

Data encrypted this way can securely pass
through a server accessible by the data collection software provider
or by your local IT team,
and you should never decrypt it until it is downloaded on your computer.
If your data collection service allows you
to browse data while it remains on the server,
then the encryption is only implemented correctly
if you are asked for the key each time
or if it specifically enables the creation of ``publishable''
fields which are not subject to the encryption.

Finally, the data security standards that apply
when receiving confidential data from the field
also apply when transferring confidential data to the field.
In some survey software,
you can use the same encryption that allows you to receive data securely
from the field, to also send confidential data,
such as an identifying list of respondents, to the field.

\subsection{Receiving or sharing data securely}

If you receive confidential data from a partner organization
or share confidential data with your research team over the internet,
it must always be encrypted.
The best practice is to reduce the amount of confidential data
that is shared or transfer.
If you do not need the confidential information, you will simplify your workflow
by acquiring or sharing data without any confidential information in the first place.
Removing confidential information, for example through de-identifying the data
(which we will discuss in detail in Chapter 5), should always be your first preference.

However, some amount of confidential data usually needs to be transferred at some point in the project.
There are multiple options for sharing encrypted data.
If you need to collaborate on a confidential data set
then you can create an encrypted folder, as described above.
An encrypted folder can safely be shared using insecure modes of communication
such as email or third-party syncing services,
as long as you only share the key
using a secure service like a password manager.
Anyone who has access to both the encrypted folder \textit{and} the encryption key
can read and collaborate on the encrypted file.
If you only need a one-time transfer of confidential information,
for example to receive a data set from a partner organization,
setting up a permanent encrypted folder may be unnecessary.
Instead, we recommend sending data over password-protected
end-to-end encrypted file sharing services.\sidenote{
	One such service at the time of writing is \url{https://send.firefox.com/}}

\subsection{Backing up original data}
In addition to encrypting your data, you must protect it 
from being accidentally overwritten or deleted.
This is done through a back-up protocol.
Here is an example of such a protocol:

\begin{enumerate}
	\item Create an encrypted folder in your project folder.
	This should be on your computer, and could be in a synced folder.

	\item Download the original data from your data source to that encrypted folder.
	If your data source is a survey and the data was encrypted during data collection,
	then you will need \textit{both} the
	private key used during data collection to be able to download the data,
	\textit{and} the key used when you created the encrypted folder to save it there.
	This the working copy of your raw data, and the copy you will source for all cleaning and analysis.

	\item Create a second encrypted folder on an external drive or another computer that you can keep in a secure location on-site.
	Copy the data you just downloaded to this second encrypted folder.
	This is the local backup copy of the raw data
	that you can access quickly if the data on your local machine is deleted or corrupted.
	You should never work with this data on a day-to-day basis.
	You should not use the same encrypted folder or the same key as above,
	because if you use the same key and lose the key,
	then you will have lost access to both encrypted folders.
	If you have a physical safe where you can securely store the external drive,
	then you do not need to encrypt the data
	and thereby do not risk losing access by losing an encryption key.

	\item Finally, create a third encrypted folder.
	Either you can create this on your computer and upload it to a long-term cloud storage service (not a sync software),
	or you can create it on	another external hard drive or computer that you then store in a second location,
	for example, at another office of your organization.
	This is the ``golden master'' backup copy of the raw data,
	because it can recover your data even in the event of
	total loss of access to your office or its contents.
	You should never store the ``golden master'' copy in a synced folder,
	as it would be deleted in the cloud storage if it is deleted on your computer.
	You should also never work with this data;
	it exists only for recovery purposes.
\end{enumerate}

\noindent This handling satisfies the \textbf{3-2-1 rule} described in Chapter 2:
there are two on-site copies of the data and one off-site copy,
so the data can never be lost in case of hardware failure.
%needs wiki page
If you remain lucky, you will never have to access your ``master'' or ``golden master'' copies --
you just want to know it is there, safe, if you one day end up needing it.


Once the original raw data is securely stored and backed up,
it can be processed into a de-identified, clean dataset
through the steps described in the next chapter.
