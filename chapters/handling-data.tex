%------------------------------------------------

\begin{fullwidth}
Development research does not just \textit{involve} real people -- it also \textit{affects} real people.
Policy decisions are made every day using the results of briefs and studies,
and these can have wide-reaching consequences on the lives of millions.
As the range and importance of the policy-relevant questions
asked by development researchers grow,
so does the (rightful) scrutiny under which methods and results are placed.
This scrutiny involves two major components: data handling and analytical quality.
Performing at a high standard in both means that
consumers of research can have confidence in its conclusions,
and that research participants are appropriately protected.
What we call ethical standards in this chapter is a set of practices
for research quality and data privacy that address these two principles.

Neither quality nor privacy is an ``all-or-nothing'' objective.
We expect that teams will do as much as they can to make their work
conform to modern practices of credibility, transparency, and reproducibility.
Similarly, we expect that teams will ensure the privacy of participants in research
by intelligently assessing and proactively averting risks they might face.
We also expect teams will report what they have and have not done
in order to provide objective measures of a research product's performance in both.
Otherwise, reputation is the primary signal for the quality of evidence, and two failures may occur:
low-quality studies from reputable sources may be used as evidence when in fact they don't warrant it,
and high-quality studies from sources without an international reputation may be ignored.
Both these outcomes reduce the quality of evidence overall.
Even more importantly, they usually mean that credibility in development research accumulates at international institutions
and top global universities instead of the people and places directly involved in and affected by it.
Simple transparency standards mean that it is easier to judge research quality,
and making high-quality research identifiable also increases its impact.
This section provides some basic guidelines and resources
for using field data ethically and responsibly to publish research findings.
\end{fullwidth}

%------------------------------------------------

\section{Protecting confidence in development research}

The empirical revolution in development research
\index{transparency}\index{credibility}\index{reproducibility}
has led to increased public scrutiny of the reliability of research.\cite{rogers_2017}
Three major components make up this scrutiny: \textbf{reproducibility}.\cite{duvendack2017meant}, \textbf{transparency},\cite{christensen2018transparency} and \textbf{credibility},\cite{ioannidis2017power}.
Reproducibility is one key component of transparency.
Transparency is necessary for consumers of research products
to be able to determine the quality of the research process and the value of the evidence.
Without it, all evidence of credibility comes from reputation,
and it's unclear what that reputation is based on, since it's not transparent.

Development researchers should take these concerns particularly seriously.
Many development research projects are purpose-built to address specific questions,
and often use unique data or small samples.
This approach opens the door to working closely with the broader development community
to answer specific programmatic questions and general research inquiries.
However, almost by definition,
primary data that researchers use for such studies has never been reviewed by anyone else,
so it is hard for others to verify that it was collected, handled, and analyzed appropriately.
Maintaining confidence in research via the components of credibility, transparency, and reproducibility
is the most important way that researchers using primary data can avoid serious error,
and therefore these are not by-products but core components of research output.

\subsection{Research reproducibility}

Reproducible research means that the actual analytical processes you used are executable by others.\cite{dafoe2014science}
(We use ``reproducibility'' to refer to the code processes in a specific study.\sidenote{\url{http://datacolada.org/76}})
All your code files involving data cleaning, construction and analysis
should be public (unless they contain identifying information).
Nobody should have to guess what exactly comprises a given index,
or what controls are included in your main regression,
or whether or not you clustered standard errors correctly.
That is, as a purely technical matter, nobody should have to ``just trust you'',
nor should they have to bother you to find out what happens
if any or all of these things were to be done slightly differently.\cite{simmons2011false,simonsohn2015specification,wicherts2016degrees}
Letting people play around with your data and code is a great way to have new questions asked and answered
based on the valuable work you have already done.
Services like GitHub that expose your code development process are valuable resources here.
Such services can show things like modifications made in response to referee comments,
by having tagged version histories at each major revision.
These services can also use issue trackers and abandoned work branches
to document the research paths and questions you may have tried to answer
(but excluded from publication)
as a resource to others who have similar questions of their own data.

Secondly, reproducible research\sidenote{\url{https://dimewiki.worldbank.org/wiki/Reproducible_Research}}
enables other researchers to re-use your code and processes
to do their own work more easily and effectively in the future.
This may mean applying your techniques to their data
or implementing a similar structure in a different context.
As a pure public good, this is nearly costless.
The useful tools and standards you create will have high value to others.
If you are personally or professionally motivated by citations,
producing these kinds of resources can lead to that as well.
Therefore, your code should be written neatly with clear instructions and published openly.
It should be easy to read and understand in terms of structure, style, and syntax.
Finally, the corresponding dataset should be openly accessible
unless for legal or ethical reasons it cannot be.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Publishing_Data}}

Reproducibility and transparency are not binary concepts:
there’s a spectrum, starting with simple materials release.
But even getting that first stage right is a challenge.
An analysis of 203 empirical papers published in top economics journals in 2016
showed that less than 1 in 7 provided all the data and code
needed to assess computational reproducibility.\cite{galiani2017incentives}
A scan of the 90,000 datasets on the Harvard Dataverse
found that only 10% have the necessary files and documentation
for computational reproducibility
(and a check of 3,000 of those that met requirements
found that 85\% did not replicate).
Longer-term goals to meet reproducibility and transparency standards
include making tools for research transparency part and parcel
of the quest for efficiency gains in the research production function.
People seem to systematically underestimate the benefits
and overestimate the costs to adopting modern research practices.

\subsection{Research transparency}

Transparent research will expose not only the code,
but all the other research processes involved in developing the analytical approach.\sidenote{\url{http://www.princeton.edu/~mjs3/open_and_reproducible_opr_2017.pdf}}
This means that readers be able to judge for themselves if the research was done well,
and if the decision-making process was sound.
If the research is well-structured, and all of the relevant documentation\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_Documentation}} is shared,
this makes it as easy as possible for the reader to implement the same analysis.
Expecting process transparency is also an incentive for researchers to make better decisions,
be skeptical and thorough about their assumptions,
and, as we hope to convince you, make the process easier for themselves,
because it requires methodical organization that is labor-saving and efficient over the complete course of a project.

Tools like pre-registration, pre-analysis plans, and
\textbf{Registered Reports}\sidenote{\url{https://blogs.worldbank.org/impactevaluations/registered-reports-piloting-pre-results-review-process-journal-development-economics}}
can help with this process where they are available.
By pre-specifying a large portion of the research design,\sidenote{\url{https://www.bitss.org/2019/04/18/better-pre-analysis-plans-through-design-declaration-and-diagnosis/}}
a great deal of analytical planning has already been completed,
and at least some research questions are pre-committed for publication regardless of the outcome.
This is meant to combat the ``file-drawer problem'',\cite{simonsohn2014p}
and ensure that researchers are transparent in the additional sense that
all the results obtained from registered studies are actually published.
In no way should this be viewed as binding the hands of the researcher:
anything outside the original plan is just as interesting and valuable
as it would have been if the the plan was never published;
but having pre-committed to any particular inquiry makes its results
immune to a wide range of criticisms of specification searching or multiple testing.

Documenting a project in detail greatly increases transparency.
Many disciplines have a tradition of keeping a ``lab notebook'',
and adapting and expanding this process for the development
of lab-style working groups in development is a critical step.
This means explicitly noting decisions as they are made, and explaining the process behind them.
Documentation on data processing and additional hypotheses tested will be expected in the supplemental materials to any publication.
Careful documentation will also save the research team a lot of time during a project,
as it prevents you to have the same discussion twice (or more!),
since you have a record of why something was done in a particular way.
There are a number of available tools
that will contribute to producing documentation,
\index{project documentation}
but project documentation should always be an active and ongoing process,
not a one-time requirement or retrospective task.
New decisions are always being made as the plan begins contact with reality,
and there is nothing wrong with sensible adaptation so long as it is recorded and disclosed.
(Email is \textit{not} a note-taking service, because communications are rarely well-ordered and easy to delete.)

There are various software solutions for building documentation over time.
The \textbf{Open Science Framework}\sidenote{\url{https://osf.io/}} provides one such solution,
with integrated file storage, version histories, and collaborative wiki pages.
\textbf{GitHub}\sidenote{\url{https://github.com}} provides a transparent documentation system,\sidenote{\url{https://dimewiki.worldbank.org/wiki/Getting_started_with_GitHub}}
\index{task management}\index{GitHub}
in addition to version histories and wiki pages.
Such services offers multiple different ways to record the decision process leading to changes and additions,
track and register discussions, and manage tasks.
These are flexibles tool that can be adapted to different team and project dynamics,
but GitHub, unfortunately is less effective for file storage.
Each project has specific requirements for data, code, and documentation management,
and the exact shape of this process can be molded to the team's needs,
but it should be agreed upon prior to project launch.
This way, you can start building a project's documentation as soon as you start making decisions.

\subsection{Research credibility}

The credibility of research is traditionally a function of design choices.\cite{angrist2010credibility,ioannidis2005most}
Is the research design sufficiently powered through its sampling and randomization?
Were the key research outcomes pre-specified or chosen ex-post?
How sensitive are the results to changes in specifications or definitions?
Tools such as \textbf{pre-analysis plans}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Pre-Analysis_Plan}}
can be used to assuage these concerns for experimental evaluations
\index{pre-analysis plan}
by fully specifying some set of analysis intended to be conducted,
but they may feel like ``golden handcuffs'' for other types of research.\cite{olken2015promises}
Regardless of whether or not a formal pre-analysis plan is utilized,
all experimental and observational studies should be \textbf{pre-registered}\sidenote{
	\url{https://dimewiki.worldbank.org/wiki/Pre-Registration}}
simply to create a record of the fact that the study was undertaken.
This is increasingly required by publishers and can be done very quickly
using the \textbf{AEA} database,\sidenote{\url{https://www.socialscienceregistry.org/}}
the \textbf{3ie} database,\sidenote{\url{http://ridie.3ieimpact.org/}},
the \textbf{eGAP} database,\sidenote{\url{http://egap.org/content/registration/}},
or the \textbf{OSF} registry\sidenote{\url{https://osf.io/registries}} as appropriate.\sidenote{\url{http://datacolada.org/12}}
\index{pre-registration}

Garden varieties of research standards from journals, funders, and others feature both ex ante
(or ”regulation”) and ex post (or “verification”) policies.
Ex ante policies requires that the authors bear the burden
of ensuring they provide some set of materials before publication
and their quality meet some minimum standard.
Ex post policies require that authors make certain materials available to the public,
but their quality is not a direct condition for publication.
Still others have suggested “guidance” policies that would offer checklists
for which practices to adopt, such as reporting on whether and how
various practices were implemented.

With this ongoing rise of empirical research and increased public scrutiny of scientific evidence,
this is no longer enough to guarantee that findings will hold their credibility.
Even if your methods are highly precise,
your evidence is just as good as your data,
and there are plenty of mistakes that can be made between establishing a design and generating final results that would compromise its conclusions.
That is why transparency is key for research credibility.
It allows other researchers, and research consumers,
to verify the steps to a conclusion by themselves,
and decide whether their standards for accepting a finding as evidence are met.
Therefore we encourage you to work, gradually, towards improving
the documentation and release of your research materials,
and finding the tools and workflows that best match your project and team.
Every investment you make in documentation and transparency up front
protects your project down the line, particularly as these standards continue to tighten.
Since projects span over many years,
the records you will need to have available for publication are
only bound to increase by the time you do so.


%------------------------------------------------

\section{Ensuring privacy and security in research data}


Anytime you are collecting primary data in a development research project,\index{primary data}
you are almost certainly handling data that include \textbf{personally-identifying information (PII)}.\sidenote{\textbf{Personally-identifying information:}
	any piece or set of information that can be used to identify an individual research subject.
	\url{https://dimewiki.worldbank.org/wiki/De-identification\#Personally\_Identifiable\_Information}}
\index{personally-identifying information}
PII data contains information that can, without any transformation, be used to identify
individual people, households, villages, or firms that were included in \textbf{data collection}.
\index{data collection}
This includes names, addresses, and geolocations, and extends to personal information
\index{geodata}
such as email addresses, phone numbers, and financial information.
\index{de-identification}
It is important to keep in mind these data privacy principles not only for the individual respondent but also the PII data of their household members or other caregivers who are covered under the survey.
In some contexts this list may be more extensive --
for example, if you are working in an environment that is either small, specific,
or has extensive linkable data sources available to others,
information like someone's age and gender may be sufficient to identify them
even though these would not be considered PII in a larger context.
Therefore you will have to use careful judgment in each case
to decide which pieces of information fall into this category.\sidenote{
	\url{https://sdcpractice.readthedocs.io/en/latest/}}

In all cases where this type of information is involved,
you must make sure that you adhere to several core processes,
including approval, consent, security, and privacy.
If you are a US-based researcher, you will become familiar
with a set of governance standards known as ``The Common Rule''.\sidenote{\url{https://www.hhs.gov/ohrp/regulations-and-policy/regulations/common-rule/index.html}}
If you interact with European institutions or persons,
you will also become familiar with ``GDPR'',\sidenote{\url{http://blogs.lshtm.ac.uk/library/2018/01/15/gdpr-for-research-data/}}
a set of regulations governing data ownership and privacy standards.
In all settings, you should have a clear understanding of
who owns your data (it may not be you, even if you collect or possess it),
the rights of the people whose information is reflected there,
and the necessary level of caution and risk involved in
storing and transferring this information.
Due to the increasing scrutiny on many organizations
from recently advanced standards and rights,
these considerations are critically important.
Check with your organization if you have any legal questions;
in general, you are responsible to avoid taking any action that
knowingly or recklessly ignores these considerations.

\subsection{Ethical approval and consent processes}

For almost all data collection or research activities that involves PII data,
you will be required to complete some form of Institutional Review Board (IRB) process.
Most commonly this consists of a formal application for approval of a specific
protocol for consent, data collection, and data handling.
The IRB which has authority over your project is not always apparent,
particularly if your institution does not have its own.
It is customary to obtain an approval from the university IRB where one PI is affiliated,
and if work is being done in an international setting approval is often also required
from a local institution subject to local law.

The primary consideration of IRBs is the protection of the people whose data is being collected.
Many jurisdictions (especially those responsible to EU law) view all personal data
as being intrinsically owned by the persons who they describe.
This means that those persons have the right to refuse to participate in data collection
before it happens, as it is happening, or after it has already happened.
It also means that they must explicitly and affirmatively consent
to the collection, storage, and use of their information for all purposes.
Therefore, the development of these consent processes is of primary importance.
Ensuring that research participants are aware that their information
will be stored and may be used for various research purposes is critical.
There are special additional protections in place for vulnerable populations,
such as minors, prisoners, and people with disabilities,
and these should be confirmed with relevant authorities if your research includes them.

Make sure you have significant advance timing with your IRB submissions.
You may not begin data collection until approval is in place,
and IRBs may have infrequent meeting schedules
or require several rounds of review for an application to be completed.
If there are any deviations from an approved plan or expected adjustments,
report these as early as you can so that you can update or revise the protocol.
Particularly at universities, IRBs have the power to retroactively deny
the right to use data which was not collected in accordance with an approved plan.
This is extremely rare, but shows the seriousness of these considerations
since the institution itself may face governmental penalties if its IRB
is unable to enforce them. As always, as long as you work in good faith,
you should not have any issues complying with these expectations.

\subsection{Transmitting and storing data securely}

Raw data which contains PII \textit{must} be \textbf{encrypted}\sidenote{\url{https://dimewiki.worldbank.org/wiki/encryption}}
\index{encryption}
during data collection, storage, and transfer.
\index{data transfer}\index{data storage}
Most modern data collection software has features that, if enabled, make the first part straightforward.\sidenote{\url{https://dimewiki.worldbank.org/wiki/SurveyCTO_Form_Settings}}
However, secure storage and transfer are your responsibility.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_Security}}
There are plenty of options available to keep your data safe,
at different prices, from enterprise-grade solutions to combined free options.
You will also want to setup a password manager that allows you to share encryption keys inside your team.
These will vary in level of security and ease of use,
and sticking to a standard practice will make your life easier,
so agreeing on a protocol from the start of a project is ideal.

\subsection{Protecting personally-identifying information}

Most of the field research done in development involves human subjects.\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Human_Subjects_Approval}}
\index{human subjects}
As a researcher, you are asking people to trust you with personal information about themselves:
where they live, how rich they are, whether they have committed or been victims of crimes,
their names, their national identity numbers, and all sorts of other data.
PII data carries strict expectations about data storage and handling,
and it is the responsibility of the research team to satisfy these expectations.\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Research_Ethics}}
Your donor or employer will most likely require you to hold a certification from a source
such as Protecting Human Research Participants\sidenote{
\url{https://humansubjects.nih.gov/sites/hs/phrp/PHRP_Archived_Course_Materials.pdf}}
or the CITI Program.\sidenote{
\url{https://about.citiprogram.org/en/series/human-subjects-research-hsr/}}

In general, though, you shouldn't need to handle PII data very often.
Once data is securely collected and stored, the first thing you will generally do is \textbf{de-identify} it.\sidenote{\url{https://dimewiki.worldbank.org/wiki/De-identification}}
\index{de-identification}
(We will provide more detail on this in the chapter on data collection.)
This will create a working copy that can safely be shared among collaborators.
De-identified data should avoid, for example, you being sent back to every household
to alert them that someone dropped all their personal information on a public bus and we don't know who has it.
This simply means creating a copy of the data that contains no personally-identifiable information.
This data should be an exact copy of the raw data,
except it would be okay for it to be publicly released.\cite{matthews2011data}
Ideally, all machines used to store and process PII data are not only password protected, but also encrypted at the hard drive level
(most modern operating systems provide such a tool).
This means that even if you lose your computer with identifying data in it,
anyone who gets hold of it still cannot access the information.

Complete data publication, unlike reproducibility checks,
brings along with it a set of serious privacy concerns,
particularly when sensitive data is used in key analyses.
There are a number of tools developed to help researchers de-identify data
(\texttt{PII\_detection}\sidenote{\url{https://github.com/PovertyAction/PII\_detection}} from IPA,
\texttt{PII-scan}\sidenote{\url{https://github.com/J-PAL/PII-Scan}} from JPAL,
and \texttt{sdcMicro}\sidenote{\url{https://sdcpractice.readthedocs.io/en/latest/sdcMicro.html}} from the World Bank).
But is it ever possible to fully protect privacy in an era of big data?
One option is to add noise to data, as the US Census has proposed,
as it makes the trade-off between data accuracy and privacy explicit.
But there are no established norms for such “differential privacy” approaches:
most approaches fundamentally rely on judging “how harmful” disclosure would be.
