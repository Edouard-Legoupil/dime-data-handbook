% ----------------------------------------------------------------------------------------------

\begin{fullwidth}
Preparation for collaborative data work begins long before you collect any data,
and involves planning both the software tools you will use yourself
and the collaboration platforms and processes for your team.
In order to be prepared to work on the data you receive with a group,
you need to plan out the structure of your workflow in advance.
This means knowing which data sets and output you need at the end of the process,
how they will stay organized, what types of data you'll handle,
and whether the data will require special handling due to size or privacy considerations.
Identifying these details should help you map out the data needs for your project,
giving you and your team a sense of how information resources should be organized.
It's okay to update this map once the project is underway --
the point is that everyone knows what the plan is.

To implement this plan, you will need to prepare collaborative tools and workflows.
Changing software or protocols halfway through a project can be costly and time-consuming,
so it's important to think ahead about decisions that may seem of little consequence.
For example, things as simple as sharing services, folder structures, and filenames
can be extremely painful to alter down the line in any project.
Similarly, making sure to set up a self-documenting discussion platform
and version control processes
makes working together on outputs much easier from the very first discussion.
This chapter will discuss some tools and processes that
will help prepare you for collaboration and replication.
We will provide free, open-source, and platform-agnostic tools wherever possible,
and point to more detailed instructions when relevant.
(Stata is the notable exception here due to its current popularity in the field.)
Most have a learning and adaptation process,
meaning you will become most comfortable with each tool
only by using it in real-world work.
Get to know them well early on,
so that you do not spend a lot of time learning through trial and error.
\end{fullwidth}

% ----------------------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------------------

\section{Preparing a collaborative work environment}

Being comfortable using your computer and having the tools you need in reach is key.
This section provides a brief introduction to core concepts and tools
that can help you handle the work you will be primarily responsible for.
Some of these skills may seem elementary,
but thinking about simple things from a workflow perspective
can help you make marginal improvements every day you work
that add up to substantial gains over the course of many projects.
Together, these processes should form a collaborative workflow
that will greatly accelerate your team's ability to get tasks done
on every project you take on together.

Teams often develop their workflows over time,
solving new challenges as they arise.
This is good. But it is important to recognize
that there are a number of tasks that will exist for every project,
and that their corresponding workflows can be agreed on in advance.
These include documentation methods, software choices,
naming schema, organizing folders and outputs, collaborating on code,
managing revisions to files, and reviewing each other's work.
These tasks appear in almost every project,
and their solutions translate well between projects.
Therefore, there are large efficiency gains over time to
thinking in advance about the best way to do these tasks,
instead of throwing together a solution when the task arises.
This chapter will outline the main points to discuss within the team,
and suggest some common solutions for these tasks.

% ----------------------------------------------------------------------------------------------
\subsection{Setting up your computer}

First things first: turn on your computer.
Make sure you have fully updated the operating system,
that it is in good working order,
and that you have a \textbf{password-protected} login.
  \index{password protection}
All machines should have \textbf{hard disk encryption} enabled.
  \index{encryption}
Disk encryption is built-in on most modern operating systems;
the service is currently called BitLocker on Windows or FileVault on MacOS.
Disk encryption prevents your files from ever being accessed
without first entering the system password.
This is different from file-level encryption,
which makes individual files unreadable without a specific key.
(We will address that in more detail later.)
As with all critical passwords, your system password should be strong,
memorable, and backed up in a separate secure location.

Make sure your computer is backed up to prevent information loss.
  \index{backup}
Follow the \textbf{3-2-1 rule}: maintain 3 copies of all original or irreplaceable data,
on at least 2 different hardware devices you have access to,
with 1 offsite storage method.\sidenote{
  \url{https://www.backblaze.com/blog/the-3-2-1-backup-strategy/}}
One example of this setup is having one copy on your primary computer,
one copy on an external hard drive stored in a safe place,
and one copy in the cloud.
In this case, Dropbox and other automatic file sync services do not count as a cloud copy,
since other users can alter or delete them
unless you create a specific folder for this purpose that is not shared with anyone else.
  \index{Dropbox}

Ensure you know how to get the \textbf{absolute file path} for any given file.
Using the absolute file path, starting from the filesystem root,
means that the computer will never accidentally load the wrong file.
  \index{file paths}
On MacOS this will be something like \path{/users/username/dropbox/project/...},
and on Windows, \path{C:/users/username/Github/project/...}.
Use forward slashes (\texttt{/}) in filepaths for folders,
and whenever possible use only A-Z (the 26 English characters),
dashes (\texttt{-}), and underscores (\texttt{\_}) in folder names and filenames.
For emphasis: \textit{always} use forward slashes (\texttt{/}) in file paths in code,
just like in internet addresses. Do this even if you are using a Windows machine where
both forward and backward slashes are allowed, as your code will otherwise break
if anyone tries to run it on a Mac or Linux machine.
Making the structure of your directories a core part of your workflow is very important,
since otherwise you will not be able to reliably transfer the instructions
for replicating or carrying out your analytical work.

When you are working with others, you will most likely be using
some kind of \textbf{file sharing} software.
  \index{file sharing}
The exact services you use will depend on your tasks,
but in general, there are different approaches to file sharing, and the three discussed here are the most common.
\textbf{File syncing} is the most familiar method,
and is implemented by software like Dropbox and OneDrive.
  \index{file syncing}
Sync forces everyone to have the same version of every file at the same time,
which makes simultaneous editing difficult but other tasks easier.
They also have some security concerns which we will address later.
\textbf{Version control} is another method,
commonly implemented by tools like Git and GitHub.
  \index{version control}
Version control allows everyone to access different versions of files at the same time,
making simultaneous editing easier but some other tasks harder.
It is also only optimized for specific types of files.
Finally, \textbf{server storage} is the least-common method,
because there is only one version of the materials,
and simultaneous access must be carefully regulated.
  \index{server storage}
Server storage ensures that everyone has access
to exactly the same files and environment, and it also enables
high-powered computing processes for large and complex data.
All three file sharing methods are used for collaborative workflows,
and you should review the types of data work
that you will be doing, and plan which types of files
will live in which types of sharing services.
It is important to note that they are, in general, not interoperable:
you cannot have version-controlled files inside a syncing service,
or vice versa, without setting up complex workarounds,
and you cannot shift files between them without losing historical information.
Therefore, choosing the correct sharing service at the outset is essential.

% ----------------------------------------------------------------------------------------------
\subsection{Documenting decisions and tasks}

Once your technical and sharing workspace is set up,
you need to decide how you are going to communicate with your team.
The first habit that many teams need to break
is using instant communication for management and documentation.
Email is, simply put, not a system. It is not a system for anything. Neither is WhatsApp.\index{email}\index{WhatsApp}
These tools are developed for communicating ``now'' and this is what they do well.
They are not structured to manage group membership or to present the same information
across a group of people, or to remind you when old information becomes relevant.
They are not structured to allow people to collaborate over a long time or to review old discussions.
It is therefore easy to miss or lose communications from the past when they have relevance in the present.
Everything that is communicated over e-mail or any other instant medium should
immediately be transferred into a system that is designed to keep records.
We call these systems collaboration tools, and there are several that are very useful.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Collaboration_Tools}}
  \index{collaboration tools}

Many collaboration tools are web-based
so that everyone on your team can access them simultaneously
and have live discussions about tasks and processes.
Many are based on an underlying system known as ``Kanban''.\sidenote{
  \url{https://en.wikipedia.org/wiki/Kanban_board}}
This task-oriented system allows the team to create and assign tasks,
carry out discussions related to single tasks,
track task progress across time, and quickly see the overall project state.
These systems therefore link communication to specific tasks so that
the records related to decision making on those tasks is permanently recorded
and easy to find in the future when questions about that task come up.
One popular and free implementation of this system is found in GitHub project boards.
Other tools which currently offer similar features (but are not explicitly Kanban-based)
are GitHub Issues and Dropbox Paper.
Any specific list of software will quickly be outdated;
we mention these two as an example of one that is technically-organized and one that is chronologial.
Choosing the right tool for the right needs is essential to being satisfied with the workflow.
What is important is that your team chooses its systems and stick to those choices,
so that decisions, discussions, and tasks are easily reviewable long after they are completed.

Just like we use different file sharing tools for different types of files,
we can use different collaboration tools for different types of tasks.
Our team, for example, uses GitHub Issues for code-related tasks,
and Dropbox Paper for more managerial and office-related tasks.
GitHub creates incentives for writing down why changes were made
in response to specific discussions
as they are completed, creating naturally documented code.
It is useful also because tasks in Issues can clearly be tied to file versions.
On the other hand, Dropbox Paper provides a clean interface with task notifications,
and is very intuitive for people with non-technical backgrounds.
It is useful because tasks can be easily linked to other documents saved in Dropbox.
Therefore, it is a better tool for managing non-code-related tasks.
Neither of these tools require much technical knowledge;
they merely require an agreement and workflow design
so that the people assigning the tasks are sure to set them up in the appropriate system.

% ----------------------------------------------------------------------------------------------
\subsection{Choosing software}

Choosing the right working environments can make your work significantly easier.
  \index{software environments}
It may be difficult or costly to switch halfway through a project, so
think ahead about the different software to be used.
Take into account the different levels of techiness of team members,
how important it is to access files offline constantly,
as well as the type of data you will need to access and the security needed.
Big datasets require additional infrastructure and may overburden
the traditional tools used for small datasets,
particularly if you are trying to sync or collaborate on them.
Also consider the cost of licenses, the time to learn new tools,
and the stability of the tools.
There are few strictly right or wrong choices for software,
but what is important is that you have a plan in advance
and understand how your tools will interact with your work.

Ultimately, the goal is to ensure that you will be able to hold
your code environment constant over the lifecycle of a single project.
While this means you will inevitably have different projects
with different code environments, each one will be better than the last,
and you will avoid the extremely costly process of migrating a project
into a new code enviroment while it is still ongoing.
This can be set up down to the software level:
you should ensure that even specific versions of software
and the individual packages you use
are referenced or maintained so that they can be reproduced going forward
even if their most recent releases contain changes that would break your code.
  \index{software versions}
(For example, our command \texttt{ieboilstart} in the \texttt{ietoolkit} package
provides functionality to support Stata version stability.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/ieboilstart}})

Next, think about how and where you write and execute code.
This book focuses mainly on primary survey data,
so we are going to broadly assume that you are using ``small'' datasets
in one of the two most popular desktop-based packages: R or Stata.
(If you are using another language, like Python,
or working with big data projects on a server installation,
many of the same principles apply but the specifics will be different.)
The most visible part of working with code is a code editor,
since most of your time will be spent writing and re-writing your code.
This does not need to be the same program as the code runs in,
and the various members of your team do not need to use the same editor.
Using an external editor can be preferable since your editor will not crash if your code does,
and may offer additional features aimed at writing code well.
If you are working in R, \textbf{RStudio} is the typical choice.\sidenote{
  \url{https://www.rstudio.com}}
For Stata, the built-in do-file editor is the most widely adopted code editor,
but \textbf{Atom}\sidenote{\url{https://atom.io}} and
\textbf{Sublime}\sidenote{\url{https://www.sublimetext.com/}}
can also be configured to run Stata code externally,
while offering great code accessibility and quality features.
(We recommend setting up and becoming comfortable with one of these.)
For example, these editors can access an entire directory -- rather than a single file --
which gives you access to directory views and file management actions,
such as folder management, Git integration,
and simultaneous work with other types of files, without leaving the editor.

In our field of development economics,
Stata is currently the most commonly used statistical software,
and the built-in do-file editor the most common editor for programming Stata.
We focus on Stata-specific tools and instructions in this book.
Hence, we will use the terms `script' and `do-file'
interchangeably to refer to Stata code throughout.
This is only in part due to its popularity.
Stata is primarily a scripting language for statistics and data,
meaning that its users often come from economics and statistics backgrounds
and understand Stata to be encoding a set of tasks as a record for the future.
We believe that this must change somewhat:
in particular, we think that practitioners of Stata
must begin to think about their code and programming workflows
just as methodologically as they think about their research workflows.
and that people who adopt this approach will be dramatically
more capable in their analytical ability.
This means that they will be more productive when managing teams,
and more able to focus on the challenges of experimental design
and econometric analysis, rather than spending excessive time
re-solving problems on the computer.
To support this goal, this book also includes
an introductory Stata Style Guide
that we use in our work, which provides
some new standards for coding so that code styles
can be harmonized across teams for easier understanding and reuse of code.
Stata also has relatively few resources of this type available,
and the ones that we have created and shared here
we hope will be an asset to all its users.

% ----------------------------------------------------------------------------------------------
% ----------------------------------------------------------------------------------------------
\section{Organizing code and data}

Organizing files and folders is not a trivial task.
What is intuitive to one person rarely comes naturally to another,
and searching for files and folders is everybody's least favorite task.
As often as not, you come up with the wrong one,
and then it becomes very easy to create problems that require complex resolutions later.
This section will provide basic tips on managing the folder
that will store your project's data work.

We assume you will be working with code and data throughout your project.
We further assume you will want all your processes to be recorded
and easily findable at any point in time.
Maintaining an organized file structure for data work is the best way
to ensure that you, your teammates, and others
are able to easily advance, edit, and replicate your work in the future.
It also ensures that automated processes from code and scripting tools
are able to interact well with your work,
whether they are yours or those of others.
File organization makes your own work easier as well as more transparent,
and will make your code easier to combine with tools like version control systems
that aim to cut down on the amount of repeated tasks you have to perform.
It is worth thinking in advance about how to store, name, and organize
the different types of files you will be working with,
so that there is no confusion down the line
and everyone has interoperable expectations.

% ----------------------------------------------------------------------------------------------
\subsection{Organizing files and folder structures}

Agree with your team on a specific directory structure,
and set it up at the beginning of the research project
in your top-level shared folder (the one over which you can control access permissions).
This will prevent future folder reorganizations that may slow down your workflow and,
more importantly, ensure that your code files are always able to run on any machine.
To support consistent folder organization, DIME Analytics maintains \texttt{iefolder}\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/iefolder}}
as a part of our \texttt{ietoolkit} package.\index{\texttt{iefolder}}\index{\texttt{ietoolkit}}
This Stata command sets up a pre-standardized folder structure
for what we call the \texttt{DataWork} folder.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/DataWork_Folder}}
The \texttt{DataWork} folder includes folders for all the steps of a typical project.
  \index{\texttt{DataWork} folder}
Since each project will always have its own needs,
we have tried to make it as easy as possible to adapt when that is the case.
The main advantage of having a universally standardized folder structure
is that changing from one project to another requires less
time to get acquainted with a new organization scheme.
For our group, maintaining a single unified directory structure
across the entire portfolio of projects means that everyone
can easily move between projects without having to reorient
themselves to how files and folders are organized.

Our suggested file structure is not for everyone.
But if you do not already have a standard file structure across projects,
it is intended to be an easy template to start from.
This system operates by creating a \texttt{DataWork} folder at the project level,
and within that folder, it provides standardized directory structures
for each data source (in the primary data context, ``rounds'' of data collection).
For each, \texttt{iefolder} creates folders for raw encrypted data,
raw deidentified data, cleaned data, final data, outputs, and documentation.
In parallel, it creates folders for the code files
that move the data through this progression,
and for the files that manage final analytical work.
The command also has some flexibility for the addition of
folders for non-primary data sources, although this is less well developed.
The package also includes the \texttt{iegitaddmd} command,
which can place a \texttt{README.md} file in each of these folders.
These \textbf{Markdown} files provide an easy and Git-compatible way
to document the contents of every folder in the structure.
  \index{Markdown}

The \texttt{DataWork} folder may be created either inside
an existing project-based folder structure, or it may be created separately.
It's usually created by the leading RA in agreement with the PI.
Increasingly, our recommendation is to create the \texttt{DataWork} folder
separately from the project management materials,
reserving the ``project folder'' for contracts, Terms of Reference, briefs and other administrative or management work.
  \index{project folder}
This is so the project folder can be maintained in a synced location like Dropbox,
while the code folder can be maintained in a version-controlled location like GitHub.
(Remember, a version-controlled folder can \textit{never} be stored inside a synced folder,
because the versioning features are extremely disruptive to others
when the syncing utility operates on them, and vice versa.)
Nearly all code files and raw outputs (not datasets) are best managed this way.
This is because code files are usually \textbf{plaintext} files,
and non-technical files are usually \textbf{binary} files.\index{plaintext}\index{binary files}
It's also becoming more and more common for written outputs such as reports,
presentations and documentations to be written using plaintext
tools such as {\LaTeX} and dynamic documents.\index{{\LaTeX}}\index{dynamic documents}
Keeping such plaintext files in a version-controlled folder allows you
to maintain better control of their history and functionality.
Because of the high degree of dependence between code files depend and file structure,
you will be able to enforce better practices in a separate folder than in the project folder,
which will usually be managed by a PI, FC, or field team members.

Setting up the \texttt{DataWork} folder folder in a version-controlled directory
also enables you to use Git and GitHub for version control on your code files.
A \textbf{version control system} is required to manage changes to any technical file.
A good version control system tracks who edited each file and when,
and additionally provides a protocol for ensuring that conflicting versions are avoided.
This is important, for example, for your team
to be able to find the version of a presentation that you delivered to a donor,
or to understand why the significance level of your estimates has changed.
Everyone who has ever encountered a file named something like \texttt{final\_report\_v5\_LJK\_KLE\_jun15.docx}
can appreciate how useful such a system can be.


Most syncing services offer some kind of rudimentary version control;
these are usually enough to manage changes to binary files (such as Word and PowerPoint documents)
without needing to rely on dreaded filename-based versioning conventions.
For code files, however, a more detailed version control system is usually desirable.
We recommend using Git\sidenote{
  \textbf{Git:} a multi-user version control system for collaborating on and tracking changes to code as it is written.}
for all plaintext files.
Git tracks all the changes you make to your code,
and allows you to go back to previous versions without losing the information on changes made.
It also makes it possible to work on multiple parallel versions of the code,
so you don't risk breaking the code for other team members as you try something new.
The DIME file management and organization approach is designed with this in mind.

Once the \texttt{DataWork} folder's directory structure is set up,
you should adopt a file naming convention.
You will generally be working with two types of files:
``technical'' files, which are those that are accessed by code processes,
and ``non-technical'' files, which will not be accessed by code processes.
The former takes precedent: an Excel file is a technical file
even if it is a field log, because at some point it will be used by code.
We will not give much emphasis to files that are not linked to code here;
but you should make sure to name them in an orderly fashion that works for your team.
These rules will ensure you can find files within folders
and reduce the amount of time others will spend opening files
to find out what is inside them.
The main point to be considered is that files accessed by code face more restrictions\sidenote{
  \url{http://www2.stat.duke.edu/~rcs46/lectures_2015/01-markdown-Git/slides/naming-slides/naming-slides.pdf}},
since different software and operating systems read file names in different ways.
Some of the differences between the two naming approaches are major and may be new to you,
so below are a few examples.
Introducing spaces between words in a file name (including the folder path)
can break a file's path when it's read by code,
so while a Word document may be called \texttt{2019-10-30 Sampling Procedure Description.docx},
a related do file would have a name like \texttt{sampling-endline.do}.
Adding timestamps to binary files as in the example above can be useful,
as it is not straightforward to track changes using version control software.
However, for plaintext files tracked using Git, timestamps are an unnecessary distraction.
Similarly, technical files should never include capital letters,
as strings and file paths are case-sensitive in some software.
Finally, one organizational practice that takes some getting used to
is the fact that the best names from a coding perspective
are usually the opposite of those from an English perspective.
For example, for a deidentified household dataset from the baseline round,
you should prefer a name like \texttt{baseline-household-deidentified.dta},
rather than the opposite way around as occurs in natural language.
This ensures that all \texttt{baseline} data stays together,
then all \texttt{baseline-household} data,
and finally provides unique information about this specific file.

% ----------------------------------------------------------------------------------------------
\subsection{Documenting and organizing code}

Once you start a project's data work,
the number of scripts, datasets, and outputs that you have to manage will grow very quickly.
This can get out of hand just as quickly,
so it's important to organize your data work and follow best practices from the beginning.
Adjustments will always be needed along the way,
but if the code is well-organized, they will be much easier to make.
Below we discuss a few crucial steps to code organization.
They all come from the principle that code is an output by itself,
not just a means to an end,
and should be written thinking of how easy it will be for someone to read it later.

Code documentation is one of the main factors that contribute to readability.
Start by adding a code header to every file.
This should include simple things such as the purpose of the script and the name of the person who wrote it.
If you are using a version control software,
the last time a modification was made and the person who made it will be recorded by that software.
Otherwise, you should include it in the header.
Finally, use the header to track the inputs and outputs of the script.
When you are trying to track down which code creates which data set, this will be very helpful.
While there are other ways to document decisions related to creating code
(GitHub offers a lot of different documentation options, for example),
the information that is relevant to understand the code should always be written in the code file.

In the script, alongside the code, are two types of comments that should be included.
The first type of comment describes what is being done.
This might be easy to understand from the code itself
if you know the language well enough and the code is clear,
but often it is still a great deal of work to reverse-engineer the code's intent.
Writing the task in plain English (or whichever language you communicate with your team on)
will make it easier for everyone to read and understand the code's purpose
-- and also for you to think about your code as you write it.
The second type of comment explains why the code is performing a task in a particular way.
As you are writing code, you are making a series of decisions that
(hopefully) make perfect sense to you at the time.
These are often highly specialized and may exploit a functionality
that is not obvious or has not been seen by others before.
Even you will probably not remember the exact choices that were made in a couple of weeks.
Therefore, you must document your precise processes in your code.

Code organization means keeping each piece of code in an easily findable location.
  \index{code organization}
Breaking your code into independently readable ``chunks'' is one good practice on code organization,
because it ensures each component does not depend on a complex program state
created by other chunks that are not obvious from the immediate context.
One way to do this is to create sections where a specific task is completed.
So, for example, if you want to find the line in your code where a variable was created,
you can go straight to \texttt{PART 2: Create new variables},
instead of reading line by line through the entire code.
RStudio, for example, makes it very easy to create sections,
and it compiles them into an interactive script index for you.
In Stata, you can use comments to create section headers,
though they're just there to make the reading easier and don't have functionality.
Adding an index to the header by copying and pasting section titles is the easiest way to create a code map.
You can then add and navigate through them using the \texttt{find} command.
Since Stata code is harder to navigate, as you will need to scroll through the document,
it's particularly important to avoid writing very long scripts.
Therefore, in Stata at least, you should also consider breaking code tasks down
into separate do-files, since there is no limit on how many you can have,
how detailed their names can be, and no advantage to writing longer files.
One reasonable rule of thumb is to not write do-files that have more than 200 lines.
This is an arbitrary limit, just like the standard restriction of each line to 80 characters:
it seems to be ``enough but not too much'' for most purposes.

To bring all these smaller code files together, you must maintain a master script.
  \index{master do-file}
A master script is the map of all your project's data work
which serves as a table of contents for the instructions that you code.
Anyone should be able to follow and reproduce all your work from
raw data to all outputs by simply running this single script.
By follow, we mean someone external to the project who has the master script and all the input data can
(i) run all the code and recreate all outputs,
(ii) have a general understanding of what is being done at every step, and
(iii) see how codes and outputs are related.
The master script is also where all the settings are established,
such as versions, folder paths, functions, and constants used throughout the project.

\texttt{iefolder} creates these as master do-files.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Master\_Do-files}}
Master scripts are a key element of code organization and collaboration,
and we will discuss some important features soon.
The master script should mimic the structure of the \texttt{DataWork} folder.
This is done through the creation of globals (in Stata) or string scalars (in R).
These coding shortcuts can refer to subfolders,
so that those folders can be referenced without repeatedly writing out their absolute file paths.
Because the \texttt{DataWork} folder is shared by the whole team,
its structure is the same in each team member's computer.
The only difference between machines should be
the path to the project root folder, i.e. the highest-level shared folder, which in the context of \texttt{iefolder} is the \texttt{DataWork} folder.
This is reflected in the master script in such a way that
the only change necessary to run the entire code from a new computer
is to change the path to the project folder to reflect the filesystem and username.
The code in \texttt{stata-master-dofile.do} shows how folder structure is reflected in a master do-file.

In order to maintain these practices and ensure they are functioning well,
you should agree with your team on a plan to review code as it is written.
  \index{code review}
Reading other people's code is the best way to improve your coding skills.
And having another set of eyes on your code will make you more comfortable with the results you find.
It's normal (and common) to make mistakes as you write your code.
Reading it again to organize and comment it as you prepare it to be reviewed will help you identify them.
Try to have a code review scheduled frequently,
every time you finish writing a piece of code, or complete a small task.
If you wait for a long time to have your code reviewed, and it gets too complex,
preparation and code review will require more time and work,
and that is usually the reason why this step is skipped.
One other important advantage of code review if that
making sure that the code is running properly on other machines,
and that other people can read and understand the code easily,
is the easiest way to be prepared in advance for a smooth project handover.

% ----------------------------------------------------------------------------------------------
\subsection{Output management}

The final task that needs to be discussed with your team is the best way to manage output files.
A great number of outputs will be created during the course of a project,
and these will include both raw outputs such as tables and graphs
and final products such as presentations, papers and reports.
When the first outputs are being created, agree on where to store them,
what softwares and formats to use, and how to keep track of them.

% Where to store outputs
Decisions about storage of outputs are made easier by technical constraints.
As discussed above, version control systems like Git are a great way to manage
plaintext files, and sync softwares such as Dropbox are better for binary files.
Outputs will similarly come in these two formats, depending on your software.
Binary outputs like Excel files, PDFs, PowerPoints, or Word documents can be kept in a synced folder.
Raw outputs in plaintext formats like \texttt{.tex} and \texttt{.eps}
can be created from most analytical software and managed with Git.
Tracking plaintext outputs with Git makes it easier to identify changes that affect results.
If you are re-running all of your code from the master when significant changes to the code are made,
the outputs will be overwritten, and changes in coefficients and number of observations, for example,
will be highlighted for you to review.
In fact, one of the most effective ways to check code quickly
is simply to commit all your code and outputs using Git,
then re-run the entire thing and examine any flagged changes in the directory.

No matter what choices you make,
you will need to make updates to your outputs quite frequently.
And anyone who has tried to recreate a graph after a few months probably knows
that it can be hard to remember where you saved the code that created it.
Here, naming conventions and code organization play a key role
in not re-writing scripts again and again.
It is common for teams to maintain one analyisis file or folder with ``exploratory analysis'',
which are pieces of code that are stored only to be found again in the future,
but not cleaned up to be included in any outputs yet.
Once you are happy with a result or output,
it should be named and moved to a dedicated location.
It's typically desirable to have the names of outputs and scripts linked,
so, for example, \texttt{factor-analysis.do} creates \texttt{factor-analysis-f1.eps} and so on.
Document output creation in the Master script that runs these files,
so that before the line that runs a particular analysis script
there are a few lines of comments listing
data sets and functions that are necessary for it to run,
as well as all outputs created by that script.

% What software to use
Compiling the raw outputs from your statistical software into useful formats
is the final step in producing research outputs for public consumption.
Though formatted text software such as Word and PowerPoint are still prevalent,
researchers are increasingly choosing to prepare final outputs
like documents and presentations using {\LaTeX}.\sidenote{
  \url{https://www.latex-project.org}} \index{{\LaTeX}.}
{\LaTeX} is a document preparation system that can create both text documents and presentations.
The main advantage is that {\LaTeX} uses plaintext for all formatting,
and it is necessary to learn its specific markup convention to use it.
The main advantage of using {\LaTeX} is that you can write dynamic documents,
that import inputs every time they are compiled.
This means you can skip the copying and pasting whenever an output is updated.
Because it's written in plaintext, it's also easier to control and document changes using Git.
Creating documents in {\LaTeX} using an integrated writing environment such as TeXstudio, TeXmaker or LyX
is great for outputs that focus mainly on text,
but include small chunks of code and static code outputs.
This book, for example, was written in {\LaTeX}\sidenote{\url{https://www.latex-project.org} and \url{https://github.com/worldbank/DIME-LaTeX-Templates}} and managed on GitHub\sidenote{\url{https://github.com/worldbank/d4di}}.

Another option is to use the statistical software's dynamic document engines.
This means you can write both text (in Markdown) and code in the script,
and the result will usually be a PDF or \texttt{html} file including code, text, and outputs.
Dynamic document tools are better for including large chunks of code and dynamically created graphs and tables,
but formatting these can be much trickier and less full-featured than other editors.
So dynamic documents can be great for creating appendices
or quick documents with results as you work on them,
but are not usuall considered for final papers and reports.
RMarkdown\sidenote{\url{https://rmarkdown.rstudio.com/}} is the most widely adopted solution in R.
There are also different options for Markdown in Stata,
such as \texttt{markstat},\sidenote{\url{https://data.princeton.edu/stata/markdown}}
Stata 15 dynamic documents,\sidenote{\url{https://www.stata.com/new-in-stata/markdown/}}
\texttt{webdoc},\sidenote{\url{http://repec.sowi.unibe.ch/stata/webdoc/index.html}} and
\texttt{texdoc}.\sidenote{\url{http://repec.sowi.unibe.ch/stata/texdoc/index.html}}

Whichever options you choose,
agree with your team on what tools will be used for what outputs, and
where they will be stored before you start creating them.
Take into account ease of use for different team members, but
keep in mind that learning how to use a new tool may require some
time investment upfront that will be paid off as your project advances.


% ----------------------------------------------------------------------------------------------

\codeexample{stata-master-dofile.do}{./code/stata-master-dofile.do}
