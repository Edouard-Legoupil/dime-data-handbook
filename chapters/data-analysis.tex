%------------------------------------------------

\begin{fullwidth}
Transforming raw data into a substantial contribution to scientific knowledge
requires a mix of subject expertise, programming skills,
and statistical and econometric knowledge.
The process of data analysis is typically
a back-and-forth discussion between people
with differing skill sets.
An essential part of the process is translating the
raw data received from the field into economically meaningful indicators.
To effectively do this in a team environment,
data, code and outputs must be well-organized,
with a clear system for version control,
and analytical scripts structured such that any member of the research team can run them.
Putting in time upfront to structure data work well
pays substantial dividends throughout the process.

In this chapter, we first cover data management:
how to organize your data work at the start of a project
so that coding the analysis itself is straightforward.
This includes setting up folders, organizing tasks, master scripts,
and putting in place a version control system
so that your work is easy for all research team members to follow,
and meets standards for transparency and reproducibility.
Second, we turn to de-identification,
a critical step when working with any personally-identified data.
In the third section, we offer detailed guidance on data cleaning,
from identifying duplicate entries to labeling and annotating raw data,
and how to transparently document the cleaning process.
Section four focuses on how to transform your clean data
into the actual indicators you will need for analysis,
again emphasizing the importance of transparent documentation.
Finally, we turn to analysis itself.
We do not offer instructions on how to conduct specific analyses,
as that is determined by research design;
rather, we discuss how to structure analysis code,
and how to automate common outputs so that your analysis is fully reproducible.

\end{fullwidth}

%------------------------------------------------




\section{Constructing analysis datasets}

% What is construction -------------------------------------
The third stage is construction of the dataset you will use for analysis.
It is at this stage that the cleaned data is transformed into analysis-ready data, 
by integrating different datasets and creating derived variables 
(dummies, indices, and interactions, to name a few),
as planned during research design\index{Research design},
and using the pre-analysis plan as a guide.\index{Pre-analysis plan}
During this process, the data points will typically be reshaped and aggregated
so that level of the dataset goes from the unit of observation in the survey 
to the unit of analysis.\sidenote{\url{
		https://dimewiki.worldbank.org/Unit\_of\_Observation}}


A constructed dataset is built to answer an analysis question.
Since different pieces of analysis may require different samples,
or even different units of observation,
you may have one or multiple constructed datasets,
depending on how your analysis is structured.
Don't worry if you cannot create a single, ``canonical'' analysis dataset.
It is common to have many purpose-built analysis datasets.
Think of an agricultural intervention that was randomized across villages
and only affected certain plots within each village.
The research team may want to run household-level regressions on income,
test for plot-level productivity gains,
and check if village characteristics are balanced.
Having three separate datasets for each of these three pieces of analysis
will result in much cleaner do-files than if they all started from the same dataset.

\subsection{Fitting construction into the data workflow}

Construction is done separately from data cleaning for two reasons.
First, it clearly differentiates correction of data entry errors
(necessary for all interactions with the data)
from creation of analysis indicators (necessary only for the specific analysis).
Second, it ensures that variable definition is consistent across data sources.
Unlike cleaning, construction can create many outputs from many inputs.
Let's take the example of a project that has a baseline and an endline survey.
Unless the two instruments are exactly the same,
which is preferable but often not the case,
the data cleaning for them will require different steps,
and therefore will be done separately.
However, you still want the constructed variables to be calculated in the same way, so they are comparable.
To do this, you will require at least two cleaning scripts,
and a single one for construction.

Construction of the analysis data should be done right after data cleaning and before data analysis starts,
according to the pre-analysis plan.\index{Pre-analysis plan}
In practice, however, as you analyze the data,
different constructed variables may become necessary,
as well as subsets and other alterations to the data,
and you will need to adjust the analysis data accordingly.
Even if construction and analysis are done concurrently,
you should always do the two in separate scripts.
If every script that creates a table starts by loading a dataset,
subsetting it, and manipulating variables,
any edits to construction need to be replicated in all scripts.
This increases the chances that at least one of them will have a different sample or variable definition.
Doing all variable construction in a single, separate script helps
avoid this and ensure consistency across different outputs.

\subsection{Integrating different data sources}
Often, you will combine or merge information from different data sources together
in order to create the analysis dataset.
For example, you may merge administrative data with survey data
to include demographic information in your analysis,
or you may want to integrate geographic information
in order to construct indicators or controls based on the location of observations.
To do this, you will need to consider the unit of observation for each dataset,
and the identifying variable, to understand how they can be merged.

If the datasets you need to join have the same unit of observation,
merging may be straightforward.
The simplest case is merging datasets at the same unit of observation
which use a consistent, uniquely and fully identifying ID variable. 
For example, in the case of a panel survey for firms,
you may merge baseline and endline data using the firm identification number.
In many cases, however, 
datasets at the same unit of observation may not use a consistent numeric identifier.
Identifiers that are string variables, such as names, 
often contain spelling mistakes or irregularities in capitalization, spacing or ordering. 
In this case, you will need to do a \textbf{fuzzy match}, 
to link observations that have similar identifiers. 
In these cases, you will need to extensively analyze the merging patterns
and understand what units are present in one dataset but not the other,
as well as be able to resolve fuzzy or imperfect matching.
There are some commands such as \texttt{reclink} in Stata
that can provide some useful utilities,
but often a large amount of close examination is necessary
in order to figure out what the matching pattern should be
and how to accomplish it in practice through your code.

In other cases, you will need to join data sources that have different units of observation.
For example, you might be overlaying road location data with household data,
using a spatial match,
or combining school administrative data, such as attendance records and test scores,
with household demographic characteristics from a survey.
Sometimes these cases are conceptually straightforward.
For example, merging a dataset of health care providers
with a dataset of patients comes with a clear linking relation between the two;
the challenge usually occurs in correctly defining statistical aggregations
if the merge is intended to result in a dataset at the provider level.
However, other cases may not be designed with the intention to be merged together,
such as a dataset of infrastructure access points, for example, water pumps or schools
and a dataset of household locations and roads.
In those cases, a key part of the research contribution is figuring out what
a useful way to combine the datasets is.
Since these conceptual constructs are so important
and so easy to imagine different ways to do,
it is especially important that these data integrations are not treated mechanically
and are extensively documented separately from other data construction tasks.

Integrating different datasets may involve changing the structure of the data,
e.g. changing the unit of observation through collapses or reshapes.
This should always be done with great care. 
Two issues to pay extra attention to are missing values and dropped observations.
Merging, reshaping and aggregating data sets can change both the total number of observations
and the number of observations with missing values.
Make sure to read about how each command treats missing observations and,
whenever possible, add automated checks in the script that throw an error message if the result is different than what you expect.
If you are subsetting your data,
drop observations explicitly,
indicating why you are doing that and how the data set changed.

\subsection{Constructing analytical variables}
Once you have assembled your different data sources, 
it's time to create the specific indicators of interest for analysis. 
New variables should be assigned functional names, 
and the dataset ordered such that related variables are together.
Adding notes to each variable will make your dataset more user-friendly.


Before constructing new variables,
you must check and double-check the value-assignments of questions,
as well as the units and scales.
This is when you will use the knowledge of the data and the documentation you acquired during cleaning.
First, check that all categorical variables have the same value assignment, i.e.,
that labels and levels have the same correspondence across variables that use the same options.
For example, it's possible that in one question \texttt{0} means ``no'' and \texttt{1} means ``yes'',
while in another one the same answers were coded as \texttt{1} and \texttt{2}.
(We recommend coding binary questions as either \texttt{1} and \texttt{0} or \texttt{TRUE} and \texttt{FALSE},
so they can be used numerically as frequencies in means and as dummies in regressions.
Note that this implies re-expressing categorical variables like \texttt{sex} to binary variables like \texttt{woman}.)
Second, make sure that any numeric variables you are comparing are converted to the same scale or unit of measure.
You cannot add one hectare and two acres and get a meaningful number.

You will also need to decide how to handle any outliers or unusual values identified during data cleaning. 
How to treat outliers is a question for the research team (as there are multiple possible approaches),
but make sure to note what decision was made and why.
Results can be sensitive to the treatment of outliers,
so keeping the original variable in the dataset will allow you to test how much it affects the estimates.
These points also apply to imputation of missing values and other distributional patterns.

Finally, creating a panel with survey data involves additional timing complexities.
It is common to construct indicators soon after receiving data from a new survey round.
However, creating indicators for each round separately increases the risk of using different definitions every time.
Having a well-established definition for each constructed variable helps prevent that mistake,
but the best way to guarantee it won't happen is to create the indicators for all rounds in the same script.
Say you constructed variables after baseline, and are now receiving midline data.
Then the first thing you should do is create a cleaned panel dataset,
ignoring the previous constructed version of the baseline data.
The \texttt{iecodebook append} subcommand will help you reconcile and append the cleaned survey rounds.
After that, adapt a single variable construction script so it can be used on the panel dataset as a whole.
In addition to preventing inconsistencies,
this process will also save you time and give you an opportunity to review your original code.


\subsection{Documenting variable construction}

Because data construction involves translating concrete data points to more abstract measurements,
it is important to document exactly how each variable is derived or calculated.
Adding comments to the code explaining what you are doing and why is a crucial step both to prevent mistakes and to guarantee transparency.
To make sure that these comments can be more easily navigated,
it is wise to start writing a variable dictionary as soon as you begin making changes to the data.
Carefully record how specific variables have been combined, recoded, and scaled,
and refer to those records in the code.
This can be part of a wider discussion with your team about creating protocols for variable definition,
which will guarantee that indicators are defined consistently across projects.
When all your final variables have been created,
you can use the \texttt{iecodebook export} subcommand to list all variables in the dataset,
and complement it with the variable definitions you wrote during construction to create a concise metadata document.
Documentation is an output of construction as relevant as the code and the data.
Someone unfamiliar with the project should be able to understand the contents of the analysis datasets,
the steps taken to create them,
and the decision-making process through your documentation.
The construction documentation will complement the reports and notes created during data cleaning.
Together, they will form a detailed account of the data processing.

%------------------------------------------------

\section{Writing data analysis code}

% Intro --------------------------------------------------------------
When data is cleaned and indicators constructed, you are ready to generate analytical outputs.
\index{data analysis}
There are many existing resources for data analysis, such as
\textit{R for Data Science};\sidenote{\url{https://r4ds.had.co.nz}}
\textit{A Practical Introduction to Stata};\sidenote{\url{https://scholar.harvard.edu/files/mcgovern/files/practical\_introduction\_to\_stata.pdf}}
\textit{Mostly Harmless Econometrics};\sidenote{\url{https://www.researchgate.net/publication/51992844\_Mostly\_Harmless\_Econometrics\_An\_Empiricist's\_Companion}}
and \textit{Causal Inference: The Mixtape}.\sidenote{\url{https://scunning.com/mixtape.html}}
We focus on how to \textit{code} data analysis, rather than how to conduct specific analyses.

\subsection{Organizing analysis code}

The analysis stage usually starts with a process we call exploratory data analysis.
This is when you are trying different things and looking for patterns in your data.
It progresses into final analysis when your team starts to decide what are the main results,
those that will make it into the research output.
The way you deal with code and outputs for exploratory and final analysis is different.
During exploratory data analysis,
you will be tempted to write lots of analysis into one big, impressive, start-to-finish script.
It subtly encourages poor practices such as not clearing the workspace and not reloading the constructed dataset before each analysis task.
To avoid mistakes, it's important to take the time
to organize the code that you want to use again in a clean manner.

A well-organized analysis script starts with a completely fresh workspace
and explicitly loads data before analyzing it, for each output it creates.
This setup encourages data manipulation to be done earlier in the workflow
(that is, during construction).
It also and prevents you from accidentally writing pieces of analysis code that depend on one another
and require manual instructions for all necessary chunks of code to be run in the right order.
Each chunk of analysis code should run completely independently of all other code,
except for the master script.
You could go as far as coding every output in a separate script (although you usually won't).

There is nothing wrong with code files being short and simple.
In fact, analysis scripts should be as simple as possible,
so whoever is reading them can focus on the econometrics, not the coding.
All research questions and statistical decisions should be very explicit in the code,
and should be very easy to detect from the way the code is written.
This includes clustering, sampling, and control variables, to name a few.
If you have multiple analysis datasets,
each of them should have a descriptive name about its sample and unit of observation.
As your team comes to a decision about model specification,
you can create globals or objects in the master script to use across scripts.
This is a good way to make sure specifications are consistent throughout the analysis.
Using pre-specified globals or objects also makes your code more dynamic,
so it is easy to update specifications and results without changing every script.
It is completely acceptable to have folders for each task,
and compartmentalize each analysis as much as needed.

To accomplish this, you will need to make sure that you have an effective data management system,
including naming, file organization, and version control.
Just like you did with each of the analysis datasets,
name each of the individual analysis files descriptively.
Code files such as \path{spatial-diff-in-diff.do},
\path{matching-villages.R}, and \path{summary-statistics.py}
are clear indicators of what each file is doing, and allow you to find code quickly.
If you intend to numerically order the code as they appear in a paper or report,
leave this to near publication time.

\subsection{Visualizing data}

\textbf{Data visualization}\sidenote{\url{https://dimewiki.worldbank.org/Data\_visualization}} \index{data visualization}
is increasingly popular, and is becoming a field in its own right.\cite{healy2018data,wilke2019fundamentals}
Whole books have been written on how to create good data visualizations,
so we will not attempt to give you advice on it.
Rather, here are a few resources we have found useful.
The Tapestry conference focuses on ``storytelling with data''.\sidenote{
	\url{https://www.youtube.com/playlist?list=PLb0GkPPcZCVE9EAm9qhlg5eXMgLrrfMRq}}
\textit{Fundamentals of Data Visualization} provides extensive details on practical application;\sidenote{
	\url{https://serialmentor.com/dataviz}}
as does \textit{Data Visualization: A Practical Introduction}.\sidenote{
	\url{http://socvis.co}}
Graphics tools like Stata are highly customizable.
There is a fair amount of learning curve associated with extremely-fine-grained adjustment,
but it is well worth reviewing the graphics manual.\sidenote{\url{https://www.stata.com/manuals/g.pdf}}
For an easier way around it, Gray Kimbrough's \textit{Uncluttered Stata Graphs}
code is an excellent default replacement for Stata graphics that is easy to install.\sidenote{
	\url{https://graykimbrough.github.io/uncluttered-stata-graphs}}
If you are an R user, the \textit{R Graphics Cookbook}\sidenote{\url{https://r-graphics.org}}
is a great resource for the its popular visualization package \texttt{ggplot}\sidenote{
	\url{https://ggplot2.tidyverse.org}}.
But there are a variety of other visualization packages,
such as \texttt{highcharter},\sidenote{\url{http://jkunst.com/highcharter}}
\texttt{r2d3},\sidenote{\url{https://rstudio.github.io/r2d3}}
\texttt{leaflet},\sidenote{\url{https://rstudio.github.io/leaflet}}
and \texttt{plotly},\sidenote{\url{https://plot.ly/r}} to name a few.
We have no intention of creating an exhaustive list, but this is a good place to start.

We attribute some of the difficulty of creating good data visualization
to writing code to create them.
Making a visually compelling graph would already be hard enough if
you didn't have to go through many rounds of googling to understand a command.
The trickiest part of using plot commands is to get the data in the right format.
This is why we created the \textbf{Stata Visual Library}\sidenote{
	\url{https://worldbank.github.io/Stata-IE-Visual-Library}},
which has examples of graphs created in Stata and curated by us.\sidenote{A similar resource for R is \textit{The R Graph Gallery}. \\\url{https://www.r-graph-gallery.com}}
The Stata Visual Library includes example datasets to use with each do-file,
so you get a good sense of what your data should look like
before you can start writing code to create a visualization.

\subsection{Exporting analysis outputs}

Our team has created a few products to automate common outputs and save you
precious research time.
The \texttt{ietoolkit} package includes two commands to export nicely formatted tables.
\texttt{iebaltab}\sidenote{\url{https://dimewiki.worldbank.org/iebaltab}}
creates and exports balance tables to excel or {\LaTeX}.
\texttt{ieddtab}\sidenote{\url{https://dimewiki.worldbank.org/ieddtab}}
does the same for difference-in-differences regressions.
It also includes a command, \texttt{iegraph},\sidenote{
	\url{https://dimewiki.worldbank.org/iegraph}}
to export pre-formatted impact evaluation results graphs.

It's okay to not export each and every table and graph created during exploratory analysis.
Final analysis scripts, on the other hand, should export final outputs,
which are ready to be included to a paper or report.
No manual edits, including formatting, should be necessary after exporting final outputs --
those that require copying and pasting edited outputs,
in particular, are absolutely not advisable.
Manual edits are difficult to replicate,
and you will inevitably need to make changes to the outputs.
Automating them will save you time by the end of the process.
However, don't spend too much time formatting tables and graphs until you are ready to publish.\sidenote{
	For a more detailed discussion on this, including different ways to export tables from Stata, see \url{https://github.com/bbdaniels/stata-tables}}
Polishing final outputs can be a time-consuming process,
and you want to it as few times as possible.

We cannot stress this enough:
don't ever set up a workflow that requires copying and pasting results.
Copying results from Excel to Word is error-prone and inefficient.
Copying results from a software console is risk-prone,
even more inefficient, and totally unnecessary.
There are numerous commands to export outputs from both R and Stata to a myriad of formats.\sidenote{
	Some examples are \url{http://repec.sowi.unibe.ch/stata/estout}{\texttt{estout}}, \url{https://www.princeton.edu/~otorres/Outreg2.pdf}{\texttt{outreg2}},
	and \url{https://www.benjaminbdaniels.com/stata-code/outwrite}{\texttt{outwrite}} in Stata,
	and \url{https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf}{\texttt{stargazer}}
	and \url{https://ggplot2.tidyverse.org/reference/ggsave.html}{\texttt{ggsave}} in R.}
Save outputs in accessible and, whenever possible, lightweight formats.
Accessible means that it's easy for other people to open them.
In Stata, that would mean always using \texttt{graph export} to save images as
\texttt{.jpg}, \texttt{.png}, \texttt{.pdf}, etc.,
instead of \texttt{graph save},
which creates a \texttt{.gph} file that can only be opened through a Stata installation.
Some publications require ``lossless'' TIFF or EPS files, which are created by specifying the desired extension.
Whichever format you decide to use, remember to always specify the file extension explicitly.
For tables there are less options and more consideration to be made.
Exporting table to \texttt{.tex} should be preferred.
Excel \texttt{.xlsx} and \texttt{.csv} are also commonly used,
but require the extra step of copying the tables into the final output.
The amount of work needed in a copy-paste workflow increases
rapidly with the number of tables and figures included in a research output,
and so do the chances of having the wrong version a result in your paper or report.

If you need to create a table with a very particular format
that is not automated by any command you know, consider writing it manually
(Stata's \texttt{filewrite}, for example, allows you to do that).
This will allow you to write a cleaner script that focuses on the econometrics,
and not on complicated commands to create and append intermediate matrices.
To avoid cluttering your scripts with formatting and ensure that formatting is consistent across outputs,
define formatting options in an R object or a Stata global and call them when needed.

Keep in mind that final outputs should be self-standing.
This means it should be easy to read and understand them with only the information they contain.
Make sure labels and notes cover all relevant information, such as sample,
unit of observation, unit of measurement and variable definition.\sidenote{
	\url{https://dimewiki.worldbank.org/Checklist:\_Reviewing\_Graphs} and
	\url{https://dimewiki.worldbank.org/Checklist:\_Submit\_Table}}

If you follow the steps outlined in this chapter,
most of the data work involved in the last step of the research process
-- publication -- will already be done.
If you used de-identified data for analysis,
publishing the cleaned dataset in a trusted repository will allow you to cite your data.
Some of the documentation produced during cleaning and construction can be published
even if the data cannot due to confidentiality.
Your analysis code will be organized in a reproducible way,
so will need to do release a replication package is a last round of code review.
This will allow you to focus on what matters:
writing up your results into a compelling story.

\subsection{Managing outputs}

The final task that needs to be discussed with your team is the best way to manage output files.
A great number of outputs will be created during the course of a project,
and these will include both raw outputs such as tables and graphs
and final products such as presentations, papers and reports.
When the first outputs are being created, agree on where to store them,
what softwares and formats to use, and how to keep track of them.

% Where to store outputs
Decisions about storage of outputs are made easier by technical constraints.
As discussed above, version control systems like Git are a great way to manage
plaintext files, and sync softwares such as Dropbox are better for binary files.
Outputs will similarly come in these two formats, depending on your software.
Binary outputs like Excel files, PDFs, PowerPoints, or Word documents can be kept in a synced folder.
Raw outputs in plaintext formats like \texttt{.tex} and \texttt{.eps}
can be created from most analytical software and managed with Git.
Tracking plaintext outputs with Git makes it easier to identify changes that affect results.
If you are re-running all of your code from the master script,
the outputs will be overwritten,
and any changes in coefficients and number of observations, for example,
will be automatically flagged for you or a reviewer to check.

No matter what choices you make,
you will need to make updates to your outputs quite frequently.
And anyone who has tried to recreate a graph after a few months probably knows
that it can be hard to remember where you saved the code that created it.
Here, naming conventions and code organization play a key role
in not re-writing scripts again and again.
It is common for teams to maintain one analyisis file or folder with draft code or ``exploratory analysis'',
which are pieces of code that are stored only to be found again in the future,
but not cleaned up to be included in any final outputs yet.
Once you are happy with a result or output,
it should be named and moved to a dedicated location.
It's typically desirable to have the names of outputs and scripts linked,
so, for example, \texttt{factor-analysis.do} creates \texttt{f1-factor-analysis.eps} and so on.
Document output creation in the master script that runs these files,
so that before the line that runs a particular analysis script
there are a few lines of comments listing
datasets and functions that are necessary for it to run,
as well as all outputs created by that script.

% What software to use
Compiling the raw outputs from your statistical software into useful formats
is the final step in producing research outputs for public consumption.
Though formatted text software such as Word and PowerPoint are still prevalent,
researchers are increasingly choosing to prepare final outputs
like documents and presentations using {\LaTeX}\index{{\LaTeX}}.\sidenote{
	\url{https://www.latex-project.org} and \url{https://github.com/worldbank/DIME-LaTeX-Templates}.}
{\LaTeX} is a document preparation system that can create both text documents and presentations.
{\LaTeX} uses plaintext for all formatting,
and it is necessary to learn its specific markup convention to use it.

The main advantage of using {\LaTeX} is that you can write dynamic documents,
that import inputs every time they are compiled.
This means you can skip the copying and pasting whenever an output is updated.
Because it's written in plaintext, it's also easier to control and document changes using Git.
Creating documents in {\LaTeX} using an integrated writing environment such as TeXstudio, TeXmaker or LyX
is great for outputs that focus mainly on text,
but include small chunks of code and static code outputs.
This book, for example, was written in {\LaTeX} and managed on GitHub\sidenote{\url{https://github.com/worldbank/d4di}}.

Another option is to use the statistical software's dynamic document engines.
This means you can write both text (in Markdown) and code in the script,
and the result will usually be a PDF or HTML file including code, text, and outputs.
Dynamic document tools are better for including large chunks of code and dynamically created graphs and tables,
but formatting these can be much trickier and less full-featured than other editors.
So dynamic documents can be great for creating appendices
or quick documents with results as you work on them,
but are not usually considered for final papers and reports.
RMarkdown\sidenote{\url{https://rmarkdown.rstudio.com}} is the most widely adopted solution in R.
There are also different options for Markdown in Stata,
such as \texttt{markstat},\sidenote{\url{https://data.princeton.edu/stata/markdown}}
Stata 15 dynamic documents,\sidenote{\url{https://www.stata.com/new-in-stata/markdown}}
\texttt{webdoc},\sidenote{\url{http://repec.sowi.unibe.ch/stata/webdoc}} and
\texttt{texdoc}.\sidenote{\url{http://repec.sowi.unibe.ch/stata/texdoc}}

Whichever options you choose,
agree with your team on what tools will be used for what outputs, and
where they will be stored before you start creating them.
Take into account ease of use for different team members, but
keep in mind that learning how to use a new tool may require some
time investment upfront that will be paid off as your project advances.


\subsection{Preparing dynamic documents}

Dynamic documents are a broad class of tools that enable a streamlined, reproducible workflow.
The term ``dynamic'' can refer to any document-creation technology
that allows the inclusion of explicitly encoded linkages to raw output files.
This means that, whenever outputs are updated,
the next time the document is loaded or compiled, it will automatically include
all changes made to all outputs without any additional intervention from the user.
This way, updates will never be accidentally excluded,
and updating results will not become more difficult
as the number of inputs grows,
because they are all managed by a single integrated process.

You will note that this is not possible in tools like Microsoft Office,
although there are various tools and add-ons that produce similar functionality,
and we will introduce some later in this book.
In Word, by default, you have to copy and paste each object individually
whenever tables, graphs, or other inputs have to be updated.
This creates complex inefficiency: updates may be accidentally excluded
and ensuring they are not will become more difficult as the document grows.
As time goes on, it therefore becomes more and more likely
that a mistake will be made or something will be missed.
Therefore this is a broadly unsuitable way to prepare technical documents.

The most widely utilized software
for dynamically managing both text and results is \LaTeX (pronounced ``lah-tek'').\sidenote{
	\url{https://github.com/worldbank/DIME-LaTeX-Templates}}
\index{\LaTeX}
\LaTeX is a document preparation and typesetting system with a unique syntax.
While this tool has a significant learning curve,
its enormous flexibility in terms of operation, collaboration, output formatting, and styling
make it the primary choice for most large technical outputs.
In fact, \LaTeX operates behind-the-scenes in many other dynamic document tools (discussed below).
Therefore, we recommend that you learn to use \LaTeX directly
as soon as you are able to and provide several resources for doing so in the next section.

There are tools that can generate dynamic documents from within your scripts, 
such as R's RMarkdown\sidenote{\url{https://rmarkdown.rstudio.com}}
Stata offers a built-in package for dynamic documents, \texttt{dyndoc}\sidenote{\url{https://www.stata.com/manuals/rptdyndoc.pdf}}, and user-written commands such \texttt{texdoc}\sidenote{\url{http://repec.sowi.unibe.ch/stata/texdoc}} and \texttt{markstat}\sidenote{\url{https://data.princeton.edu/stata/markdown}} allow for additional functionalities.
These tools ``knit'' or ``weave'' text and code together,
and are programmed to insert code outputs in pre-specified locations.
Documents called ``notebooks'' (such as Jupyter\sidenote{\url{https://jupyter.org}}) work similarly,
as they also use the underlying analytical software to create the document.
These tools are usually appropriate for short or informal documents
because it tends to be difficult to edit the content unless using the tool 
and often does not have as extensive formatting option as, for example, Word.

There are also simple tools for dynamic documents
that do not require direct operation of the underlying code or software,
simply access to the updated outputs.
An example of this is Dropbox Paper,
a free online writing tool that allows linkages to files in Dropbox
which are automatically updated anytime the file is replaced.
They have limited functionality in terms of version control and formatting,
and may never include any references to confidential data,
but can be useful for working on informal outputs, such as blogposts,
with collaborators who do not code. 



%------------------------------------------------
