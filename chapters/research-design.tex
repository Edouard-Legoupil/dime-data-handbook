%-----------------------------------------------------------------------------------------------

\begin{fullwidth}
Research design is the process of structuring field work
-- both experimental design and data collection --
that will answer a specific research question.
You don't need to be an expert in this,
and there are lots of good resources out there
that focus on designing interventions and evaluations
as well as on econometric approaches.
This section will present a brief overview
of the most common methods that are used in development research.
Specifically, we will introduce you to several ``causal inference'' methods
that are frequently used to understand the impact of real development programs.
The intent is for you to obtain an understanding of
the way in which each method constructs treatment and control groups,
the data structures needed to estimate the corresponding effects,
and any available code tools that will assist you in this process.

This is important to understand before going into the field for several reasons.
If you do not understand how to calculate the correct estimator for your study,
you will not be able to assess the power of your research design.
You will also be unable to make tradeoffs in the field
when you inevitable have to allocate scarce resources
between tasks like maximizing sample size
and ensuring follow-up with specific individuals.
You will save a lot of time by understanding the way
your data needs to be organized and set up as it comes in
before you will be able to calculate meaningful results.
Just as importantly, understanding each of these approaches
will allow you to keep your eyes open for research opportunities:
many of the most interesting projects occur because people in the field
recognize the opportunity to implement one of these methods on the fly
in response to an unexpected event in the field.
While somewhat more conceptual than practical,
a basic understanding of your project's chosen approach will make you
much more effective at the analytical part of your work.
\end{fullwidth}

%-----------------------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------

\section{Causality, inference, and identification}

The primary goal of research design is to establish \textbf{identification}
for a parameter of interest -- that is, to demonstrate
a source of variation in a particular input that has no other possible channel
to alter a particular outcome, in order to assert that some change in that outcome
was caused by that change in the input.
  \index{identification}
When we are discussing the types of inputs commonly referred to as
``programs'' or ``interventions'', we are typically attempting to obtain estimates
of a program-specific \textbf{treatment effect}, or the change in outcomes
directly attributable to exposure to what we call the \textbf{treatment}.\cite{abadie2018econometric}
  \index{treatment effect}
You can categorize most research designs into one of two main types:
\textbf{experimental} designs, in which the research team
is directly responsible for creating the variation in treatment,
and \textbf{quasi-experimental} designs, in which the team
identifies a ``natural'' source of variation and uses it for identification.
Nearly all methods can fall into either category.


%-----------------------------------------------------------------------------------------------
\subsection{Defining treatment and control groups}

The key assumption behind estimating treatment effects is that every
person, facility, village, or whatever the unit of intervention is
has two possible states: their outcome if they do not recieve the treatment
and their outcome if they do recieve the treatment.
Each unit's treatment effect is the difference between these two states,
and the true \textbf{average treatment effect} is the average of all of
these differences across the potentially treated population.
  \index{average treatment effect}

In reality, we never see the same unit in both their treated and untreated state simultaneously,
so measuring and averaging these effects directly is impossible.\sidenote{
  \url{http://www.stat.columbia.edu/~cook/qr33.pdf}}
Instead, we typically make inferences from samples.
\textbf{Causal inference} methods are those in which we are able to estimate the
average treatment effect without observing individual-level effects,
but can obtain it from some comparison of averages with a \textbf{control} group.
  \index{causal inference}\index{control group}
Each method is based around a way of comparing another set of observations --
the ``control'' observations -- with the treatment group,
which would have identical to the treated group in the absence of the treatment.
Therefore, almost all of these designs can be accurately described
as a series of between-group comparisons.\sidenote{
  \url{http://nickchk.com/econ305.html}}

This \textbf{control group} serves as a counterfactual to the treatment group,
and we compare the distributions of outcomes within each
to make a computation of how different the groups are from each other.
\textit{Causal Inference} and \textit{Causal Inference: The Mixtape}
provides a detailed practical introduction to and history of
each of these methods, so we will only introduce you to
them very abstractly in this chapter.\sidenote{\url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}
\\ \noindent \url{http://scunning.com/cunningham_mixtape.pdf}}
Each of the methods described in this chapter
relies on some variant of this basic strategy.
In counterfactual causal analysis,
the econometric models and estimating equations
do not attempt to create a predictive or comprehensive model
of how the outcome of interest is generated --
typically we do not care about measures of fit or predictive accuracy
like R-squared values or root mean square errors.
Instead, the econometric models desribed here aim to
correctly describe the experimental design being used,
so that the correct estimate of the difference
between the treatment and control groups is obtained
and can be interpreted as the effect of the treatment on outcomes.

Correctly describing the experiment means accounting for design factors
such as stratification and clustering, and
ensuring that time trends are handled sensibly.
We aren't even going to get into regression models here.
The models you will construct and estimate are intended to do two things:
to express the intention of your research design,
and to help you group the potentially endless concepts of field reality
into intellectually tractable categories.
In other words, these models tell the story of your research design.

%-----------------------------------------------------------------------------------------------
\subsection{Experimental research designs}

Experimental research designs explicitly allow the research team
to change the condition of the populations being studied,\sidenote{\url{https://dimewiki.worldbank.org/wiki/Experimental_Methods}}
in the form of NGO programs, government regulations,
information campaigns, and many more types of interventions.\cite{banerjee2009experimental}
The classic method is the \textbf{randomized control trial (RCT)}.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Randomized_Control_Trials}}
(Not everyone agrees this is the best way to do research.\sidenote{\url{https://www.nber.org/papers/w14690.pdf}})
\index{randomized control trial}
There treatment and control groups are drawn from the same underlying population
so that the strong condition of statistical equality
in the absence of the experiment can be assumed.
Three RCT-based methods are discussed here:
\textbf{cross-sectional randomization} (``endline-only'' studies),
\textbf{difference-in-difference} (``panel-data'' studies),
and \textbf{regression discontinuity} (``cutoff'' studies).

%-----------------------------------------------------------------------------------------------
\subsection{Quasi-experimental research designs}

\textbf{Quasi-experimental} research designs,\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Quasi-Experimental_Methods}}
are inference methods based on methods other than explicit experimentation.
Instead, they rely on ``experiments of nature'',
in which natural variation can be argued to approximate
the type of exogenous variations in circumstances
that a researcher would attempt to create with an experiment.\cite{dinardo2016natural}

Unlike with planned experimental designs,
quasi-experimental designs typically require the extra luck
of having data collected at the right times and places
to exploit events that occurred in the past.
Therefore, these methods often use either secondary data,
or use primary data in a cross-sectional retrospective method,
applying additional corrections as needed to make
the treatment and comparison groups plausibly identical.

%-----------------------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------
\section{Working with specific research designs}


%-----------------------------------------------------------------------------------------------
\subsection{Cross-sectional RCTs}

\textbf{Cross-sectional RCTs} are the simplest possible study design:
a program is implemented, surveys are conducted, and data is analyzed.
The randomization process
draws the treatment and control groups from the same underlying population.
This implies the groups' outcome means would be identical in expectation
before intervention, and would have been identical at measurement --
therefore, differences are due to the effect of the intervention.
Cross-sectional data is simple because
for research teams do not need track individuals over time,
or analyze attrition and follow-up other than non-response.
Cross-sectional designs can have a time dimension;
they are then called ``repeated cross-sections'',
but do not imply a panel structure for individual observations.

Typically, the cross-sectional model is developed
only with controls for the research design.
\textbf{Balance checks}\sidenote{\url{https://dimewiki.worldbank.org/wiki/iebaltab}} can be utilized, but an effective experiment
can use \textbf{stratification} (sometimes called blocking) aggressively\sidenote{\url{https://blogs.worldbank.org/impactevaluations/impactevaluations/how-randomize-using-many-baseline-variables-guest-post-thomas-barrios}} to ensure balance
before data is collected.\cite{athey2017econometrics}
\index{balance}
Stratification disaggregates a single experiment to a collection
of smaller experiments by conducting randomization within
``sufficiently similar'' strata groups.
Adjustments for balance variables are never necessary in RCTs,
because it is certain that the true data-generating process
has no correlation between the treatment and the balance factors.\sidenote{\url{https://blogs.worldbank.org/impactevaluations/should-we-require-balance-t-tests-baseline-observables-randomized-experiments}}
However, controls for imbalance that are not part of the design
may reduce the variance of estimates, but there is debate on
the importance of these tests and corrections.

%-----------------------------------------------------------------------------------------------
\subsection{Differences-in-differences}

\textbf{Differences-in-differences}\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Difference-in-Differences}}
\index{differences-in-differences}
(abbreviated as DD, DiD, diff-in-diff, and other variants)
deals with the construction of controls differently:
it uses a panel data structure to additionally use each
unit in the pre-treatment phase as an additional control for itself post-treatment (the first difference),
then comparing that mean change with the control group (the second difference).\cite{mckenzie2012beyond}
Therefore, rather than relying entirely on treatment-control balance for identification,
this class of designs intends to test whether \textit{changes}
in outcomes over time were different in the treatment group than the control group.\sidenote{\url{https://blogs.worldbank.org/impactevaluations/often-unspoken-assumptions-behind-difference-difference-estimator-practice}}
The primary identifying assumption for diff-in-diff is \textbf{parallel trends},
the idea that the change in all groups over time would have been identical
in the absence of the treatment.

Diff-in-diff experiments therefore require substantially more effort
in the field work portion, so that the \textbf{panel} of observations is well-constructed.\sidenote{\url{https://www.princeton.edu/~otorres/Panel101.pdf}}
Since baseline and endline data collection may be far apart,
it is important to create careful records during the first round
so that follow-ups can be conducted with the same subjects,
and \textbf{attrition} across rounds can be properly taken into account.\sidenote{\url{http://blogs.worldbank.org/impactevaluations/dealing-attrition-field-experiments}}
Depending on the distribution of results,
estimates may become completely uninformative
with relatively little loss to follow-up.

The diff-in-diff model is a four-way comparison.\sidenote{\url{https://dimewiki.worldbank.org/wiki/ieddtab}}
The experimental design intends to compare treatment to control,
after taking out the pre-levels for both.\sidenote{\url{https://www.princeton.edu/~otorres/DID101.pdf}}
Therefore the model includes a time period indicator,
a treatment group indicator (the pre-treatment control is the base level),
and it is the \textit{interaction} of treatment and time indicators
that we interpret as the differential effect of the treatment assignment.

%-----------------------------------------------------------------------------------------------
\subsection{Regression discontinuity}

\textbf{Regression discontinuity (RD)} designs differ from other RCTs
\index{regression discontinuity}
in that the treatment group is not directly randomly assigned,
even though it is often applied in the context of a specific experiment.\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Regression_Discontinuity}}
(In practice, many RDs are quasi-experimental, but this section
will treat them as though they are designed by the researcher.)
In an RD design, there is a \textbf{running variable}
which gives eligible people access to some program,
and a strict cutoff determines who is included.\cite{lee2010regression}
This is usually justified by budget limitations.
The running variable should not be the outcome of interest,
and while it can be time, that may require additional modeling assumptions.
Those who qualify are given the intervention and those who don't are not;
this process substitutes for explicit randomization.\sidenote{\url{http://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn}}

For example, imagine that there is a strict income cutoff created
for a program that subsidizes some educational resources.
Here, income is the running variable.
The intuition is that the people who are ``barely eligible''
should not in reality be very different from those who are ``barely ineligible'',
and that resulting differences between them at measurement
are therefore due to the intervention or program.\cite{imbens2008regression}
For the modeling component, the \textbf{bandwidth},
or the size of the window around the cutoff to use,
has to be decided and tested against various options for robustness.
The rest of the model depends largely on the design and execution of the experiment.

%-----------------------------------------------------------------------------------------------
\subsection{Instrumental variables}

Instrumental variables designs utilize variation in an
otherwise-unrelated predictor of exposure to a treatment condition
as an ``instrument'' for the treatment condition itself.\sidenote{\url{https://dimewiki.worldbank.org/wiki/instrumental_variables}}
\index{instrumental variables}
The simplest example is actually experimental --
in a randomization design, we can use instrumental variables
based on an \textit{offer} to join some program,
rather than on the actual inclusion in the program.\cite{angrist2001instrumental}
The reason for doing this is that the \textbf{second stage}
of actual program takeup may be severely self-selected,
making the group of program participants in fact
wildly different from the group of non-participants.\sidenote{\url{http://www.rebeccabarter.com/blog/2018-05-23-instrumental_variables/}}
The corresponding \textbf{two-stage-least-squares (2SLS)} estimator\sidenote{\url{http://www.nuff.ox.ac.uk/teaching/economics/bond/IV\%20Estimation\%20Using\%20Stata.pdf}}
solves this by conditioning on only the random portion of takeup --
in this case, the randomized offer of enrollment in the program.

Unfortunately, instrumental variables designs are known
to have very high variances relative to \textbf{ordinary least squares}.\cite{young2017consistency}
IV designs furthermore rely on strong but untestable assumptions
about the relationship between the instrument and the outcome.\cite{bound1995problems}
Therefore IV designs face special scrutiny,
and only the most believable designs,
usually those backed by extensive qualitative analysis,
are acceptable as high-quality evidence.

%-----------------------------------------------------------------------------------------------
\subsection{Matching estimators}

\textbf{Matching} estimators rely on the assumption that,
\index{matching}
conditional on some observable characteristics,
untreated units can be compared to treated units,
as if the treatment had been fully randomized.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Matching}}
In other words, they assert that differential takeup
is sufficiently predictable by observed characteristics.
These assertions are somewhat testable,\sidenote{\url{https://dimewiki.worldbank.org/wiki/iematch}}
and there are a large number of ``treatment effect''
packages devoted to standardizing reporting of various tests.\sidenote{\url{http://fmwww.bc.edu/repec/usug2016/drukker_uksug16.pdf}}

However, since most matching models rely on a specific linear model,
such as the typical \textbf{propensity score matching} estimator,
they are open to the criticism of ``specification searching'',
meaning that researchers can try different models of matching
until one, by chance, leads to the final result that was desired.
Newer methods, such as \textbf{coarsened exact matching},\cite{iacus2012causal}
are designed to remove some of the modelling,
such that simple differences between matched observations
are sufficient to estimate treatment effects
given somewhat weaker assumptions on the structure of that effect.
One solution, as with the experimental variant of 2SLS proposed above,
is to incorporate matching models into explicitly experimental designs.

%-----------------------------------------------------------------------------------------------
\subsection{Synthetic controls}

\textbf{Synthetic controls} methods\cite{abadie2015comparative}
\index{synthetic controls}
are designed for a particularly interesting situation:
one where useful controls for an intervention simply do not exist.
Canonical examples are policy changes at state or national levels,
since at that scope there are no other units quite like
the one that was affected by the policy change
(much less sufficient \textit{N} for a regression estimation).\cite{gobillon2016regional}
In this method, \textbf{time series data} is almost always required,
and the control comparison is contructed by creating
a linear combination of other units such that pre-treatment outcomes
for the treated unit are best approximated by that specific combination.
