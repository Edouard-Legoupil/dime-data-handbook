%-----------------------------------------------------------------------------------------------

\begin{fullwidth}
Research design is the process of structuring field work
-- both experimental design and data collection --
that will answer a specific research question.
You don't need to be an expert in this,
and there are lots of good resources out there
that focus on designing interventions and evaluations
as well as on econometric approaches.
This section will present a brief overview
of the most common methods that are used in development research.
Specifically, we will introduce you to several ``causal inference'' methods
that are frequently used to understand the impact of real development programs.
The intent is for you to obtain an understanding of
the way in which each method constructs treatment and control groups,
the data structures needed to estimate the corresponding effects,
and any available code tools that will assist you in this process.

This is important to understand before going into the field for several reasons.
If you do not understand how to calculate the correct estimator for your study,
you will not be able to assess the power of your research design.
You will also be unable to make tradeoffs in the field
when you inevitable have to allocate scarce resources
between tasks like maximizing sample size
and ensuring follow-up with specific individuals.
You will save a lot of time by understanding the way
your data needs to be organized and set up as it comes in
before you will be able to calculate meaningful results.
Just as importantly, understanding each of these approaches
will allow you to keep your eyes open for research opportunities:
many of the most interesting projects occur because people in the field
recognize the opportunity to implement one of these methods on the fly
in response to an unexpected event in the field.
While somewhat more conceptual than practical,
a basic understanding of your project's chosen approach will make you
much more effective at the analytical part of your work.
\end{fullwidth}

%-----------------------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------

\section{Causality, inference, and identification}

The primary goal of research design is to establish \textbf{identification}
for a parameter of interest -- that is, to demonstrate
a source of variation in a particular input that has no other possible channel
to alter a particular outcome, in order to assert that some change in that outcome
was caused by that change in the input.
  \index{identification}
When we are discussing the types of inputs commonly referred to as
``programs'' or ``interventions'', we are typically attempting to obtain estimates
of a program-specific \textbf{treatment effect}, or the change in outcomes
directly attributable to exposure to what we call the \textbf{treatment}.\cite{abadie2018econometric}
  \index{treatment effect}
You can categorize most research designs into one of two main types:
\textbf{experimental} designs, in which the research team
is directly responsible for creating the variation in treatment,
and \textbf{quasi-experimental} designs, in which the team
identifies a ``natural'' source of variation and uses it for identification.
Nearly all methods can fall into either category.

%-----------------------------------------------------------------------------------------------
\subsection{Estimating treatment effects using control groups}

The key assumption behind estimating treatment effects is that every
person, facility, or village (or whatever the unit of intervention is)
has two possible states: their outcomes if they do not recieve some treatment
and their outcomes if they do recieve that treatment.
Each unit's treatment effect is the individual difference between these two states,
and the \textbf{average treatment effect (ATE)} is the average of all of
these differences across the potentially treated population.
  \index{average treatment effect}
This is the most common parameter that research designs will want to estimate.
In most designs, the goal is to establish a ``counterfactual scenario'' for the treatment group
with which outcomes can be directly compared.
There are several resources that provide more or less mathematically intensive
approaches to understanding how various methods to his.
\textit{Causal Inference} and \textit{Causal Inference: The Mixtape}
provides a detailed practical introduction to and history of
each of these methods.\sidenote{
  \url{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}
  \\ \noindent \url{http://scunning.com/cunningham_mixtape.pdf}}
\textit{Mostly Harmless Econometrics} and \textit{Mastering Metrics}
are canonical treatments of the mathematics behind all econometric approaches.\sidenote{
  \url{https://www.researchgate.net/publication/51992844\_Mostly\_Harmless\_Econometrics\_An\_Empiricist's\_Companion}
  \\ \noindent \url{http://assets.press.princeton.edu/chapters/s10363.pdf}}

Intuitively, the problem is as follows: we can never observe the same unit
in both their treated and untreated states simultaneously,
so measuring and averaging these effects directly is impossible.\sidenote{
  \url{http://www.stat.columbia.edu/~cook/qr33.pdf}}
Instead, we typically make inferences from samples.
\textbf{Causal inference} methods are those in which we are able to estimate the
average treatment effect without observing individual-level effects,
but can obtain it from some comparison of averages with a \textbf{control} group.
  \index{causal inference}\index{control group}
Every research design is based around a way of comparing another set of observations --
the ``control'' observations -- against the treatment group.
They all work to establish that the control observations would have been
identical \textit{on average} to the treated group in the absence of the treatment.
Then, the mathematical properties of averages implies that the calculated
difference in averages is equivalent to the average difference:
exactly the parameter we are seeking to estimate.
Therefore, almost all designs can be accurately described
as a series of between-group comparisons.\sidenote{
  \url{http://nickchk.com/econ305.html}}

Most of the methods that you will encounter rely on some variant of this strategy,
which is designed to maximize the ability to estimate the effect
of an average unit being offered the treatment being evaluated.
The focus on identification of the treatment effect, however,
means there are several essential features to this approach
that are not common in other types of statistical and data science work.
First, the econometric models and estimating equations used
do not attempt to create a predictive or comprehensive model
of how the outcome of interest is generated.
Typically, these designs are not interested in predictive accuracy,
and the estimates and predictions that these models produce
will not be as good at predicting outcomes or fitting the data as other models.
Additionally, when control variables or other variables are used in estimation,
there is no guarantee that those parameters are marginal effects.
They can only be interpreted as correlative averages,
unless the experimenter has additional sources of identification for them.
The models you will construct and estimate are intended to do exactly one thing:
to express the intention of your project's research design,
and to accurately estimate the effect of the treatment it is evaluating.
In other words, these models tell the story of the research design
in a way that clarifies the exact comparison being made between control and treatment.

%-----------------------------------------------------------------------------------------------
\subsection{Experimental and quasi-experimental research designs}

Experimental research designs explicitly allow the research team
to change the condition of the populations being studied,\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Experimental_Methods}}
often in the form of NGO programs, government regulations,
information campaigns, and many more types of interventions.\cite{banerjee2009experimental}
The classic method is the \textbf{randomized control trial (RCT)}.\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Randomized_Control_Trials}}
  \index{randomized control trials}
In randomized control trials, the control group is randomized --
that is, from an eligible population,
a random subset of units are not given access to the treatment,
so that they may serve as a counterfactual for those who are.
A randomized control group, intuitively, is meant to represent
how things would have turned out for the treated group
if they had not been treated, and it is particularly effective at doing so
as evidenced by its broad credibility in fields ranging from clinical medicine to development.
However, there are many types of treatments that are impractical or unethical
to effectively approach using an experimental strategy,
and therefore many limitations to accessing ``big questions''
through RCT approaches.\sidenote{
  \url{https://www.nber.org/papers/w14690.pdf}}

Randomized designs all share several major statistical concerns.
The first is the fact that it is always possible to select a control group,
by chance, which was not in fact going to be very similar to the treatment group.
This feature is called randomization noise, and all RCTs share the need to understand
how randomization noise may impact the estimates that are obtained.
Second, takeup and implementation fidelity are extremely important,
since programs will by definition have no effect
if they are not in fact accepted by or delivered to
the people who are supposed to recieve them.
Unfortunately, these effects kick in very quickly and are highly nonlinear:
70\% takeup or efficacy doubles the required sample, and 50\% quadruples it.\sidenote{
  \url{https://blogs.worldbank.org/impactevaluations/power-calculations-101-dealing-with-incomplete-take-up}}
Such effects are also very hard to correct ex post,
since they require strong assumptions about the randomness or non-randomness of takeup.
Therefore a large amount of field time and descriptive work
must be dedicated to understanding how these effects played out in a given study,
and often overshadow the effort put into the econometric design itself.

\textbf{Quasi-experimental} research designs,\sidenote{
  \url{https://dimewiki.worldbank.org/wiki/Quasi-Experimental_Methods}}
by contrast, are inference methods based on events not controlled by the research team.
Instead, they rely on ``experiments of nature'',
in which natural variation can be argued to approximate
the type of exogenous variation in treatment availability
that a researcher would attempt to create with an experiment.\cite{dinardo2016natural}
Unlike with carefully planned experimental designs,
quasi-experimental designs typically require the extra luck
of having access to data collected at the right times and places
to exploit events that occurred in the past,
or having the ability to collect data in a time and place
dictated by the availability of identification.
Therefore, these methods often use either secondary data,
or use primary data in a cross-sectional retrospective method.

Quasi-experimental designs therefore can access a much broader range of questions,
and with much less effort in terms of executing an intervention.
However, they require in-depth understanding of the precise events
the researcher wishes to address in order to know what data to collect
and how to model the underlying natural experiment.
Additionally, because the population who will have been exposed
to such events is limited by the scale of the event,
quasi-experimental designs are often power-constrained.
There is nothing the research team can do to increase power
by providing treatment to more people or expanding the control group:
instead, power is typically maximized by ensuring
that sampling is carried out effectively
and that attrition from the sampled groups is dealt with effectively.
Sampling noise and survey non-response are therefore analogous
to the randomization noise and implementation failures
that can be observed in RCT designs, and have similar implications for field work.

%-----------------------------------------------------------------------------------------------
%-----------------------------------------------------------------------------------------------
\section{Working with specific research designs}


%-----------------------------------------------------------------------------------------------
\subsection{Cross-sectional RCTs}

\textbf{Cross-sectional RCTs} are the simplest possible study design:
a program is implemented, surveys are conducted, and data is analyzed.
The randomization process
draws the treatment and control groups from the same underlying population.
This implies the groups' outcome means would be identical in expectation
before intervention, and would have been identical at measurement --
therefore, differences are due to the effect of the intervention.
Cross-sectional data is simple because
for research teams do not need track individuals over time,
or analyze attrition and follow-up other than non-response.
Cross-sectional designs can have a time dimension;
they are then called ``repeated cross-sections'',
but do not imply a panel structure for individual observations.

Typically, the cross-sectional model is developed
only with controls for the research design.
\textbf{Balance checks}\sidenote{\url{https://dimewiki.worldbank.org/wiki/iebaltab}} can be utilized, but an effective experiment
can use \textbf{stratification} (sometimes called blocking) aggressively\sidenote{\url{https://blogs.worldbank.org/impactevaluations/impactevaluations/how-randomize-using-many-baseline-variables-guest-post-thomas-barrios}} to ensure balance
before data is collected.\cite{athey2017econometrics}
\index{balance}
Stratification disaggregates a single experiment to a collection
of smaller experiments by conducting randomization within
``sufficiently similar'' strata groups.
Adjustments for balance variables are never necessary in RCTs,
because it is certain that the true data-generating process
has no correlation between the treatment and the balance factors.\sidenote{\url{https://blogs.worldbank.org/impactevaluations/should-we-require-balance-t-tests-baseline-observables-randomized-experiments}}
However, controls for imbalance that are not part of the design
may reduce the variance of estimates, but there is debate on
the importance of these tests and corrections.

%-----------------------------------------------------------------------------------------------
\subsection{Differences-in-differences}

\textbf{Differences-in-differences}\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Difference-in-Differences}}
\index{differences-in-differences}
(abbreviated as DD, DiD, diff-in-diff, and other variants)
deals with the construction of controls differently:
it uses a panel data structure to additionally use each
unit in the pre-treatment phase as an additional control for itself post-treatment (the first difference),
then comparing that mean change with the control group (the second difference).\cite{mckenzie2012beyond}
Therefore, rather than relying entirely on treatment-control balance for identification,
this class of designs intends to test whether \textit{changes}
in outcomes over time were different in the treatment group than the control group.\sidenote{\url{https://blogs.worldbank.org/impactevaluations/often-unspoken-assumptions-behind-difference-difference-estimator-practice}}
The primary identifying assumption for diff-in-diff is \textbf{parallel trends},
the idea that the change in all groups over time would have been identical
in the absence of the treatment.

Diff-in-diff experiments therefore require substantially more effort
in the field work portion, so that the \textbf{panel} of observations is well-constructed.\sidenote{\url{https://www.princeton.edu/~otorres/Panel101.pdf}}
Since baseline and endline data collection may be far apart,
it is important to create careful records during the first round
so that follow-ups can be conducted with the same subjects,
and \textbf{attrition} across rounds can be properly taken into account.\sidenote{\url{http://blogs.worldbank.org/impactevaluations/dealing-attrition-field-experiments}}
Depending on the distribution of results,
estimates may become completely uninformative
with relatively little loss to follow-up.

The diff-in-diff model is a four-way comparison.\sidenote{\url{https://dimewiki.worldbank.org/wiki/ieddtab}}
The experimental design intends to compare treatment to control,
after taking out the pre-levels for both.\sidenote{\url{https://www.princeton.edu/~otorres/DID101.pdf}}
Therefore the model includes a time period indicator,
a treatment group indicator (the pre-treatment control is the base level),
and it is the \textit{interaction} of treatment and time indicators
that we interpret as the differential effect of the treatment assignment.

%-----------------------------------------------------------------------------------------------
\subsection{Regression discontinuity}

\textbf{Regression discontinuity (RD)} designs differ from other RCTs
\index{regression discontinuity}
in that the treatment group is not directly randomly assigned,
even though it is often applied in the context of a specific experiment.\sidenote{
\url{https://dimewiki.worldbank.org/wiki/Regression_Discontinuity}}
(In practice, many RDs are quasi-experimental, but this section
will treat them as though they are designed by the researcher.)
In an RD design, there is a \textbf{running variable}
which gives eligible people access to some program,
and a strict cutoff determines who is included.\cite{lee2010regression}
This is usually justified by budget limitations.
The running variable should not be the outcome of interest,
and while it can be time, that may require additional modeling assumptions.
Those who qualify are given the intervention and those who don't are not;
this process substitutes for explicit randomization.\sidenote{\url{http://blogs.worldbank.org/impactevaluations/regression-discontinuity-porn}}

For example, imagine that there is a strict income cutoff created
for a program that subsidizes some educational resources.
Here, income is the running variable.
The intuition is that the people who are ``barely eligible''
should not in reality be very different from those who are ``barely ineligible'',
and that resulting differences between them at measurement
are therefore due to the intervention or program.\cite{imbens2008regression}
For the modeling component, the \textbf{bandwidth},
or the size of the window around the cutoff to use,
has to be decided and tested against various options for robustness.
The rest of the model depends largely on the design and execution of the experiment.

%-----------------------------------------------------------------------------------------------
\subsection{Instrumental variables}

Instrumental variables designs utilize variation in an
otherwise-unrelated predictor of exposure to a treatment condition
as an ``instrument'' for the treatment condition itself.\sidenote{\url{https://dimewiki.worldbank.org/wiki/instrumental_variables}}
\index{instrumental variables}
The simplest example is actually experimental --
in a randomization design, we can use instrumental variables
based on an \textit{offer} to join some program,
rather than on the actual inclusion in the program.\cite{angrist2001instrumental}
The reason for doing this is that the \textbf{second stage}
of actual program takeup may be severely self-selected,
making the group of program participants in fact
wildly different from the group of non-participants.\sidenote{\url{http://www.rebeccabarter.com/blog/2018-05-23-instrumental_variables/}}
The corresponding \textbf{two-stage-least-squares (2SLS)} estimator\sidenote{\url{http://www.nuff.ox.ac.uk/teaching/economics/bond/IV\%20Estimation\%20Using\%20Stata.pdf}}
solves this by conditioning on only the random portion of takeup --
in this case, the randomized offer of enrollment in the program.

Unfortunately, instrumental variables designs are known
to have very high variances relative to \textbf{ordinary least squares}.\cite{young2017consistency}
IV designs furthermore rely on strong but untestable assumptions
about the relationship between the instrument and the outcome.\cite{bound1995problems}
Therefore IV designs face special scrutiny,
and only the most believable designs,
usually those backed by extensive qualitative analysis,
are acceptable as high-quality evidence.

%-----------------------------------------------------------------------------------------------
\subsection{Matching estimators}

\textbf{Matching} estimators rely on the assumption that,
\index{matching}
conditional on some observable characteristics,
untreated units can be compared to treated units,
as if the treatment had been fully randomized.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Matching}}
In other words, they assert that differential takeup
is sufficiently predictable by observed characteristics.
These assertions are somewhat testable,\sidenote{\url{https://dimewiki.worldbank.org/wiki/iematch}}
and there are a large number of ``treatment effect''
packages devoted to standardizing reporting of various tests.\sidenote{\url{http://fmwww.bc.edu/repec/usug2016/drukker_uksug16.pdf}}

However, since most matching models rely on a specific linear model,
such as the typical \textbf{propensity score matching} estimator,
they are open to the criticism of ``specification searching'',
meaning that researchers can try different models of matching
until one, by chance, leads to the final result that was desired.
Newer methods, such as \textbf{coarsened exact matching},\cite{iacus2012causal}
are designed to remove some of the modelling,
such that simple differences between matched observations
are sufficient to estimate treatment effects
given somewhat weaker assumptions on the structure of that effect.
One solution, as with the experimental variant of 2SLS proposed above,
is to incorporate matching models into explicitly experimental designs.

%-----------------------------------------------------------------------------------------------
\subsection{Synthetic controls}

\textbf{Synthetic controls} methods\cite{abadie2015comparative}
\index{synthetic controls}
are designed for a particularly interesting situation:
one where useful controls for an intervention simply do not exist.
Canonical examples are policy changes at state or national levels,
since at that scope there are no other units quite like
the one that was affected by the policy change
(much less sufficient \textit{N} for a regression estimation).\cite{gobillon2016regional}
In this method, \textbf{time series data} is almost always required,
and the control comparison is contructed by creating
a linear combination of other units such that pre-treatment outcomes
for the treated unit are best approximated by that specific combination.
