\section{Data quality assurance}

Whether you are acquiring data from a partner or collecting it directly,
it is important to make sure that data faithfully reflects ground realities.
Data quality assurance requires a combination of real-time data checks
and back-checks or validation audits, which often means tracking down
the people whose information is in the dataset.

\subsection{Implementing high frequency quality checks}

% What are HFCs
A key advantage of continuous electronic data intake methods,
as compared to traditional paper surveys and one-time data dumps,
is the ability to access and analyze the data while the project is ongoing.
Data issues can be identified and resolved in real-time.
Designing systematic data checks and running them routinely throughout data intake
simplifies monitoring and improves data quality.
As part of data acquisition preparation,
the research team should develop a \textbf{data quality assurance plan}\sidenote{
	\url{https://dimewiki.worldbank.org/Data\_Quality\_Assurance\_Plan}}.
While data acquisition is ongoing,
a research assistant or data analyst should work closely with the field team or partner
to ensure that the data collection is progressing correctly,
and set up and perform \textbf{high-frequency checks (HFCs)} with the incoming data.

% Why they should be made in real-time
High-frequency checks (HFCs) should carefully inspect key treatment and outcome variables
so that the data quality of core experimental variables is uniformly high,
and that additional effort is centered where it is most important.
Data quality checks should be run on the data every time it is received from the field or partner
to flag irregularities in the aquisition progress, in sample completeness, or in response quality.
The faster issues are identified, the more likely they are to be solved.
\texttt{ipacheck}\sidenote{
	\url{https://github.com/PovertyAction/high-frequency-checks/wiki}}
is a very useful command that automates some of these tasks,
regardless of the source of the data.

% Completeness and duplicates
It is important to check continuously that the observations in the data match the intended sample.
In surveys, the software often provides some form of case management features
through which sampled units are directly assigned to individual enumerators.
For data received from partners, this may be harder to validate,
since they are the authoritative source of the data,
so cross-referencing with other data sources may be necessary to ensure completeness of the data.
Even with careful management, it is often the case that raw data includes duplicate or missing entries,
which may occur due to data entry errors or failed submissions to data servers.\sidenote{
	\url{https://dimewiki.worldbank.org/Duplicates_and_Survey_Logs}}
\texttt{ieduplicates}\sidenote{
	\url{https://dimewiki.worldbank.org/ieduplicates}}
provides a workflow for collaborating on the resolution of duplicate entries between you and the provider.
Then, observed units in the data must be validated against the expected sample:
this is as straightforward as merging the sample list with the survey data and checking for mismatches.
Reporting errors and duplicate observations in real-time allows the field team to make corrections efficiently.
Tracking data collection progress is important for monitoring attrition,
so that it is clear early on if a change in protocols or additional tracking will be needed.
It is also important to check data collection completion rate
and sample compliance by surveyor and survey team, if applicable,
or compare data missingness across administrative regions,
to identify any clusters that may be providing data of suspect quality.

% Consistency
High frequency checks should also include content-specific data checks.
Electronic survey and data entry software often incorporates many quality control features,
so these checks should focus on issues survey software cannot check automatically.
As most of these checks are project specific,
it is difficult to provide general guidance.
An in-depth knowledge of the questionnaire and a careful examination of the analysis plan
is the best preparation.
Examples include verifying consistency across multiple response fields or data sources,
validation of complex calculations like crop yields or medicine stocks (which require unit conversions),
suspicious patterns in survey timing,
or atypical response patters from specific data sources or enumerators.\sidenote{
	\url{https://dimewiki.worldbank.org/Monitoring_Data_Quality}}
Electronic data entry software typically provides rich metadata,
which can be useful in assessing data quality.
For example, automatically collected timestamps show when data was submitted
and (for surveys) how long enumerators spent on each question,
and trace histories show how many
times answers were changed before or after the data was submitted.

% Follow-up on inconsistencies
High-frequency checks will only improve data quality
if the issues they catch are communicated to the data provider.
There are lots of ways to do this;
what's most important is to find a way to create actionable information for your team.
\texttt{ipacheck}, for example, generates a spreadsheet with flagged errors;
these can be sent directly to the data collection teams.
Many teams choose other formats to display results,
such as online dashboards created by custom scripts.
It is also possible to automate communication of errors to the field team
by adding scripts to link the HFCs with a messaging program such as WhatsApp.
Any of these solutions are possible:
what works best for your team will depend on such variables as
cellular networks in field work areas, whether field supervisors have access to laptops,
internet speed, and coding skills of the team preparing the HFC workflows.
\section{Cleaning data for analysis}

% What is data cleaning
\textbf{Data cleaning} is a widely used expression used to refer to different tasks.
The cleaning process, as defined in this book, involves
(1) making the dataset easy to use and understand, and 
(2) carefully exploring each variable to document their distributions and identify patterns that may bias the analysis.
The resulting dataset will contain only the variables collected in the field, and
no modifications to data points will be made, 
except for corrections of mistaken entries.
Apart from the \textbf{cleaned dataset} (or datasets) itself,
cleaning will also yield extensive documentation describing  it.

% Section overview
During data cleaning, you will acquire in-depth understanding of the contents and structure of your data.
This knowledge will be key to correctly construct final indicators and analyze them.
So don't rush through this step.
Explore the dataset using tabulations, summaries, and descriptive plots.
It is common for cleaning to be the most time-consuming task in a project.
In this section, we will introduce some concepts and tools to make it more efficient and productive.

%\subsection{Correcting data points}

\subsection{Recoding and annotating data}

% Why recoding and annotating data are important
The cleaned dataset is the starting point of data analysis.
It will be extensively manipulated to construct analysis indicators,
so it is important for it to be easily processed by statistical software.
To make the analysis process smoother, 
anyone opening it for the first time should have all the information needed to interact with it,
even if they were not involved in the acquisition or cleaning process.
This will save them time going back as forward between the dataset and its accompanying documentation. 

% Encoding variables
Often times, datasets are not imported into statistical software in the most efficient format.
The most common example are string variables:
categorical variables and open-ended responses are often read as strings.
However, variables in these format cannot be used for quantitative analysis.
Therefore, categorical variables must be transformed into easier to use formats,
such as \texttt{factors} in R and \texttt{labeled integers}\sidenote{https://dimewiki.worldbank.org/wiki/Data_Cleaning#Value_Labels} in Stata.
Additionally, open-ended responses stored as strings usually have a high risk of being identifiers, 
so cleaning them requires extra attention.
The option names in categorical variables
(called \textit{value labels} in Stata and \textit{levels} in R)
should be accurate and concise, 
and correspond exactly to the data collection instrument.
Adding option names to categorical variables 
makes it easier to understand your data as you explore it,
and thus reduces the risk of small errors making their way through into the analysis stage.

% Recoding missing values
In survey data, it is common for non-responses such as ``Don't know'' and ``Declined to answer''
to be represented by negative survey codes. 
The presence of these negative values could bias your analysis,
since they don't represent actual observations of a variable.
So they need to be turned into \textit{missing values}.
However, the fact that a respondent didn't know how to answer a question is also useful information,
that would be lost by this transformation.
In Stata, this information can be elegantly conserved using extended missing values.\sidenote{
	\url{https://dimewiki.worldbank.org/Data\_Cleaning\#Survey\_Codes\_and\_Missing\_Values}}

% Labelling variables
We recommend that the cleaned data set by kept as similar to the raw data as possible.
This is particularly important regarding variable names:
keeping them consistent with the raw data makes data processing and construction more transparent.
Unfortunately, not all variable names are informative.
In such cases, one important piece of documentation,
the variable dictionary, makes the data easier to handle.
When a data collection instrument is available, 
it is often the best dictionary one could ask for.
But even in this cases, going back-and-forth between files can be inefficient,
so annotating variables in a dataset is extremely useful.
In Stata, \textit{variable labels}\sidenote{\url{
	https://dimewiki.worldbank.org/wiki/Data_Cleaning#Variable_Labels}} must always be present in a cleaned dataset.
They should include a short and clear description of the variable.
A lengthier description, that may include, for example,
the exact wording of a question, may be added through \textit{variable notes}.
In R, it is less common to use variable labels,
and a separate dataset with a variable dictionary is often preferred,
but \texttt{dataframe attributes} can be used for the same purpose.

% tools: iecodebook, tidyverse
Although all these tasks are key to making the data easy to use,
implementing them can be quite repetitive and create convoluted scripts.
The \texttt{iecodebook} command suite, part of the \texttt{iefieldkit} Stata package,
is designed to make some of the most tedious components of this process more efficient.\sidenote{
	\url{https://dimewiki.worldbank.org/iecodebook}}
\index{iecodebook}
It also creates a self-documenting workflow,
so your data cleaning documentation is created alongside that code,
with no extra steps.
As far as we know, there are no similar resources in R.
However, the \texttt{tidyverse}\sidenote{https://www.tidyverse.org/} packages
compose a consistent and useful grammar to perform the same tasks.

