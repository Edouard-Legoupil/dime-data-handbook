%------------------------------------------------

\begin{fullwidth}

	Policy decisions are made every day using the results of development briefs and studies,
	and these have wide-reaching effects on the lives of millions.
	As the range of policy questions asked by researchers grows,
	so too does the scrutiny under which methods and results are placed.
  High-quality research should be able to be easily examined and recreated by readers
  and policymakers who will make choices based on that evidence.
	It is useful to think of research as a public service
  that requires researchers as a group to be accountable to the public.
	This means acting to collectively protect the credibility of development research
	by following modern practices for research transparency and reproducibility.

  Across the social sciences, the open science movement has been fueled
  by the  of low-quality research practices,
	data and code that are inaccessible to the public,
  analytical errors in major research papers,
	and in some cases even outright fraud.
  While the development research community has not yet
  experienced any major scandals,
  it has become clear that there are necessary improvements
	in the way that code and data are handled as part of open research.
  As a communit, having common standards and practices
  for creating and sharing materials, code, and data with others
  will continue to improve the value of the work we do.
	In this chapter, we outline principles and practices that help to ensure
	research consumers can be confident in the conclusions reached.

\end{fullwidth}

%------------------------------------------------

\section{Preparing a transparent research project}

% Why development researchers should care about transparency
The empirical revolution in development research\cite{angrist2017economic}
has led to increased public scrutiny of the reliability of research.\cite{rogers_2017}\index{transparency}\index{credibility}\index{reproducibility}
Three major components make up this scrutiny: \textbf{reproducibility}\cite{duvendack2017meant}, \textbf{transparency},\cite{christensen2018transparency} and \textbf{credibility}.\cite{ioannidis2017power}
Development researchers should take these concerns seriously.
Many development research projects are purpose-built to address specific questions,
and often use unique data or small samples.
As a result, it is often the case that the data
researchers use for such studies has never been reviewed by anyone else,
so it is hard for others to verify that it was
collected, handled, and analyzed appropriately.

Reproducible and transparent methods are key to maintaining credibility
and avoiding serious errors.\cite{christensen2019transparent}
This is especially relevant for research that relies on original or novel data sources,
from innovative big data sources to survey datasets in unique contexts.
The field is rapidly moving in the direction of requiring strict adherence
to specific transparency guidelines.
Major publishers and funders, most notably the American Economic Association,
have taken steps to require that code and data
are accurately reported, cited, and preserved as outputs in themselves.\sidenote{
	\url{https://www.aeaweb.org/journals/policies/data-code}}

% Pre-registration
\subsection{Registering research studies}

Registration of research studies is an increasingly common practice
intended to ensure that a complete record of research inquiry is easily available.\sidenote{
	\url{https://dimewiki.worldbank.org/Pre-Registration}}
Registering research studies ensures that future scholars can quickly
find out what work has been carried out on a given question,
even if some or all of the work done never results in formal publication.
Registration of studies is increasingly required by publishers
and can be done very quickly either before, during, or after the study
with only basic information about the study purpose.
Some common registries are the \textbf{AEA} database,\sidenote{\url{https://www.socialscienceregistry.org}}
the \textbf{3ie} database,\sidenote{\url{https://ridie.3ieimpact.org}}
the \textbf{eGAP} database,\sidenote{\url{https://egap.org/content/registration}}
or the \textbf{OSF} registry,\sidenote{\url{https://osf.io/registries}} as appropriate.
\index{pre-registration}

Pre-registering studies before they begin work is a further extension of this principle.\cite{nosek2018preregistration}
Registration of a study before it goes to field,
particularly when specific hypotheses are included in the registration,
provides a simple and low-effort way for researchers
to conclusively demonstrate that a particular line of inquiry
was not generated by the process of data collection or analysis itself.
Pre-registrations need not provide exhaustive details about how
a particular hypothesis will be approached; only that it will be.
This can be highly valuable and requires very little additional investment.
As a result, we recommend pre-registration of all studies
in a public database with at least some general hypotheses prespecified.\sidenote{\url{https://datacolada.org/12}}

% Pre-analysis plans
\subsection{Writing pre-analysis plans}

In many cases, study registration or pre-registration
may be completely sufficient to avoid any criticism of
``hypothesizing after the results are known'', or HARKing.\cite{kerr1998harking}
In other cases, a researcher will know that they will have
a large amount of flexibility to define exactly how they approach a particular hypothesis.
This could include a diverse set of measures of an abstract concept;
it could include choices about sample inclusion or exclusion;
or it could include decisions about how to construct derived indicators.
When the researcher is collecting a large amount of information
and has leverage over even a moderate number of these options,
it is almost guaranteed that they can come up with any result they like.\cite{gelman2013garden}

Pre-analysis plans can be used to assuage these concerns for any evaluations
\index{pre-analysis plan}
by fully specifying some set of analyses intended to be conducted.\sidenote{
	\url{https://dimewiki.worldbank.org/Pre-Analysis_Plan}}
In particular, such a plan should be written up in detail
for areas that are known to provide a large amount of leeway
for researchers to make later decisions,
particularly things like interaction effects or subgroup analysis.
Even when used, pre-analysis plans shoud not be viewed as binding the hands of the researcher.\cite{olken2015promises}
Anything outside the original plan is just as interesting and valuable
as it would have been if the the plan was never published;
but having pre-committed to the details of a particular inquiry makes its results
immune to a wide range of criticisms of specification searching or multiple testing.\cite{duflo2020praise}
Regardless of whether or not a complete formal pre-analysis plan is utilized,
studies should still be pre-registered
to create a record of the fact that the study was undertaken.\sidenote{
  \url{https://blogs.worldbank.org/impactevaluations/pre-analysis-plans-and-registered-reports-what-new-opinion-piece-does-and-doesnt}}

% Registered reports

\textbf{Registered reports}\sidenote{
	\url{https://blogs.worldbank.org/impactevaluations/registered-reports-piloting-pre-results-review-process-journal-development-economics}}
are sometimes used to take this process of pre-planning a complex research design
to the level of a formal publication.
In a registered report, a journal or other publisher
will accept a particular style of study pre-description for publication,
typically then guaranteeing the acceptance of a later publication
that carries out the analysis described in the registered report.
While far stricter and more complex to carry out than
ordinary study registration or pre-analysis planning,
the registered report has the added benefit
of soliciting peer review and expert feedback
on the design and structure of the proposed study.

This process is in part meant to combat the ``file-drawer problem'',\cite{simonsohn2014p}
and ensure that researchers are transparent in the sense that
all promised results obtained from registered-report studies are actually published.
This approach has the advantage of pre-specifying in great detail
a complete research and analytical design,
and securing a commitment for publication regardless of the outcome.
This may be of special interest for researchers
studying events or programs where either there is a substantial risk
that they would either not be able to publish a null or negative result,
or where they may wish to avoid any pressure toward finding a particular result
when the program or event is subject to substantial social or political pressures.
As with pre-registration and pre-analysis,
nothing in a registered report should be understood
to prevent a researcher from pursuing additional avenures of inquiry
once the study is complete, either in the same or separate research outputs.

\section{Documenting and cataloguing data collection}

% Documenting a project carefully makes it more transparent
Documenting a project in detail greatly increases transparency.
Many disciplines have a tradition of keeping a ``lab notebook'',
and adapting and expanding this process to create a
lab-style workflow in the development field is a
critical step towards more transparent practices.
This means explicitly noting decisions as they are made,
and explaining the process behind the decision-making.
Documentation on data processing and additional hypotheses tested
will be expected in the supplemental materials to any publication.
Careful documentation will also save the research team a lot of time during a project,
as it prevents you from having the same discussion twice (or more!),
since you have a record of why something was done in a particular way.
There are a number of available tools
that will contribute to producing documentation,
\index{project documentation}
but project documentation should always be an active and ongoing process,
not a one-time requirement or retrospective task.
New decisions are always being made as the plan begins contact with reality,
and there is nothing wrong with sensible adaptation so long as it is recorded and disclosed.

% Tools for transparency: GitHub, OSF
There are various software solutions for building documentation over time.
The \textbf{Open Science Framework}\sidenote{\url{https://osf.io}} provides one such solution,\index{Open Science Framework}
with integrated file storage, version histories, and collaborative wiki pages.
\textbf{GitHub}\sidenote{\url{https://github.com}} provides a transparent documentation system\sidenote{
	\url{https://dimewiki.worldbank.org/Getting_started_with_GitHub}},\index{task management}\index{GitHub}
in addition to version histories and wiki pages.
Such services offer multiple different ways
to record the decision process leading to changes and additions,
track and register discussions, and manage tasks.
These are flexible tools that can be adapted to different team and project dynamics.
Services that log your research process can show things like modifications made in response to referee comments,
by having tagged version histories at each major revision.
They also allow you to use issue trackers
to document the research paths and questions you may have tried to answer
as a resource to others who have similar questions.
Each project has specific requirements for data, code, and documentation management,
and the exact transparency tools to use will depend on the team's needs,
but they should be agreed upon prior to project launch.
This way, you can start building a project's documentation as soon as you start making decisions.
Email, however, is \textit{not} a note-taking service, because communications are rarely well-ordered,
can be easily deleted, and are not available for future team members.

% Documenting survey instrument and survey code

% Preparing an initial catalog and release of data

\section{Preparing a reproducible workflow}

The credibility of research is traditionally a function of design choices.\cite{angrist2010credibility,ioannidis2005most}
Is the research design sufficiently powered through its sampling and randomization?
Were the key research outcomes pre-specified or chosen ex-post?
How sensitive are the results to changes in specifications or definitions?

% Ex-ante and ex-post transparency policies
Common research standards from journals and funders feature both ex ante
(or ``regulation'') and ex post (or ``verification'') policies.\cite{stodden2013toward}
Ex ante policies require that authors bear the burden
of ensuring they provide some set of materials before publication
and their quality meet some minimum standard.
Ex post policies require that authors make certain materials available to the public,
but their quality is not a direct condition for publication.
Still others have suggested ``guidance'' policies that would offer checklists
for which practices to adopt, such as reporting on whether and how
various practices were implemented.\cite{nosek2015promoting}

% What is reproducibility
Can another researcher reuse the same code on the same data
and get the exact same results as in your published paper?\sidenote{
  \url{https://blogs.worldbank.org/impactevaluations/what-development-economists-talk-about-when-they-talk-about-reproducibility}}
This is a standard known as \textbf{computational reproducibility},
and it is an increasingly common requirement for publication.\sidenote{
\url{https://www.nap.edu/resource/25303/R&R.pdf}}
It is best practice to verify computational reproducibility before submitting a paper before publication.
This should be done by someone who is not on your research team, on a different computer,
using exactly the package of code and data files you plan to submit with your paper.
Code that is well-organized into a master script, and written to be easily run by others,
makes this task simpler.
The next chapter discusses organization of data work in detail.

% Open data is necessary for reproducibility
For research to be reproducible,
all code files for data cleaning, construction and analysis
should be public, unless they contain confidential information.
Nobody should have to guess what exactly comprises a given index,
or what controls are included in your main regression,
or whether or not you clustered standard errors correctly.
That is, as a purely technical matter, nobody should have to ``just trust you'',
nor should they have to bother you to find out what happens
if any or all of these things were to be done slightly differently.\cite{simmons2011false,simonsohn2015specification,wicherts2016degrees}
Letting people play around with your data and code
is a great way to have new questions asked and answered
based on the valuable work you have already done.\sidenote{
	\url{https://blogs.worldbank.org/opendata/making-analytics-reusable}}

% Reproducible research is a public good
Making your research reproducible is also a public good.\sidenote{
	\url{https://dimewiki.worldbank.org/Reproducible_Research}}
It enables other researchers to re-use your code and processes
to do their own work more easily and effectively in the future.
This may mean applying your techniques to their data
or implementing a similar structure in a different context.
As a pure public good, this is nearly costless.
The useful tools and standards you create will have high value to others.
If you are personally or professionally motivated by citations,
producing these kinds of resources can lead to that as well.
Therefore, your code should be written neatly with clear instructions and published openly.
It should be easy to read and understand in terms of structure, style, and syntax.
Finally, the corresponding dataset should be openly accessible
unless for legal or ethical reasons it cannot be.\sidenote{
	\url{https://dimewiki.worldbank.org/Publishing_Data}}

% What is transparency and how it makes research better
Transparent research will expose not only the code,
but all research processes involved in developing the analytical approach.\sidenote{
	\url{https://www.princeton.edu/~mjs3/open_and_reproducible_opr_2017.pdf}}
This means that readers are able to judge for themselves if the research was done well
and the decision-making process was sound.
If the research is well-structured, and all of the relevant documentation\sidenote{
	\url{https://dimewiki.worldbank.org/Data_Documentation}}
is shared, this makes it easy for the reader to understand the analysis later.
Expecting process transparency is also an incentive for researchers to make better decisions,
be skeptical and thorough about their assumptions,
and, as we hope to convince you, make the process easier for themselves,
because it requires methodical organization that is labor-saving over the complete course of a project.

% Concluding paragraph
With the ongoing rise of empirical research and increased public scrutiny of scientific evidence,
simply making analysis code and data available
is no longer sufficient on its own to guarantee that findings will hold their credibility.
Even if your methods are highly precise,
your evidence is only as good as your data --
and there are plenty of mistakes that can be made between
establishing a design and generating final results that would compromise its conclusions.
That is why transparency is key for research credibility.
It allows other researchers, and research consumers,
to verify the steps to a conclusion by themselves,
and decide whether their standards for accepting a finding as evidence are met.
Therefore we encourage you to work, gradually, towards improving
the documentation and release of your research materials,
and finding the tools and workflows that best match your project and team.
Every investment you make in documentation and transparency up front
protects your project down the line, particularly as these standards continue to tighten.
Since projects tend to span over many years,
the records you will need to have available for publication are
only bound to increase by the time you do so.
