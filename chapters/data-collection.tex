%------------------------------------------------

\begin{fullwidth}
High-quality data is essential to most modern development research.
Many research questions require novel data, 
because no source of publicly available data addresses the 
inputs or outcomes of interest for the relevant population.
Data acquisition can take many forms, including: 
primary data generated through surveys; 
private sector partnerships granting access to new data sources, such as administrative and sensor data;
digitization of paper records, including administrative data;
primary data capture by unmanned aerial vehicles or other types of remote sensing;
or novel integration of various types of datasets, e.g. combining survey and sensor data.
Much of the recent push toward credibility in the social sciences has focused on analytical practices.
However, credible development research often depends, first and foremost, on the quality of the raw data.
For data work to be reproducible, the data acquisition process must be clearly and carefully documented.

This chapter covers the necessary components for securely acquiring all types of data,
special considerations for generating high-quality survey data, 
and protocols for safe handling of all types of confidential data. 
The first section discusses key ethical and legal considerations for data acquisition:
how to clearly document that you have the right to use the data and publish analytical products. 
This applies to all development datasets that are not licensed for public use, 
whether collected for the first time through surveys or sensors or acquired through a unique partnership.
The second section goes into detail on ensuring data quality if you are collecting survey data.
It provides detailed guidance on the electronic survey workflow,
from questionnaire design to programming and monitoring data quality.
We conclude with a discussion of safe data handling, 
providing guidance on how to receive, transfer, store, and share confidential data.
Secure file management is a basic requirement to comply with the legal and 
ethical agreements that allow  access to personal information for research purposes.


\end{fullwidth}

%------------------------------------------------
\section{Acquiring data}
Clearly establishing and documenting data access is critical for reproducible research.  
When acquiring data, the first step is to determine who owns the data, 
and establish and document the research team's right to use the data. 
Except in the case of primary surveys funded by the research team, 
the data is typically not owned by the research team. 
Rather, they must enter into data licensing agreements 
to access the data and publish derivative work. 
These agreements should make clear from the outset whether the 
research team can make the original data, or any portion thereof, public. 

It is the researchers' responsibility to respect the rights
of people who own the data and the people who are described by it;
but also to make sure that information is as available and accessible as possible.
These twin responsibilities can and do come into tension,
so it is important to be fully informed about what others are doing
and to fully inform others of what you are doing.
Writing down and agreeing to specific details is a good way of doing that.

\subsection{Data ownership}
Data ownership can be challenging to establish,
as many jurisdictions have differing laws regarding data and information,
and the research team may be subject to multiple conflicting regulations.
In some countries, data is implicitly owned by the people who it is about.
In others, it is owned by the people who collected it.
In still more, it is highly unclear and there are varying norms.
The best approach is always to consult with a local partner, 
and enter into specific legal agreements establishing ownership, 
access, and publication rights.
This is particularly critical where confidential data is involved
-- that is, when people are disclosing information to you
that you could not obtain simply by observation or through public records. 

For data generated for the research study, such as survey data, 
it is important to clarify up front who owns the data,
and who will have access to the data.
These details need to be shared with respondents when they are offered the opportunity 
to consent to participate in the study. 
If the research team is not collecting survey data directly --
for example, if a government, private company, or research partner is doing the data collection --
make sure that you have an explicit agreement
about who owns the resulting data. 
The contract for data collection should include specific terms 
as to the rights and responsibilities of each stakeholder.
It must clearly stipulate that the research team owns the data
and maintains full intellectual property rights. 
The contract should also explicitly stipulate that the contracted firm
is responsible for protecting respondent privacy, 
that the data collection will not be delegated to any third parties,
and that the data will not be used by the firm or subcontractors for any purpose not expressly stated in the contract,
before, during or after the assignment. 
The contract should also stipulate that the vendor is required to comply with 
ethical standards for social science research, 
and adhere to the specific terms of agreement with the relevant 
Institutional Review Board or applicable local authority.
Finally, it should include policies on reuse, storage, and retention or destruction of data. 

Research teams that generate their own data must also consider the terms
under which they will release that data to other researchers or to the general public. 
Will you publicly release the data in full (removing personal identifiers)?
Would you be okay with it being stored on servers anywhere in the world,
even ones that are owned by corporations or governments in countries other than your own?
Would you prefer to decide permission on a case-by-case basis, dependent on specific proposed uses? 
Would you expect that users of your data cite you or give you credit,
or would you require them in turn to release
their derivative data or publications under similar licenses as yours?
Whatever your answers are to these questions,
make sure your license or other agreement
under which you publish the data
specifically details those requirements.

\subsection{Data licensing agreements}
Data licensing is the formal act of the dataset owner giving some data rights to a specific user,
while retaining ownership of the dataset.
If you are not the owner of the dataset you want to analyze,
you must enter into a licensing agreement to access it for research purposes.
Similarly, when you own a dataset,
you must consider whether you will make the dataset accessible to other researchers,
and what terms-of-use you require. 

If the research team requires access to existing data for novel research,
terms of use should be agreed on with the data owner, 
typically through a data licensing agreement.
Keep in mind that the data owner is likely not familiar
with the research process, and therefore may be surprised
at some of the things you want to do if you are not clear up front.
You will typically want intellectual property rights to all research outputs developed used the data,
a license for all uses of derivative works, including public distribution 
(unless ethical considerations contraindicate this).
This is important to allow the research team to store, catalog, and publish, in whole or in part,
either the original licensed dataset or datasets derived from the original.
Make sure that the license you obtain from the data owner allows these uses,
and that you consult with the owner if you foresee exceptions with specific portions of the data.

The World Bank has a template Data License Agreement which DIME follows.{\sidenote \url{https://worldbankgroup.sharepoint.com/teams/ddh/SiteAssets/SitePages/ddh/DataLicenseAgreementTemplate_v4.pdf?cid=68a54269-bbff-4b47-846d-cab248ad7de1}}
The template specifies the specific objectives of the data sharing, 
and whether the data can be used only for the established purpose or for other objectives 
consistent with the mandates of the signatory organizations. 
It establishes the type of future data access, using the following categories:
\begin{itemize}
	\item open data: data freely available to the public
	\item licensed use: data available to the public by request; requesting users complete a licensing agreement with the Bank
	\item official use only: access limited to World Bank staff
	\item restricted access: access restricted to pre-authorized staff, or a specific business unit. 
\end{itemize}
The data provider may impose similar restrictions to sharing derivative works and any or all of the metadata. 
The template also specifies the required citation for the data. 


%------------------------------------------------
\section{Collecting data using electronic surveys}
In this section, we detail specific considerations for acquiring high-quality data through surveys.
As there are many excellent resources on questionnaire design and field supervision,
but few covering the particular challenges and opportunities presented by electronic surveys,
we focus on specific workflow considerations for digitally-generated data. 
There are many survey software options, and the market is rapidly evolving,
so we focus on workflows and primary concepts rather than software-specific tools.
If you are collecting data directly from the research subjects yourself,
you are most likely designing and fielding an electronic survey.
These types of data collection technologies
have greatly accelerated our ability to bring in high-quality data
using purpose-built survey instruments,
and therefore improved the precision of research.
At the same time, electronic surveys create new pitfalls to avoid.
Programming surveys efficiently requires a very different mindset
than simply designing them in word processing software,
and ensuring that they flow correctly and produce data
that can be used in statistical software requires careful organization.
This section will outline the major steps and technical considerations
you will need to follow whenever you field a custom survey instrument,
no matter the scale.

\subsection{Developing a data collection instrument}

A well-designed questionnaire results from careful planning,
consideration of analysis and indicators, close review of existing questionnaires,
survey pilots, and research team and stakeholder review.
There are many excellent resources on questionnaire design,
such as from the World Bank's Living Standards Measurement Survey.\cite{glewwe2000designing}
The focus of this section is the design of electronic field surveys,
often referred to as Computer Assisted Personal Interviews (CAPI).\sidenote{
  \url{https://dimewiki.worldbank.org/Computer-Assisted\_Personal\_Interviews\_(CAPI)}}
Although most surveys are now collected electronically, by tablet, mobile phone or web browser,
\textbf{questionnaire design}\sidenote{
  \url{https://dimewiki.worldbank.org/Questionnaire_Design}}
  \index{questionnaire design}
(content development) and questionnaire programming (functionality development)
should be seen as two strictly separate tasks.
Therefore, the research team should agree on all questionnaire content
and design a paper version of the survey before beginning to program the electronic version.
This facilitates a focus on content during the design process
and ensures teams have a readable, printable version of their questionnaire.
Most importantly, it means the research, not the technology, drives the questionnaire design.

We recommend this approach because an easy-to-read paper questionnaire
is especially useful for training data collection staff,
by focusing on the survey content and structure before diving into the technical component.
It is much easier for enumerators to understand the range of possible participant responses
and how to handle them correctly on a paper survey than on a tablet,
and it is much easier for them to translate that logic to digital functionality later.
Finalizing this version of the questionnaire before beginning any programming
also avoids version control concerns that arise from concurrent work
on paper and electronic survey instruments.
Finally, a readable paper questionnaire is a necessary component of data documentation,
since it is difficult to work backwards from the survey program to the intended concepts.

The workflow for designing a questionnaire will feel much like writing an essay, or writing pseudocode:
begin from broad concepts and slowly flesh out the specifics.\sidenote{
  \url{https://iriss.stanford.edu/sites/g/files/sbiybj6196/f/questionnaire_design_1.pdf}}
It is essential to start with a clear understanding of the
\textbf{theory of change}\sidenote{
  \url{https://dimewiki.worldbank.org/Theory_of_Change}}
and \textbf{experimental design} for your project.
The first step of questionnaire design is to list key outcomes of interest,
as well as the main covariates to control for and any variables needed for experimental design.
The ideal starting point for this is a \textbf{pre-analysis plan}.\sidenote{
  \url{https://dimewiki.worldbank.org/Pre-Analysis_Plan}}

Use the list of key outcomes to create an outline of questionnaire \textit{modules}.
Do not number the modules; instead use a short prefix so they can be easily reordered.
For each module, determine if the module is applicable to the full sample,
an appropriate respondent, and whether or how often the module should be repeated.
A few examples: a module on maternal health only applies to household with a woman who has children,
a household income module should be answered by the person responsible for household finances,
and a module on agricultural production might be repeated for each crop the household cultivated.
Each module should then be expanded into specific indicators to observe in the field.\sidenote{
  \url{https://dimewiki.worldbank.org/Literature_Review_for_Questionnaire}}
At this point, it is useful to do a  \textbf{content-focused pilot}\sidenote{
  \url{https://dimewiki.worldbank.org/Piloting_Survey_Content}}
of the questionnaire.
Doing this pilot with a pen-and-paper questionnaire encourages more significant revisions,
as there is no need to factor in costs of re-programming,
and as a result improves the overall quality of the survey instrument.
Questionnaires must also include ways to document the reasons for \textbf{attrition} and
treatment \textbf{contamination}.
\index{attrition}\index{contamination}
These are essential data components for completing CONSORT records,
a standardized system for reporting enrollment, intervention allocation, follow-up,
and data analysis through the phases of a randomized trial.\cite{begg1996improving}

Once the content of the survey is drawn up,
the team should conduct a small \textbf{survey pilot}\sidenote{
  \url{https://dimewiki.worldbank.org/Survey_Pilot}}
using the paper forms to finalize questionnaire design and detect any content issues.
A content-focused pilot\sidenote{
  \url{https://dimewiki.worldbank.org/Piloting_Survey_Content}}
is best done on pen and paper, before the questionnaire is programmed,
because changes at this point may be deep and structural, which are hard to adjust in code.
The objective is to improve the structure and length of the questionnaire,
refine the phrasing and translation of specific questions,
and confirm coded response options are exhaustive.\sidenote{
  \url{https://dimewiki.worldbank.org/index.php?title=Checklist:_Refine_the_Questionnaire_(Content)}}
In addition, it is an opportunity to test and refine all survey protocols,
such as how units will be sampled or pre-selected units identified.
The pilot must be done out-of-sample,
but in a context as similar as possible to the study sample.
Once the team is satisfied with the content and structure of the survey,
it is time to move on to implementing it electronically.

\subsection{Designing surveys for electronic deployment}

Electronic data collection has great potential to simplify survey implementation and improve data quality.
Electronic questionnaires are typically created in a spreadsheet (e.g. Excel or Google Sheets)
or a software-specific form builder, all of which are accessible even to novice users.\sidenote{
  \url{https://dimewiki.worldbank.org/Questionnaire_Programming}}
We will not address software-specific form design in this book;
rather, we focus on coding conventions that are important to follow
for electronic surveys regardless of software choice.\sidenote{
  \url{https://dimewiki.worldbank.org/SurveyCTO_Coding_Practices}}
Survey software tools provide a wide range of features
designed to make implementing even highly complex surveys easy, scalable, and secure.
However, these are not fully automatic: you need to actively design and manage the survey.
Here, we discuss specific practices that you need to follow
to take advantage of electronic survey features
and ensure that the exported data is compatible with your analysis software.

From a data perspective, questions with pre-coded response options
are always preferable to open-ended questions.
The content-based pilot is an excellent time to ask open-ended questions
and refine fixed responses for the final version of the questionnaire --
do not count on coding up lots of free text after a full survey.
Coding responses helps to ensure that the data will be useful for quantitative analysis.
Two examples help illustrate the point.
First, instead of asking ``How do you feel about the proposed policy change?'',
use techniques like \textbf{Likert scales}\sidenote{
  \textbf{Likert scale:} an ordered selection of choices indicating the respondent's level of agreement or disagreement with a proposed statement.}.
Second, if collecting data on medication use or supplies, you could collect:
the brand name of the product; the generic name of the product; the coded compound of the product;
or the broad category to which each product belongs (antibiotic, etc.).
All four may be useful for different reasons,
but the latter two are likely to be the most useful for data analysis.
The coded compound requires providing a translation dictionary to field staff,
but enables automated rapid recoding for analysis with no loss of information.
The generic class requires agreement on the broad categories of interest,
but allows for much more comprehensible top-line statistics and data quality checks.
Rigorous field testing is required to ensure that answer categories are comprehensive;
however, it is best practice to include an \textit{other, specify} option.
Keep track of those responses in the first few weeks of field work.
Adding an answer category for a response frequently showing up as \textit{other} can save time,
as it avoids extensive post-coding.

It is essential to name the fields in your questionnaire
in a way that will also work in your data analysis software.
Most survey programs will not enforce this by default,
since limits vary by software,
and surveys will subtly encourage you to use long sentences
and detailed descriptions of choice options.
This is what you want for the enumerator-respondent interaction,
but you should already have analysis-compatible labels programmed in the background
so the resulting data can be rapidly imported in analytical software.
There is some debate over how exactly individual questions should be identified:
formats like \texttt{hq\_1} are hard to remember and unpleasant to reorder,
but formats like \texttt{hq\_asked\_about\_loans} quickly become cumbersome.
We recommend using descriptive names with clear prefixes so that variables
within a module stay together when sorted alphabetically.\sidenote{
  \url{https://medium.com/@janschenk/variable-names-in-survey-research-a18429d2d4d8}}
Variable names should never include spaces or mixed cases
(we prefer all-lowercase naming).
Take special care with the length: very long names will be cut off in some softwares,
which could result in a loss of uniqueness and lots of manual work to restore compatibility.
We further discourage explicit question numbering,
at least at first, as it discourages re-ordering questions,
which is a common recommended change after the pilot.
In the case of follow-up surveys, numbering can quickly become convoluted,
too often resulting in uninformative variables names like
\texttt{ag\_15a}, \texttt{ag\_15\_new}, \texttt{ag\_15\_fup2}, and so on.

\subsection{Programming electronic questionnaires}

The starting point for questionnaire programming is therefore a complete paper version of the questionnaire,
piloted for content and translated where needed.
Doing so reduces version control issues that arise from making significant changes
to concurrent paper and electronic survey instruments.
Changing structural components of the survey after programming has been started
often requires the coder to substantially re-work the entire code.
This is because the more efficient way to code surveys is non-linear.
When programming, we do not start with the first question and proceed through to the last question.
Instead, we code from high level to small detail,
following the same questionnaire outline established at design phase.
The outline provides the basis for pseudocode,
allowing you to start with high level structure and work down to the level of individual questions.
This will save time and reduce errors,
particularly where sections or field are interdependent or repeated in complex ways.

Electronic surveys are more than simply a paper questionnaire displayed on a mobile device or web browser.
All common survey software allow you to automate survey logic
and add in hard and soft constraints on survey responses.
These features make enumerators' work easier,
and they create the opportunity to identify and resolve data issues in real-time,
simplifying data cleaning and improving response quality.
Well-programmed questionnaires should include most or all of the following features:

\begin{itemize}
  \item{\textbf{Localizations}}: the survey instrument should display full text questions and responses in the survey language, and it should also have English and code-compatible versions of all text and labels.
	\item{\textbf{Survey logic}}: build in all logic, so that only relevant questions appear, rather than relying on enumerators to follow complex survey logic. This covers simple skip codes, as well as more complex interdependencies (e.g., a child health module is only asked to households that report the presence of a child under 5).
	\item{\textbf{Range checks}}:  add range checks for all numeric variables to catch data entry mistakes (e.g. age must be less than 120).
	\item{\textbf{Confirmation of key variables}}: require double entry of essential information (such as a contact phone number in a survey with planned phone follow-ups), with automatic validation that the two entries match.
	\item{\textbf{Multimedia}}: electronic questionnaires facilitate collection of images, video, and geolocation data directly during the survey, using the camera and GPS built into the tablet or phone.
	\item{\textbf{Preloaded data}}: data from previous rounds or related surveys can be used to prepopulate certain sections of the questionnaire, and validated during the interview.
	\item{\textbf{Filtered response options}}: filters reduce the number of response options dynamically (e.g. filtering the cities list based on the state provided).
	\item{\textbf{Location checks}}: enumerators submit their actual location using in-built GPS, to confirm they are in the right place for the interview.
	\item{\textbf{Consistency checks}}: check that answers to related questions align, and trigger a warning if not so that enumerators can probe further (.e.g., if a household reports producing 800 kg of maize, but selling 900 kg of maize from their own production).
	\item{\textbf{Calculations}}: make the electronic survey instrument do all math, rather than relying on the enumerator or asking them to carry a calculator.
\end{itemize}

All survey softwares include debugging and test options
to correct syntax errors and make sure that the survey instruments will successfully compile.
This is not sufficient, however, to ensure that the resulting dataset
will load without errors in your data analysis software of choice.
We developed the \texttt{ietestform} command,\sidenote{
  \url{https://dimewiki.worldbank.org/ietestform}}
part of the Stata package \texttt{iefieldkit},
to implement a form-checking routine for \textbf{SurveyCTO},
a proprietary implementation of the \textbf{Open Data Kit (ODK)} software.
Intended for use during questionnaire programming and before field work,
\texttt{ietestform} tests for best practices in coding, naming and labeling, and choice lists.
Although \texttt{ietestform} is software-specific,
many of the tests it runs are general and important to consider regardless of software choice.
To give a few examples, \texttt{ietestform} tests that no variable names exceed
32 characters, the limit in Stata (variable names that exceed that limit will
be truncated, and as a result may no longer be unique).
It checks whether ranges are included for numeric variables.
\texttt{ietestform} also removes all leading and trailing blanks from response lists,
which could be handled inconsistently across software.

A second survey pilot should be done after the questionnaire is programmed.
The objective of this \textbf{data-focused pilot}\sidenote{
  \url{https://dimewiki.worldbank.org/index.php?title=Checklist:_Refine_the_Questionnaire_(Data)}}
is to validate the programming and export a sample dataset.
Significant desk-testing of the instrument is required to debug the programming
as fully as possible before going to the field.
It is important to plan for multiple days of piloting,
so that any further debugging or other revisions to the electronic survey instrument
can be made at the end of each day and tested the following, until no further field errors arise.
The data-focused pilot should be done in advance of enumerator training.


\subsection{Finalizing data collection}

When all data collection is complete, the survey team should prepare a final field report,
which should report reasons for any deviations between the original sample and the data collected.
Identification and reporting of \textbf{missing data} and \textbf{attrition}
is critical to the interpretation of survey data.
It is important to structure this reporting in a way that not only
groups broad rationales into specific categories
but also collects all the detailed, open-ended responses
to questions the field team can provide for any observations that they were unable to complete.
This reporting should be validated and saved alongside the final raw data, and treated the same way.
This information should be stored as a dataset in its own right
-- a \textbf{tracking dataset} -- that records all events in which survey substitutions
and attrition occurred in the field and how they were implemented and resolved.


%------------------------------------------------
\section{Handling data securely}

All confidential data must be handled in a way such that only people specifically 
approved by an Institutional Review Board (IRB)\sidenote{
  \url{https://dimewiki.worldbank.org/IRB\_Approval}}
for the specific project are able to access the data.
Data can be confidential for multiple reasons, but the two most
common reasons are that it contains personally identifiable information (PII)\sidenote{
  \url{https://dimewiki.worldbank.org/Personally\_Identifiable\_Information\_(PII)}}
or that the partner providing the data has specified restricted access.

\index{encryption}\textbf{Data encryption} is central to secure data handling. 
Data encryption is a group of methods that ensure that files are unreadable 
even if laptops are stolen, serversare hacked, 
or unauthorized access to the data is obtained in any other way.\sidenote{
  \url{https://dimewiki.worldbank.org/Encryption}}
Proper encryption is rarely just a single method,
as the data will travel through many servers, devices, and computers
from the source of the data to the final analysis.
Encryption should be seen as a system that is only as secure as its weakest link.
This section recommends a streamlined workflow,
so that it is easy as possible to make sure the weakest link is still sufficiently secure.

Encrypted data is made readable again using decryption,
and decryption requires a password or a key.
It is never secure to share passwords or keys by email,
WhatsApp or other insecure modes of communication;
instead, use a secure password manager.\sidenote{
  \url{https://lastpass.com} or \url{https://bitwarden.com}}
In addition to providing a way to securely share passwords,
password managers also provide a secure location
for long term storage for passwords and keys, whether they are shared or not.

Many data-sharing software providers will promote their services
by advertising on-the-fly encryption and decryption.
While useful, on-the-fly encryption is never sufficient for securing confidential data.
In order to automate on-the-fly encryption,
the service provider needs to keep a copy of the password or key.
This implies the service provider has access to the data.
This is the case for all file syncing software such as Dropbox,
which, in addition to being able to view your data, may require
additional legal usage rights in order to host it on their servers.
It is possible, in some enterprise versions of data sharing software,
to set up appropriately secure on-the-fly encryption.
However, that setup is advanced, and you should never trust it
unless a cybersecurity expert within your organization
has specified what it can be used for.
In all other cases you should follow the steps laid out in this section.


\subsection{Receiving data from development partners}

Research teams granted access to existing data may receive that data in a number of different ways.
You may receive access to servers or accounts that already exist.
You may receive a one-time transfer of a block of data,
or you may be given access to a restricted area to extract information.
In all cases, you must take action to ensure that data is transferred through 
secure channels so that confidential data is not compromised.
Talk to an information-technology specialist,
either at your organization or at the partner organization,
to ensure that data is being transferred, received, and stored
in a method that conforms to the relevant level of security.
Keep in mind that compliance with ethical standards may 
in some cases require a stricter level of security than initially proposed by the partner agency.

Another important consideration at this stage is 
proper documentation and cataloging of data and associated metadata.
It is not always clear what pieces of information jointly constitute a ``dataset'',
and many of the sources you receive data from will not be organized for research.
To help you keep organized and to put some structure on the materials you will be receiving,
you should always retain the original data as received
alongside a copy of the corresponding ownership agreement or license.
You should make a simple ``readme'' document noting the date of receipt,
the source and recipient of the data, and a brief description of what it is.
All too often data produced by systems is provided as vaguely-named spreadsheets,
or transferred as electronic communications with non-specific titles,
and it is not possible to keep track of these kinds of information as data over time.
Eventually, you will want to make sure that you are creating a collection or object
that can be properly submitted to a data catalog and given a reference and citation.
The metadata - documentation about the data - is critical for future use of the data.
Metadata should include documentation of how the data was created, 
what they measure, and how they are to be used.
In the case of survey data, this includes the survey instrument and associated manuals; 
the sampling protocols and field adherence to those protocols, and any sampling weights;
what variable(s) uniquely identify the dataset(s), and how different datasets can be linked;
and a description of field procedures and quality controls. 
We use as a standard the Data Documentation Initiative (DDI), which is supported by the 
World Bank's Microdata Catalog.\sidenote{\url{https://microdata.worldbank.org}}

As soon as the requisite pieces of information are stored together,
think about which ones are the components of what you would call a dataset.
This is more of an art than a science:
you want to keep things together that belong together,
but you also want to keep things apart that belong apart.
There usually won't be a precise way to answer this question,
so consult with others about what is the appropriate level of aggregation
for the data you have endeavored to obtain.
This is the object you will think about cataloging, releasing, and licensing
as you move towards the publication part of the research process.
This may require you to re-check with the provider
about what portions are acceptable to license,
particularly if you are combining various datasets
that may provide even more information about specific individuals.

\subsection{Collecting data securely}

In field surveys, most common data collection software will automatically encrypt
all data in transit (i.e., upload from field or download from server).\sidenote{
  \url{https://dimewiki.worldbank.org/Encryption\#Encryption\_in\_Transit}}
If this is implemented by the software you are using,
then your data will be encrypted from the time it leaves the device
(in tablet-assisted data collection) or browser (in web data collection),
until it reaches the server.
Therefore, as long as you are using an established survey software,
this step is largely taken care of.
However, the research team must ensure that all computers, tablets,
and accounts that are used in data collection have a secure logon
password and are never left unlocked.

Even though your data is therefore usually safe while it is being transmitted,
it is not automatically secure when it is being stored.
\textbf{Encryption at rest}\sidenote{
  \url{https://dimewiki.worldbank.org/Encryption\#Encryption\_at\_Rest}}
is the only way to ensure that confidential data remains private when it is stored on a
server on the internet.
You must keep your data encrypted on the data collection server whenever PII is collected,
or when this is required by the data sharing agreement.
If you do not, the raw data will be accessible by
individuals who are not approved by your IRB,
such as tech support personnel,
server administrators, and other third-party staff.
Encryption at rest must be used to make
data files completely unusable without access to a security key specific to that
data -- a higher level of security than password-protection.
Encryption at rest requires active participation from the user,
and you should be fully aware that if your decryption key is lost,
there is absolutely no way to recover your data.

You should not assume that your data is encrypted at rest by default because of
the careful protocols necessary.
In most data collection platforms,
encryption at rest needs to be explicitly enabled and operated by the user.
There is no automatic way to implement this protocol,
because the encryption key that is generated may
never pass through the hands of a third party,
including the data storage application.
Most survey software implement \textbf{asymmetric encryption}\sidenote{
  \url{https://dimewiki.worldbank.org/Encryption\#Asymmetric\_Encryption}}
where there are two keys in a public/private key pair.
Only the private key can be used to decrypt the encrypted data,
and the public key can only be used to encrypt the data.
It is therefore safe to send the public key
to the tablet or the browser used to collect the data.

When you enable encryption, the survey software will allow you to create and
download -- once -- the public/private key pair needed to encrypt and decrypt the data.
You upload the public key when you start a new survey, and all data collected using that
public key can only be accessed with the private key from that specific public/private key pair.
You must store the key pair in a secure location, such as a password manager,
as there is no way to access your data if the private key is lost.
Make sure you store keyfiles with descriptive names to match the survey to which they correspond.
Any time anyone accesses the data --
either when viewing it in the browser or downloading it to your computer --
they will be asked to provide the key.
Only project team members named in the IRB are allowed access to the private key.

\subsection{Storing data securely}

For most analytical needs, you typically need a to store the data somewhere other
than the survey software's server, for example, on your computer or a cloud drive.
While public/private key encryption is optimal for one-way transfer
from the data collection device to the data collection server,
it is not practical once you start interacting with the data.
Instead, we use \textbf{symmetric encryption}\sidenote{
  \url{https://dimewiki.worldbank.org/Encryption\#Symmetric\_Encryption}}
where we create a secure encrypted folder,
using, for example, VeraCrypt.\sidenote{\url{https://www.veracrypt.fr}}
Here, a single key is used to both encrypt and decrypt the information.
Since only one key is used, the workflow can be simplified:
the re-encryption after decrypted access can be done automatically,
and the same secure folder can be used for multiple files.
These files can be interacted with and modified like any unencrypted file as long as you have the key.
The following workflow allows you to receive data and store it securely,
without compromising data security:

\begin{enumerate}
	\item Create a secure encrypted folder in your project folder.
  This should be on your computer, and could be in a shared folder.
	\item Download data from the data collection server to that secure folder.
	If you encrypted the data during data collection, you will need \textit{both} the
	private key used during data collection to be able to download the data,
	\textit{and} you will need the key used when you created the secure folder to save it there.
	This your first copy of your raw data, and the copy you will use in your cleaning and analysis.
	\item Create a secure folder on a flash drive or a external hard drive that you can keep in your office.
x	Copy the data you just downloaded to this second secure	folder.
	This is your ``master'' copy of your raw data.
	(Instead of creating a second secure folder, you can simply copy the first secure folder.)
	\item Finally, create a third secure folder.
	Either you can create this on your computer and upload it to a long-term cloud storage service,
	or you can create it on	an external hard drive that you then store in a separate location,
	for example, at another office of your organization.
	This is your ``golden master'' copy of your raw	data.
	You should never store the ``golden master'' copy of your raw data in a synced
	folder, where it is also deleted in the cloud storage if it is deleted on your computer.
	(Instead of creating a third secure folder, you can simply copy the first secure folder.)
\end{enumerate}

\noindent This handling satisfies the \textbf{3-2-1 rule}:
there are two on-site copies of the data and one off-site copy,
so the data can never be lost in case of hardware failure.\sidenote{
  \url{https://www.backblaze.com/blog/the-3-2-1-backup-strategy}}
However, you still need to keep track of your encryption keys as without them your data is lost.
If you remain lucky, you will never have to access your ``master'' or ``golden master'' copies --
you just want to know it is there, safe, if you need it.

\subsection{Sharing data securely}
You and your team will use your first copy of the raw data
as the starting point for data cleaning and analysis of the data.
This raw dataset must remain encrypted at all times if it includes confidential data,
which is almost always the case.
As long as the data is properly encrypted,
it can be shared using insecure modes of communication
such as email or third-party syncing services.
While this is safe from a data security perspective,
this is a burdensome workflow, as anyone accessing the raw data must be listed on the IRB,
have access to the decryption key and know how to use that key.
Fortunately, there is a way to simplify the workflow without compromising data security.

To simplify the workflow,
the confidential variables should be removed from your data at the earliest possible opportunity.
This is particularly common in survey data,
as identifying variables are often only needed during data collection.
In this case, such variables may be removed as soon as the field work is completed,
creating a de-identified copy of the data.
Once the data is de-identified,
it no longer needs to be encrypted --
you and you team members can share it directly
without having to encrypt it and handle decryption keys.
The next chapter will discuss how to de-identify your data.
This may not be so straightforward when access to the data
is restricted by request of the data owner.
If confidential information is directly required for the analysis itself,
it will be necessary to keep at least a subset of the data encrypted through the data analysis process.

The data security standards that apply when receiving confidential data also apply when transferring confidential data.
A common example where this is often forgotten involves sharing survey information,
such as sampling lists, with a field partner.
This data is -- by all definitions -- also PII data and must be encrypted.
A sampling list can often be used to reverse identify a de-identified dataset,
so if you were to share it using an insecure method,
then that would be your weakest link that could render useless all the other steps
you have taken to ensure the privacy of the respondents.

In some survey software, you can use the same encryption that allows you to receive data securely
from the field, to also send data, such a sampling list, to the field.
But if you are not sure how that is done, or even can be done,
in the survey software you are using,
then you should create a secure folder using, for example,
VeraCrypt and share that secure folder with the field team.
Remember that you must always share passwords and keys in a secure way like password managers.

At this point, the raw data securely stored and backed up.
It can now be transformed into your final analysis dataset,
through the steps described in the next chapter.
Once the data collection is over,
you typically will no longer need to interact with the identified data.
So you should create a working version of it that you can safely interact with.
This is described in the next chapter as the first task in the data cleaning process,
but it's useful to get it started as soon as encrypted data is downloaded to disk.
