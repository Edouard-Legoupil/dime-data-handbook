%------------------------------------------------

\begin{fullwidth}
Publishing academic research today extends well beyond writing up and submitting a Word document alone.
Typically, various contributors collaborate on both code and writing,
manuscripts go through many iterations and revisions,
and the final package for publication includes not just a manuscript
but also the code and data used to generate the results.
Ideally, your team will spend as little time as possible
fussing with the technical requirements of publication.
It is in nobody's interest for a skilled and busy researcher
to spend days re-numbering references (and it can take days)
if a small amount of up-front effort could automate the task.
In this section we suggest several methods --
collectively refered to as ``dynamic documents'' --
for managing the process of collaboration on any technical product.

For most research projects, completing a manuscript is not the end of the task.
Academic journals increasingly require submission of a replication package,
which contains the code and materials needed to create the results.
These represent an intellectual contribution in their own right,
because they enable others to learn from your process
and better understand the results you have obtained.
Holding code and data to the same standards as written work
is a new discipline for many researchers,
and here we provide some basic guidelines and responsibilities for that process
that will help you prepare a functioning and informative replication package.
In all cases, we note that technology is rapidly evolving
and that the specific tools noted here may not remain cutting-edge,
but the core principles involved in publication and transparency will endure.
\end{fullwidth}

%------------------------------------------------

\section{Collaborating on technical writing}

It is increasingly rare that a single author will prepare an entire manuscript alone.
More often than not, documents will pass back and forth between several writers
before they are ready for publication,
so it is essential to use technology and workflows that avoid conflicts.
Just as with the preparation of analytical outputs,
this means adopting tools and practices that enable tasks
such as version control and simultaneous contribution.
Furthermore, it means preparing documents that are \textbf{dynamic} --
meaning that updates to the analytical outputs that constitute them
can be passed on to the final output with a single process,
rather than copy-and-pasted or otherwise handled individually.
Thinking of the writing process in this way
is intended to improve organization and reduce error,
such that there is no risk of materials being compiled
with out-of-date results, or of completed work being lost or redundant.

\subsection{Dynamic documents}

Dynamic documents are a broad class of tools that enable a streamlined, reproducible workflow.
The term ``dynamic'' can refer to any document-creation technology
that allows the inclusion of explicitly encoded linkages to raw output files.
This means that, whenever outputs are updated,
the next time the document is loaded or compiled, it will automatically include
all changes made to all outputs without any additional intervention from the user.
This means that updates will never be accidentally excluded,
and it further means that updating results will not become more difficult
as the number of inputs grows,
because they are all managed by a single integrated process.

You will note that this is not possible in tools like Microsoft Office,
although there are various tools and add-ons that produce similar functionality,
and we will introduce some later in this book.
In Word, by default, you have to copy and paste each object individually
whenever tables, graphs, or other inputs have to be updated.
This creates complex inefficiency: updates may be accidentally excluded
and ensuring they are not will become more difficult as the document grows.
As time goes on, it therefore becomes more and more likely
that a mistake will be made or something will be missed.
Furthermore, it is very hard to simultaneously edit or track changes
in a Microsoft Word document.
It is usually the case that a file needs to be passed back and forth
and the order of contributions strictly controlled
so that time-consuming resolutions of differences can be avoided.
Therefore this is a broadly unsuitable way to prepare technical documents.

There are a number of tools that can be used for dynamic documents.
In the first group are code-based tools such as R's RMarkdown\sidenote{\url{https://rmarkdown.rstudio.com/}}
and Stata's \texttt{dyndoc}\sidenote{\url{https://www.stata.com/manuals/rptdyndoc.pdf}}.
These tools ``knit'' or ``weave'' text and code together,
and are programmed to insert code outputs in pre-specified locations.
Documents called ``notebooks'' (such as Jupyter\sidenote{\url{https://jupyter.org/}}) work similarly,
as they also use the underlying analytical software to create the document.
These types of dynamic documents are usually appropriate for short or informal materials
because they tend to offer restricted editability outside the base software
and often have limited abilities to incorporate precision formatting.

The second group of dynamic document tools do not require
direct operation of underlying code or software, but simply require
that the writer have access to the updated outputs.
One very simple one is Dropbox Paper, a free online writing tool
that allows linkages to files in Dropbox,
which are then automatically updated anytime the file is replaced.
Dropbox Paper has very few formatting options,
but it is appropriate for working with collaborators who are not using statistical software.

However, the most widely utilized software
for dynamically managing both text and results is \LaTeX\ (pronounced ``lah-tek'').\sidenote{
  \url{https://www.maths.tcd.ie/~dwilkins/LaTeXPrimer/GSWLaTeX.pdf}}
  \index{\LaTeX}
Rather than using a coding language that is built for another purpose
or trying to hide the code entirely,
\LaTeX\ is a special code language designed for document preparation and typesetting.
While this tool has a significant learning curve,
its enormous flexibility in terms of operation, collaboration,
and output formatting and styling
makes it the primary choice for most large technical outputs today,
and it has proven to have enduring popularity.
In fact, \LaTeX\ operates behind-the-scenes in many of the tools listed before.
Therefore, we recommend that you learn to use \LaTeX\ directly
as soon as you are able to and provide several resources for doing so.

\subsection{Technical writing with \LaTeX}

\LaTeX\ is billed as a ``document preparation system''.
What this means is worth unpacking.
In {\LaTeX}, instead of writing in a ``what-you-see-is-what-you-get'' mode
as you do in Word or the equivalent,
you write plain text interlaced with coded instructions for formatting
(similar in concept to HTML).
Because it is written in a plain text file format,
\texttt{.tex} can be version-controlled using Git.
This is why it has become the dominant ``document preparation system'' in technical writing.
\LaTeX\ enables automatically-organized documents,
manages tables and figures dynamically,
and includes commands for simple markup
like font styles, paragraph formatting, section headers and the like.
It also includes special controls for including tables and figures,
footnotes and endnotes, complex mathematical notation, and automated bibliography preparation.
It also allows publishers to apply global styles and templates to already-written material,
allowing them to reformat entire documents in house styles with only a few keystrokes.

One of the most important tools available in \LaTeX\ is the BibTeX bibliography manager.\sidenote{
  \url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.365.3194&rep=rep1&type=pdf}}
BibTeX keeps all the references you might use in an auxiliary file,
then references them using a simple element typed directly in the document: a \texttt{cite} command.
The same principles that apply to figures and tables are therefore applied here:
You can make changes to the references in one place (the \texttt{.bib} file),
and then everywhere they are used they are updated correctly with one process.
Specifically, \LaTeX\ inserts references in text using the \texttt{\textbackslash cite\{\}} command.
Once this is written, \LaTeX\ automatically pulls all the citations into text
and creates a complete bibliography based on the citations you use when you compile the document.
The system allows you to specify exactly how references should be displayed in text
(such as superscripts, inline references, etc.)
as well as how the bibliography should be styled and in what order
(such as Chicago, MLA, Harvard, or other common styles).
To obtain the references for the \texttt{.bib} file,
you can copy the specification directly from Google Scholar
by clicking ``BibTeX'' at the bottom of the Cite window.
When pasted into the \texttt{.bib} file they look like the following:

\codeexample{sample.bib}{./code/sample.bib}

\noindent BibTeX citations are then used as follows:

\codeexample{citation.tex}{./code/citation.tex}

With these tools, you can ensure that references are handled
in a format you can manage and control.\cite{flom2005latex}
Finally, \LaTeX\ has one more useful trick:
using \textbf{\texttt{pandoc}},\sidenote{
  \url{http://pandoc.org/}}
you can translate the raw document into Word
(or a number of other formats)
by running the following code from the command line:

\codeexample{pandoc.sh}{./code/pandoc.sh}

\noindent The last portion after \texttt{csl=} specifies the bibliography style.
You can download a CSL (Citation Styles Library) file\sidenote{
  \url{https://github.com/citation-style-language/styles}}
for nearly any journal and have it applied automatically in this process.
Therefore, even in the case where you are requested to provide
\texttt{.docx} versions of materials to others, or tracked-changes versions,
you can create them effortlessly,
and use external tools like Word's compare feature
to generate integrated tracked versions when needed.

Unfortunately, despite these advantages, \LaTeX\ can be a challenge to set up and use at first,
particularly if you are new to working with plain text code and file management.
It is also unfortunately weak with spelling and grammar checking.
This is because \LaTeX\ requires that all formatting be done in its special code language,
and it is not particularly informative when you do something wrong.
This can be off-putting very quickly for people
who simply want to get to writing, like senior researchers.
While integrated editing and compiling tools like TeXStudio\sidenote{
  \url{https://www.texstudio.org}}
and \texttt{atom-latex}\sidenote{
  \url{https://atom.io/packages/atom-latex}}
offer the most flexibility to work with \LaTeX\ on your computer,
such as advanced integration with Git,
the entire group of writers needs to be comfortable
with \LaTeX\ before adopting one of these tools.
They can require a lot of troubleshooting at a basic level at first,
and non-technical staff may not be willing or able to acquire the required knowledge.
Therefore, to take advantage of the features of \LaTeX,
while making it easy and accessible to the entire writing team,
we need to abstract away from the technical details where possible.

\subsection{Getting started with \LaTeX\ in the cloud}

\LaTeX\ is a challenging tool to get started using,
but the control it offers over the writing process is invaluable.
In order to make it as easy as possible for your team
to use \LaTeX\ without all members having to invest in new skills,
we suggest using a web-based implementation as your first foray into \LaTeX\ writing.
Most such sites offer a subscription feature with useful extensions and various sharing permissions,
and some offer free-to-use versions with basic tools that are sufficient
for a broad variety of applications,
up to and including writing a complete academic paper with coauthors.

Cloud-based implementations of \LaTeX\ are suggested here for several reasons.
Since they are completely hosted online,
they avoids the inevitable troubleshooting of setting up a \LaTeX\ installation
on various personal computers run by the different members of your team.
They also typically maintain a single continuously synced master copy of the document
so that different writers do not create conflicted or out-of-sync copies,
or need to deal with Git themselves to maintain that sync.
They typically allow inviting collaborators to edit in a fashion similar to Google Docs,
though different services vary the number of collaborators and documents allowed at each tier.
Most importantly, some tools provide a ``rich text'' editor
that behaves pretty similarly to familiar tools like Word,
so that collaborators can write text directly into the document without worrying too much
about the underlying \LaTeX\ coding.
Cloud services also usually offer a convenient selection of templates
so it is easy to start up a project and see results right away
without needing to know a lot of the code that controls document formatting.

On the downside, there is a small amount of up-front learning required,
continous access to the Internet is necessary,
and updating figures and tables requires a bulk file upload that is tough to automate.
One of the most common issues you will face using online editors will be special characters
which, because of code functions, need to be handled differently than in Word.
Most critically, the ampersand (\texttt{\&}), percent (\texttt{\%}), and underscore (\texttt{\_})
need to be ``escaped'' (interpreted as text and not code) in order to render.
This is done by by writing a backslash (\texttt{\textbackslash}) before them,
such as writing \texttt{40\textbackslash\%} for the percent sign to appear in text.
Despite this, we believe that with minimal learning and workflow adjustments,
cloud-based implementations are often the easiest way to allow coauthors to write and edit in \LaTeX\,
so long as you make sure you are available to troubleshoot minor issues like these.

%------------------------------------------------

\section{Preparing a complete replication package}

While we have focused so far on the preparation of written materials for publication,
it is increasingly important for you to consider how you will publish
the data and code you used for your research as well.
More and more major journals are requiring that publications
provide direct links to both the code and data used to create the results,
and some even require being able to reproduce the results themselves
before they will approve a paper for publication.\sidenote{
  \url{https://www.aeaweb.org/journals/policies/data-code/}}
If your material has been well-structured throughout the analytical process,
this will only require a small amount of extra work;
if not, paring it down to the ``replication package'' may take some time.
A complete replication package should accomplish several core functions.
It must provide the exact data and code that is used for a paper,
all necessary de-identified data for the analysis,
and all code necessary for the analysis.
The code should exactly reproduce the raw outputs you have used for the paper,
and should include no documentation or PII data you would not share publicly.

\subsection{Publishing data for replication}

Enabling permanent access to the data used in your study
is an important contribution you can make along with the publication of results.
It allows other researchers to validate the mechanical construction of your results,
to investigate what other results might be obtained from the same population,
and test alternative approaches to other questions.
Therefore you should make clear in your study
where and how data are stored, and how and under what circumstances it might be accessed.
You do not always have to complete the data publication data yourself,
as long as you cite or otherwise directly reference data that you cannot release.
Even if you think your raw data is owned by someone else,
in many cases you will have the right to release
at least some subset of your analytical dataset or the indicators you constructed.
Check with the data supplier or other professional about licensing questions,
particularly your right to publish derivative materials.
You should only directly publish data which is fully de-identified
and, to the extent required to ensure reasonable privacy,
potentially identifying characteristics are further masked or removed.
In all other cases, you should contact an appropriate data catalog
to determine what privacy and licensing options are available.

Make sure you have a clear understanding of the rights associated with the data release
and communicate them to any future users of the data.
You must provide a license with any data release.\sidenote{
  \url{https://iatistandard.org/en/guidance/preparing-organisation/organisation-data-publication/how-to-license-your-data/}}
This document need not be extremely detailed,
but it should clearly communicate to the reader what they are allowed to do with your data and
how credit should be given and to whom in further work that uses it.
Keep in mind that you may or may not own your data,
depending on how it was collected,
and the best time to resolve any questions about these rights
is at the time that data collection or transfer agreements are signed.
Even if you cannot release data immediately or publicly,
there are often options to catalog or archive the data without open publication.
These may take the form of metadata catalogs or embargoed releases.
Such setups allow you to hold an archival version of your data
which your publication can reference,
as well as provide information about the contents of the datasets
and how future users might request permission to access them
(even if you are not the person who can grant that permission).
They can also provide for timed future releases of datasets
once the need for exclusive access has ended.

Data publication should release the dataset in a widely recognized format.
While software-specific datasets are acceptable accompaniments to the code
(since those precise materials are probably necessary),
you should also consider releasing generic datasets
such as CSV files with accompanying codebooks,
since these will be re-adaptable by any researcher.
Additionally, you should also release
the data collection instrument or survey questionnaire
so that readers can understand which data components are
collected directly in the field and which are derived.
You should provide a clean version of the data
which corresponds exactly to the original database or questionnaire
as well as the constructed or derived dataset used for analysis.
Wherever possible, you should also release the code
that constructs any derived measures,
particularly where definitions may vary,
so that others can learn from your work and adapt it as they like.

\subsection{Publishing code for replication}

Before publishing your code, you should edit it for content and clarity
just as if it were written material.
The purpose of releasing code is to allow others to understand
exactly what you have done in order to obtain your results,
as well as to apply similar methods in future projects.
Therefore it should both be functional and readable.
If you've followed the recommendations in this book,
this will be much easier to do.
Code is often not written this way when it is first prepared,
so it is important for you to review the content and organization
so that a new reader can figure out what and how your code should do.
Therefore, whereas your data should already be very clean at this stage,
your code is much less likely to be so, and this is where you need to make
time investments prior to releasing your replication package.
By contrast, replication code usually has few legal and privacy constraints.
In most cases code will not contain identifying information;
but make sure to check carefully that it does not.
Publishing code also requires assigning a license to it;
in a majority of cases, code publishers like GitHub
offer extremely permissive licensing options by default.
(If you do not provide a license, nobody can use your code!)

Make sure the code functions identically on a fresh install of your chosen software.
A new user should have no problem getting the code to execute perfectly.
In either a scripts folder or in the root directory,
include a master script (dofile or R script for example).
The master script should allow the reviewer
to change a single line of code: the one setting the directory path.
After that, running the master script should run the entire project
and re-create all the raw outputs exactly as supplied.
Indicate the filename and line to change.
Check that all your code will run completely on a new computer:
Install any required user-written commands in the master script
(for example, in Stata using \texttt{ssc install} or \texttt{net install}
and in R include code giving users the option to install packages,
including selecting a specific version of the package if necessary).
In many cases you can even directly provide the underlying code
for any user-installed packages that are needed to ensure forward-compatibility.
Make sure system settings like \texttt{version}, \texttt{matsize}, and \texttt{varabbrev} are set.

Finally, make sure that the code and its inputs and outputs are clearly identified.
A new user should, for example, be able to easily identify and remove
any files created by the code so that they can be recreated quickly.
They should also be able to quickly map all the outputs of the code
to the locations where they are placed in the associated published material,
such as ensuring that the raw components of figures or tables are clearly identified.
Documentation in the master script is often used to indicate this information.
For example, outputs should clearly correspond by name to an exhibit in the paper, and vice versa.
(Supplying a compiling \LaTeX\ document can support this.)
Code and outputs which are not used should be removed.

\subsection{Releasing a replication package}

If you are at this stage,
all you need to do is find a place to publish your materials.
This is slightly easier said than done,
as there are a few variables to take into consideration
and, at the time of writing, no global consensus on the best solution.
The technologies available are likely to change dramatically
over the next few years;
the specific solutions we mention here highlight some current approaches
as well as their strengths and weaknesses.
GitHub provides one solution.
Making a GitHub repository public is completely free.
It can hold any file types,
provide a structured download of your whole project,
and allow others to look at alternate versions or histories easily.
It is straightforward to simply upload a fixed directory to GitHub
apply a sharing license, and obtain a URL for the whole package.
(However, there is a strict size restriction of 100MB per file and
a restriction on the size of the repository as a whole,
so larger projects will need alternative solutions.)

However, GitHub is not ideal for other reasons.
It is not built to hold data in an efficient way
or to manage licenses or citations for datasets.
It does not provide a true archive service --
you can change or remove the contents at any time.
A repository such as the Harvard Dataverse\sidenote{
  \url{https://dataverse.harvard.edu}}
addresses these issues, as it is designed to be a citable code repository.
The Open Science Framework\sidenote{
  \url{https://osf.io}}
also provides a balanced implementation
that holds both code and data (as well as simple version histories),
as does ResearchGate\sidenote{
  \url{https://https://www.researchgate.net}}
(both of which can also assign a permanent digital object identifier link for your work).
Any of these locations is acceptable --
the main requirement is that the system can handle
the structured directory that you are submitting,
and that it can provide a stable, structured URL for your project
and report exactly what, if any, modifications you have made since initial publication.
You can even combine more than one tool if you prefer,
as long as they clearly point to each other.
Emerging technologies such as CodeOcean\sidenote{
  \url{https://codeocean.com}}
offer to store both code and data,
and also provide an online workspace in which others
can execute and modify your code
without having to download your tools and match your local environment
when packages and other underlying softwares may have changed since publication.

In addition to code and data,
you may also want to release an author's copy or preprint
of the article itself along with these raw materials.
Check with your publisher before doing so;
not all journals will accept material that has been released.
Therefore you may need to wait until acceptance is confirmed.
This can be done on a number of preprint websites,
many of which are topic-specific.\sidenote{
  \url{https://en.wikipedia.org/wiki/ArXiv}}
You can also use GitHub and link to the PDF file directly
on your personal website or whatever medium you are
sharing the preprint through.
Do not use Dropbox or Google Drive for this purpose:
many organizations do not allow access to these tools,
and that includes blocking staff from accessing your material.
